{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T04:25:56.266935Z",
     "iopub.status.busy": "2023-11-13T04:25:56.266266Z",
     "iopub.status.idle": "2023-11-13T04:25:56.271913Z",
     "shell.execute_reply": "2023-11-13T04:25:56.271059Z",
     "shell.execute_reply.started": "2023-11-13T04:25:56.266900Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T04:26:02.305362Z",
     "iopub.status.busy": "2023-11-13T04:26:02.304515Z",
     "iopub.status.idle": "2023-11-13T04:26:02.311103Z",
     "shell.execute_reply": "2023-11-13T04:26:02.310049Z",
     "shell.execute_reply.started": "2023-11-13T04:26:02.305306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the GPU available? True\n"
     ]
    }
   ],
   "source": [
    "gpu_avail = torch.cuda.is_available()\n",
    "print(f\"Is the GPU available? {gpu_avail}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T04:26:04.181149Z",
     "iopub.status.busy": "2023-11-13T04:26:04.180304Z",
     "iopub.status.idle": "2023-11-13T04:26:04.186039Z",
     "shell.execute_reply": "2023-11-13T04:26:04.185136Z",
     "shell.execute_reply.started": "2023-11-13T04:26:04.181114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T04:26:06.495444Z",
     "iopub.status.busy": "2023-11-13T04:26:06.495048Z",
     "iopub.status.idle": "2023-11-13T04:26:06.503615Z",
     "shell.execute_reply": "2023-11-13T04:26:06.502495Z",
     "shell.execute_reply.started": "2023-11-13T04:26:06.495414Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        with open(f\"{path}.sources\") as f:\n",
    "            self.sources = f.readlines()\n",
    "            self.vocab_src = set()\n",
    "            for lines in self.sources:\n",
    "                self.vocab_src.update(lines[1:-1])\n",
    "                \n",
    "        with open(f\"{path}.targets\") as f:\n",
    "            self.targets = f.readlines()\n",
    "            self.vocab_tgt = set()\n",
    "            for lines in self.targets:\n",
    "                self.vocab_tgt.update(lines)\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sources)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sources[idx][1:-1], self.targets[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T04:26:08.859028Z",
     "iopub.status.busy": "2023-11-13T04:26:08.858187Z",
     "iopub.status.idle": "2023-11-13T04:26:10.471213Z",
     "shell.execute_reply": "2023-11-13T04:26:10.470375Z",
     "shell.execute_reply.started": "2023-11-13T04:26:08.858994Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_src = set()\n",
    "vocab_tgt = set()\n",
    "train_dataset = CustomDataset('Data/A3 files/train')\n",
    "eval_dataset = CustomDataset('Data/A3 files/dev')\n",
    "test_dataset = CustomDataset('Data/A3 files/test')\n",
    "vocab_src.update(train_dataset.vocab_src)\n",
    "vocab_src.update(eval_dataset.vocab_src)\n",
    "vocab_src.update(test_dataset.vocab_src)\n",
    "vocab_tgt.update(train_dataset.vocab_tgt)\n",
    "vocab_tgt.update(eval_dataset.vocab_tgt)\n",
    "vocab_tgt.update(test_dataset.vocab_tgt)\n",
    "vocab_src = list(vocab_src)\n",
    "vocab_tgt = list(vocab_tgt)\n",
    "temp = {}\n",
    "for ind, key in enumerate(vocab_src):\n",
    "    temp[key] = ind\n",
    "vocab_src = temp\n",
    "temp = {}\n",
    "for ind, key in enumerate(vocab_tgt):\n",
    "    temp[key] = ind\n",
    "vocab_tgt = temp\n",
    "vocab_src[\"END\"] = 84\n",
    "vocab_src[\"PAD\"] = 85\n",
    "vocab_tgt[\"STR\"] = 44\n",
    "vocab_tgt[\"END\"] = 45\n",
    "vocab_tgt[\"PAD\"] = 46\n",
    "reverse_vocab_tgt = {}\n",
    "for key in vocab_tgt:\n",
    "    val = vocab_tgt[key]\n",
    "    reverse_vocab_tgt[val] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T04:26:13.612417Z",
     "iopub.status.busy": "2023-11-13T04:26:13.612066Z",
     "iopub.status.idle": "2023-11-13T04:26:13.640847Z",
     "shell.execute_reply": "2023-11-13T04:26:13.639734Z",
     "shell.execute_reply.started": "2023-11-13T04:26:13.612391Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size = 32, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 32, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T04:26:17.458618Z",
     "iopub.status.busy": "2023-11-13T04:26:17.458247Z",
     "iopub.status.idle": "2023-11-13T04:26:17.471097Z",
     "shell.execute_reply": "2023-11-13T04:26:17.469974Z",
     "shell.execute_reply.started": "2023-11-13T04:26:17.458589Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dims = 512, hidden_size = 512,num_layers = 2, max_src = 500, max_tgt = 500):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab_src),dims)\n",
    "        self.input_size = dims\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.max_src = max_src\n",
    "        self.max_tgt = max_tgt\n",
    "        self.drop = nn.Dropout(p = 0.5)\n",
    "        self.encoder = nn.LSTM(\n",
    "            dims, hidden_size, num_layers, batch_first = True,bidirectional=True, dropout = 0.5\n",
    "        )\n",
    "        \n",
    "    def encode_inp(self, x):\n",
    "        encoded_x = torch.zeros(len(x), self.max_src, dtype = int) + 85\n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(x[i])):\n",
    "                encoded_x[i][j] = vocab_src[x[i][j]]\n",
    "            encoded_x[i][len(x[i])] = vocab_src[\"END\"]\n",
    "        return encoded_x.to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded_x = self.encode_inp(x)\n",
    "        input_seq = self.drop(self.embedding(encoded_x))\n",
    "        hidden = torch.zeros(2*self.num_layers,input_seq.shape[0],self.hidden_size).to(device)\n",
    "        cell = torch.zeros(2*self.num_layers,input_seq.shape[0],self.hidden_size).to(device)\n",
    "        out, _ = self.encoder(input_seq,(hidden, cell))\n",
    "        return out\n",
    "\n",
    "    def inference(self, x):\n",
    "        self.max_src = len(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_diversity_penalty(new_sequence, existing_sequences):\n",
    "    \"\"\"\n",
    "    Calculate a diversity penalty based on the new sequence and existing sequences.\n",
    "    This is a simple example of diversity penalty calculation and can be customized.\n",
    "\n",
    "    Args:\n",
    "    - new_sequence (torch.Tensor): The new sequence to be penalized.\n",
    "    - existing_sequences (list of torch.Tensor): A list of existing sequences.\n",
    "\n",
    "    Returns:\n",
    "    - float: The diversity penalty score.\n",
    "    \"\"\"\n",
    "    penalty = 0.0\n",
    "    for seq in existing_sequences:\n",
    "        similarity = torch.sum(torch.eq(new_sequence, seq[0]).float()) / len(new_sequence)\n",
    "        penalty += similarity\n",
    "    return penalty\n",
    "\n",
    "def beam_search_decoder(probabilities, beam_width, max_length, diversity_penalty_weight=0.7):\n",
    "    \"\"\"\n",
    "    Beam search decoder for sequence generation.\n",
    "\n",
    "    Args:\n",
    "    - probabilities (torch.Tensor): A 2D tensor of shape (sequence_length, vocab_size)\n",
    "      containing the predicted probabilities for each token at each time step.\n",
    "    - beam_width (int): The number of sequences to consider at each decoding step.\n",
    "    - max_length (int): The maximum length of the generated sequence.\n",
    "\n",
    "    Returns:\n",
    "    - List of tuples, each containing (sequence, score), where:\n",
    "      - sequence (list): A list of token IDs representing the generated sequence.\n",
    "      - score (float): The log-likelihood score of the sequence.\n",
    "    \"\"\"\n",
    "    out  = torch.argmax(nn.Softmax(dim = 1)(probabilities), dim = 1)\n",
    "    seq_len = 0\n",
    "    for char in out:\n",
    "        if(char == 45):\n",
    "            break\n",
    "        else:\n",
    "            seq_len += 1\n",
    "\n",
    "    # Get the sequence length and vocabulary size\n",
    "    sequence_length, vocab_size = probabilities.shape\n",
    "    sequence_length = seq_len\n",
    "    max_length = seq_len\n",
    "\n",
    "    # Initialize the beam with the empty sequence\n",
    "    beam = [(torch.tensor([], dtype=torch.long).to(device), 0.0)]\n",
    "\n",
    "    # Iterate through each time step\n",
    "    for t in range(max_length):\n",
    "        new_beam = []\n",
    "\n",
    "        # Expand the beam by considering the top 'beam_width' candidates at each step\n",
    "        for sequence, score in beam:\n",
    "            # If the sequence is already at the maximum length, keep it as is\n",
    "            if len(sequence) == max_length:\n",
    "                new_beam.append((sequence, score))\n",
    "                continue\n",
    "\n",
    "            # Get the probabilities for the next token\n",
    "            t_probs = probabilities[t]\n",
    "\n",
    "            # Get the top 'beam_width' token IDs and their corresponding log-likelihood scores\n",
    "            top_scores, top_tokens = torch.topk(t_probs, beam_width)\n",
    "\n",
    "            # Expand the current sequence with each of the top tokens\n",
    "            for token, token_score in zip(top_tokens, top_scores):\n",
    "                new_sequence = torch.cat([sequence, token.unsqueeze(0)], dim=0)\n",
    "                new_score = score + token_score.item()\n",
    "    \n",
    "                # Apply the diversity penalty\n",
    "                if len(new_sequence) > 1:\n",
    "                    # Calculate a penalty based on sequence diversity\n",
    "                    diversity_penalty = diversity_penalty_weight * calculate_diversity_penalty(new_sequence, new_beam)\n",
    "                    new_score -= diversity_penalty\n",
    "                    \n",
    "                new_beam.append((new_sequence, new_score))\n",
    "        print(t)\n",
    "\n",
    "        # Keep the top 'beam_width' candidates\n",
    "        new_beam.sort(key=lambda x: -x[1])\n",
    "        beam = new_beam[:beam_width]\n",
    "\n",
    "    # Return the top sequence and its score\n",
    "    return [(sequence.tolist(), score) for sequence, score in beam]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T04:26:18.884154Z",
     "iopub.status.busy": "2023-11-13T04:26:18.883836Z",
     "iopub.status.idle": "2023-11-13T04:26:18.926870Z",
     "shell.execute_reply": "2023-11-13T04:26:18.925694Z",
     "shell.execute_reply.started": "2023-11-13T04:26:18.884130Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,dims = 512, hidden_size = 512,num_layers = 2, max_src = 500, max_tgt = 500):\n",
    "        super(Decoder, self).__init__()\n",
    "#         self.embedding = embedding\n",
    "        self.embedding = nn.Embedding(len(vocab_tgt),dims)\n",
    "        self.input_size = dims\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.max_src = max_src\n",
    "        self.max_tgt = max_tgt\n",
    "        self.drop = nn.Dropout(p = 0.5)\n",
    "        self.dec_cells = nn.ModuleList([nn.LSTMCell(2*hidden_size+dims, hidden_size), nn.LSTMCell(hidden_size, hidden_size)])\n",
    "        self.linear = nn.Linear(hidden_size,len(vocab_tgt)-1)\n",
    "    \n",
    "    def encode_inp(self, x):\n",
    "        encoded_x = torch.zeros(len(x), self.max_tgt+1, dtype = int) + 46\n",
    "        encoded_x_end = torch.zeros(len(x))\n",
    "        for i in range(len(x)):\n",
    "            encoded_x[i][0] = vocab_tgt[\"STR\"]\n",
    "            for j in range(len(x[i])):\n",
    "                encoded_x[i][j+1] = vocab_tgt[x[i][j]]\n",
    "            encoded_x[i][len(x[i])+1] = vocab_tgt[\"END\"]\n",
    "            encoded_x_end[i] = len(x[i])+1\n",
    "        return encoded_x.to(device), encoded_x_end\n",
    "    \n",
    "    def calcontext(self, timestep, query):\n",
    "        extended_query = torch.cat((query, query), dim = 1)\n",
    "        permuted_context = self.context.permute(1,0,2)\n",
    "#         for encoder_timestep in range(self.context.shape[1]):\n",
    "#             scores.append(torch.sum(self.context[:,encoder_timestep] * extended_query, dim = 1, keepdims=True))\n",
    "#         scores = torch.cat(scores, dim = 1)\n",
    "        scores = torch.sum(permuted_context * extended_query, dim = 2).permute(1,0)\n",
    "        weights = nn.Softmax(dim = 1)(scores).unsqueeze(2)\n",
    "        alignment = torch.sum(weights * self.context,  dim = 1)\n",
    "        \n",
    "        return alignment\n",
    "        \n",
    "    \n",
    "    def forward(self, context, target_,teacher_ratio):\n",
    "        self.context = context\n",
    "        encoded_x, encoded_x_end = self.encode_inp(target_)\n",
    "        target_seq = self.embedding(encoded_x)\n",
    "        \n",
    "        initial_hidden1 = torch.rand(target_seq.shape[0], self.hidden_size).to(device)\n",
    "        initial_cell1 = torch.rand(target_seq.shape[0], self.hidden_size).to(device)\n",
    "        initial_hidden2 = torch.rand(target_seq.shape[0], self.hidden_size).to(device)\n",
    "        initial_cell2 = torch.rand(target_seq.shape[0], self.hidden_size).to(device)\n",
    "        \n",
    "        outputs = []\n",
    "        hidden_states = []\n",
    "        cell_states = []\n",
    "        query = [initial_hidden2]\n",
    "        for timestep in range(self.max_tgt+1):\n",
    "            if(timestep == 0):\n",
    "                (h_t1, c_t1) = self.dec_cells[0](self.drop(torch.cat((target_seq[:,timestep],self.calcontext(0,query[-1])),dim=1)), (initial_hidden1, initial_cell1))\n",
    "                (h_t2, c_t2) = self.dec_cells[1](self.drop(h_t1), (initial_hidden2, initial_cell2))\n",
    "            else:\n",
    "                input = []\n",
    "                if(torch.rand(1).item() < teacher_ratio):\n",
    "                    input = target_seq[:,timestep]\n",
    "                else:\n",
    "                    input = self.embedding(torch.argmax(nn.Softmax(dim = 1)(outputs[-1][:,0]),dim=1))\n",
    "                    \n",
    "                (h_t1, c_t1) = self.dec_cells[0](self.drop(torch.cat((input,self.calcontext(timestep,query[-1])),dim=1)), (hidden_states[-1][0], cell_states[-1][0]))\n",
    "                (h_t2, c_t2) = self.dec_cells[1](self.drop(h_t1), (hidden_states[-1][1], cell_states[-1][1]))\n",
    "            hidden_states.append([h_t1, h_t2])\n",
    "            cell_states.append([c_t1, c_t2])\n",
    "            query.append(h_t2)\n",
    "            out = self.linear(h_t2)\n",
    "            outputs.append(out.unsqueeze(1))\n",
    "    \n",
    "\n",
    "        output_prob = torch.cat(outputs,dim = 1)\n",
    "        \n",
    "        return nn.LogSoftmax(dim = 2)(output_prob), encoded_x\n",
    "    \n",
    "    def seq_to_vis(self, seq):\n",
    "        vis = \"\"\n",
    "        for char in seq:\n",
    "            char = char\n",
    "            if(char == 45):\n",
    "                return vis\n",
    "            vis += reverse_vocab_tgt[char]\n",
    "        \n",
    "        return vis\n",
    "\n",
    "\n",
    "    def inference(self, context):\n",
    "        self.context = context\n",
    "        start = torch.zeros(1,1, dtype = int).to(device) + 44\n",
    "        target_seq = self.embedding(start)\n",
    "        \n",
    "        initial_hidden1 = torch.rand(target_seq.shape[0], self.hidden_size).to(device)\n",
    "        initial_cell1 = torch.rand(target_seq.shape[0], self.hidden_size).to(device)\n",
    "        initial_hidden2 = torch.rand(target_seq.shape[0], self.hidden_size).to(device)\n",
    "        initial_cell2 = torch.rand(target_seq.shape[0], self.hidden_size).to(device)\n",
    "        \n",
    "        outputs = []\n",
    "        hidden_states = []\n",
    "        cell_states = []\n",
    "        query = [initial_hidden2]\n",
    "        for timestep in range(self.max_tgt+1):\n",
    "            if(timestep == 0):\n",
    "                (h_t1, c_t1) = self.dec_cells[0](self.drop(torch.cat((target_seq[:,timestep],self.calcontext(0,query[-1])),dim=1)), (initial_hidden1, initial_cell1))\n",
    "                (h_t2, c_t2) = self.dec_cells[1](self.drop(h_t1), (initial_hidden2, initial_cell2))\n",
    "            else:\n",
    "                input = self.embedding(torch.argmax(nn.Softmax(dim = 1)(outputs[-1][:,0]),dim=1))\n",
    "                    \n",
    "                (h_t1, c_t1) = self.dec_cells[0](self.drop(torch.cat((input,self.calcontext(timestep,query[-1])),dim=1)), (hidden_states[-1][0], cell_states[-1][0]))\n",
    "                (h_t2, c_t2) = self.dec_cells[1](self.drop(h_t1), (hidden_states[-1][1], cell_states[-1][1]))\n",
    "            hidden_states.append([h_t1, h_t2])\n",
    "            cell_states.append([c_t1, c_t2])\n",
    "            query.append(h_t2)\n",
    "            out = self.linear(h_t2)\n",
    "            outputs.append(out.unsqueeze(1))\n",
    "    \n",
    "        output_prob = torch.cat(outputs,dim = 1)\n",
    "        out = torch.argmax(nn.Softmax(dim = 2)(output_prob), dim = 2)\n",
    "        \n",
    "#         Example usage:\n",
    "#         Replace 'probabilities', 'beam_width', and 'max_length' with your actual values\n",
    "#         probabilities = torch.tensor(...)  # Shape: (sequence_length, vocab_size)\n",
    "#         beam_width = 3\n",
    "#         max_length = 10\n",
    "        decoded_sequences = beam_search_decoder(output_prob[0], beam_width = 15, max_length = 500, diversity_penalty_weight = 0.7)\n",
    "        vis = []\n",
    "        for sequence, score in decoded_sequences:\n",
    "            vis.append(self.seq_to_vis(sequence))\n",
    "            # print(f\"Sequence: {sequence}, {self.seq_to_vis(sequence)}, Log-Likelihood Score: {score}\")\n",
    "\n",
    "        return vis\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-12T12:30:57.584467Z",
     "iopub.status.busy": "2023-11-12T12:30:57.583708Z",
     "iopub.status.idle": "2023-11-12T12:30:57.740710Z",
     "shell.execute_reply": "2023-11-12T12:30:57.739906Z",
     "shell.execute_reply.started": "2023-11-12T12:30:57.584433Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = Encoder()\n",
    "decoder = Decoder()\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "optimizer_encoder = optim.Adam(encoder.parameters(), lr=0.0001)\n",
    "optimizer_decoder = optim.Adam(decoder.parameters(), lr=0.0001)\n",
    "criterion = nn.NLLLoss(ignore_index = 46)\n",
    "\n",
    "# Set up early stopping parameters\n",
    "patience = 2000  # Number of epochs to wait for improvement\n",
    "best_val_loss = float('inf')\n",
    "epochs_since_improvement = 0\n",
    "loss_val = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-12T12:31:09.077395Z",
     "iopub.status.busy": "2023-11-12T12:31:09.076684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0], Batch[1], Train loss: 0.06569177657365799\n",
      "Epoch[0], Val loss: 0.05901820585131645\n",
      "Epoch[0], Batch[2], Train loss: 0.06615637987852097\n",
      "Epoch[0], Val loss: 0.06083571910858154\n",
      "Epoch[0], Batch[3], Train loss: 0.06795929372310638\n",
      "Epoch[0], Val loss: 0.060594167560338974\n",
      "Epoch[0], Batch[4], Train loss: 0.06530726701021194\n",
      "Epoch[0], Val loss: 0.06135530024766922\n",
      "Epoch[0], Batch[5], Train loss: 0.06575331836938858\n",
      "Epoch[0], Val loss: 0.06235644593834877\n",
      "Epoch[0], Batch[6], Train loss: 0.06730900704860687\n",
      "Epoch[0], Val loss: 0.05923289433121681\n",
      "Epoch[0], Batch[7], Train loss: 0.06717878580093384\n",
      "Epoch[0], Val loss: 0.05766945332288742\n",
      "Epoch[0], Batch[8], Train loss: 0.06548965722322464\n",
      "Epoch[0], Val loss: 0.058144014328718185\n",
      "Epoch[0], Batch[9], Train loss: 0.06711244583129883\n",
      "Epoch[0], Val loss: 0.05668947845697403\n",
      "Epoch[0], Batch[10], Train loss: 0.0701310783624649\n",
      "Epoch[0], Val loss: 0.06595741957426071\n",
      "Epoch[0], Batch[11], Train loss: 0.06448052078485489\n",
      "Epoch[0], Val loss: 0.05850837379693985\n",
      "Epoch[0], Batch[12], Train loss: 0.06451056897640228\n",
      "Epoch[0], Val loss: 0.0590619333088398\n",
      "Epoch[0], Batch[13], Train loss: 0.07020217180252075\n",
      "Epoch[0], Val loss: 0.05801258236169815\n",
      "Epoch[0], Batch[14], Train loss: 0.06573434174060822\n",
      "Epoch[0], Val loss: 0.056339193135499954\n",
      "Epoch[0], Batch[15], Train loss: 0.06298331171274185\n",
      "Epoch[0], Val loss: 0.05705970153212547\n",
      "Epoch[0], Batch[16], Train loss: 0.06874626874923706\n",
      "Epoch[0], Val loss: 0.058247677981853485\n",
      "Epoch[0], Batch[17], Train loss: 0.06656275689601898\n",
      "Epoch[0], Val loss: 0.060472019016742706\n",
      "Epoch[0], Batch[18], Train loss: 0.06593237817287445\n",
      "Epoch[0], Val loss: 0.058822546154260635\n",
      "Epoch[0], Batch[19], Train loss: 0.06715548038482666\n",
      "Epoch[0], Val loss: 0.06339418888092041\n",
      "Epoch[0], Batch[20], Train loss: 0.06330280005931854\n",
      "Epoch[0], Val loss: 0.06202339753508568\n",
      "Epoch[0], Batch[21], Train loss: 0.06841013580560684\n",
      "Epoch[0], Val loss: 0.06143976375460625\n",
      "Epoch[0], Batch[22], Train loss: 0.07160722464323044\n",
      "Epoch[0], Val loss: 0.05931535363197327\n",
      "Epoch[0], Batch[23], Train loss: 0.0629522055387497\n",
      "Epoch[0], Val loss: 0.05950229614973068\n",
      "Epoch[0], Batch[24], Train loss: 0.06662468612194061\n",
      "Epoch[0], Val loss: 0.06189410388469696\n",
      "Epoch[0], Batch[25], Train loss: 0.06571384519338608\n",
      "Epoch[0], Val loss: 0.05751253664493561\n",
      "Epoch[0], Batch[26], Train loss: 0.06689640134572983\n",
      "Epoch[0], Val loss: 0.055117007344961166\n",
      "Epoch[0], Batch[27], Train loss: 0.06677664816379547\n",
      "Epoch[0], Val loss: 0.05968879535794258\n",
      "Epoch[0], Batch[28], Train loss: 0.06911429762840271\n",
      "Epoch[0], Val loss: 0.05809185653924942\n",
      "Epoch[0], Batch[29], Train loss: 0.06720641255378723\n",
      "Epoch[0], Val loss: 0.05687715485692024\n",
      "Epoch[0], Batch[30], Train loss: 0.06481515616178513\n",
      "Epoch[0], Val loss: 0.06014543026685715\n",
      "Epoch[0], Batch[31], Train loss: 0.07161001861095428\n",
      "Epoch[0], Val loss: 0.05926194787025452\n",
      "Epoch[0], Batch[32], Train loss: 0.06444894522428513\n",
      "Epoch[0], Val loss: 0.058141447603702545\n",
      "Epoch[0], Batch[33], Train loss: 0.059843506664037704\n",
      "Epoch[0], Val loss: 0.05695372819900513\n",
      "Epoch[0], Batch[34], Train loss: 0.06459547579288483\n",
      "Epoch[0], Val loss: 0.059197261929512024\n",
      "Epoch[0], Batch[35], Train loss: 0.06731818616390228\n",
      "Epoch[0], Val loss: 0.05607201159000397\n",
      "Epoch[0], Batch[36], Train loss: 0.06405076384544373\n",
      "Epoch[0], Val loss: 0.058489345014095306\n",
      "Epoch[0], Batch[37], Train loss: 0.06803321838378906\n",
      "Epoch[0], Val loss: 0.06252257525920868\n",
      "Epoch[0], Batch[38], Train loss: 0.06444112211465836\n",
      "Epoch[0], Val loss: 0.06292754411697388\n",
      "Epoch[0], Batch[39], Train loss: 0.06035809963941574\n",
      "Epoch[0], Val loss: 0.057395294308662415\n",
      "Epoch[0], Batch[40], Train loss: 0.06726481765508652\n",
      "Epoch[0], Val loss: 0.061815183609724045\n",
      "Epoch[0], Batch[41], Train loss: 0.06223088502883911\n",
      "Epoch[0], Val loss: 0.05677315220236778\n",
      "Epoch[0], Batch[42], Train loss: 0.06444946676492691\n",
      "Epoch[0], Val loss: 0.06110655143857002\n",
      "Epoch[0], Batch[43], Train loss: 0.06322582066059113\n",
      "Epoch[0], Val loss: 0.057870298624038696\n",
      "Epoch[0], Batch[44], Train loss: 0.0667010173201561\n",
      "Epoch[0], Val loss: 0.05474894121289253\n",
      "Epoch[0], Batch[45], Train loss: 0.06328244507312775\n",
      "Epoch[0], Val loss: 0.058826059103012085\n",
      "Epoch[0], Batch[46], Train loss: 0.0717262551188469\n",
      "Epoch[0], Val loss: 0.0571398064494133\n",
      "Epoch[0], Batch[47], Train loss: 0.06860435009002686\n",
      "Epoch[0], Val loss: 0.06279689073562622\n",
      "Epoch[0], Batch[48], Train loss: 0.06918589025735855\n",
      "Epoch[0], Val loss: 0.061932239681482315\n",
      "Epoch[0], Batch[49], Train loss: 0.06323908269405365\n",
      "Epoch[0], Val loss: 0.05804147571325302\n",
      "Epoch[0], Batch[50], Train loss: 0.06884805113077164\n",
      "Epoch[0], Val loss: 0.06189298629760742\n",
      "Epoch[0], Batch[51], Train loss: 0.06692267954349518\n",
      "Epoch[0], Val loss: 0.05765581503510475\n",
      "Epoch[0], Batch[52], Train loss: 0.0616353340446949\n",
      "Epoch[0], Val loss: 0.0567145049571991\n",
      "Epoch[0], Batch[53], Train loss: 0.06387627124786377\n",
      "Epoch[0], Val loss: 0.061319779604673386\n",
      "Epoch[0], Batch[54], Train loss: 0.06647374480962753\n",
      "Epoch[0], Val loss: 0.05936110392212868\n",
      "Epoch[0], Batch[55], Train loss: 0.0643925666809082\n",
      "Epoch[0], Val loss: 0.05917610228061676\n",
      "Epoch[0], Batch[56], Train loss: 0.0655532106757164\n",
      "Epoch[0], Val loss: 0.057241011410951614\n",
      "Epoch[0], Batch[57], Train loss: 0.06282314658164978\n",
      "Epoch[0], Val loss: 0.05867743492126465\n",
      "Epoch[0], Batch[58], Train loss: 0.0629856064915657\n",
      "Epoch[0], Val loss: 0.053235217928886414\n",
      "Epoch[0], Batch[59], Train loss: 0.06935755163431168\n",
      "Epoch[0], Val loss: 0.05680331587791443\n",
      "Epoch[0], Batch[60], Train loss: 0.06176776438951492\n",
      "Epoch[0], Val loss: 0.060685209929943085\n",
      "Epoch[0], Batch[61], Train loss: 0.06389826536178589\n",
      "Epoch[0], Val loss: 0.05493319779634476\n",
      "Epoch[0], Batch[62], Train loss: 0.062294889241456985\n",
      "Epoch[0], Val loss: 0.05422348156571388\n",
      "Epoch[0], Batch[63], Train loss: 0.061427563428878784\n",
      "Epoch[0], Val loss: 0.05949795991182327\n",
      "Epoch[0], Batch[64], Train loss: 0.061855215579271317\n",
      "Epoch[0], Val loss: 0.058995168656110764\n",
      "Epoch[0], Batch[65], Train loss: 0.0644204169511795\n",
      "Epoch[0], Val loss: 0.056294623762369156\n",
      "Epoch[0], Batch[66], Train loss: 0.06306903809309006\n",
      "Epoch[0], Val loss: 0.06373803317546844\n",
      "Epoch[0], Batch[67], Train loss: 0.06005542352795601\n",
      "Epoch[0], Val loss: 0.05704741179943085\n",
      "Epoch[0], Batch[68], Train loss: 0.06301805377006531\n",
      "Epoch[0], Val loss: 0.06310435384511948\n",
      "Epoch[0], Batch[69], Train loss: 0.06657639890909195\n",
      "Epoch[0], Val loss: 0.061681877821683884\n",
      "Epoch[0], Batch[70], Train loss: 0.06802573055028915\n",
      "Epoch[0], Val loss: 0.05685935541987419\n",
      "Epoch[0], Batch[71], Train loss: 0.06778629124164581\n",
      "Epoch[0], Val loss: 0.05868419632315636\n",
      "Epoch[0], Batch[72], Train loss: 0.06982709467411041\n",
      "Epoch[0], Val loss: 0.05561549589037895\n",
      "Epoch[0], Batch[73], Train loss: 0.06550827622413635\n",
      "Epoch[0], Val loss: 0.06073819473385811\n",
      "Epoch[0], Batch[74], Train loss: 0.06564134359359741\n",
      "Epoch[0], Val loss: 0.05959160998463631\n",
      "Epoch[0], Batch[75], Train loss: 0.06233403831720352\n",
      "Epoch[0], Val loss: 0.058830875903367996\n",
      "Epoch[0], Batch[76], Train loss: 0.0647168755531311\n",
      "Epoch[0], Val loss: 0.06430251151323318\n",
      "Epoch[0], Batch[77], Train loss: 0.06878244876861572\n",
      "Epoch[0], Val loss: 0.0555940605700016\n",
      "Epoch[0], Batch[78], Train loss: 0.06469302624464035\n",
      "Epoch[0], Val loss: 0.05364563688635826\n",
      "Epoch[0], Batch[79], Train loss: 0.06099362671375275\n",
      "Epoch[0], Val loss: 0.0582643561065197\n",
      "Epoch[0], Batch[80], Train loss: 0.07083619385957718\n",
      "Epoch[0], Val loss: 0.051969464868307114\n",
      "Epoch[0], Batch[81], Train loss: 0.05943746119737625\n",
      "Epoch[0], Val loss: 0.060238394886255264\n",
      "Epoch[0], Batch[82], Train loss: 0.06380035728216171\n",
      "Epoch[0], Val loss: 0.056697722524404526\n",
      "Epoch[0], Batch[83], Train loss: 0.06269488483667374\n",
      "Epoch[0], Val loss: 0.056361667811870575\n",
      "Epoch[0], Batch[84], Train loss: 0.06461150199174881\n",
      "Epoch[0], Val loss: 0.0595397986471653\n",
      "Epoch[0], Batch[85], Train loss: 0.06488937139511108\n",
      "Epoch[0], Val loss: 0.054572220891714096\n",
      "Epoch[0], Batch[86], Train loss: 0.06297148019075394\n",
      "Epoch[0], Val loss: 0.05641636997461319\n",
      "Epoch[0], Batch[87], Train loss: 0.060913924127817154\n",
      "Epoch[0], Val loss: 0.05680061876773834\n",
      "Epoch[0], Batch[88], Train loss: 0.06528554856777191\n",
      "Epoch[0], Val loss: 0.05631443113088608\n",
      "Epoch[0], Batch[89], Train loss: 0.06610099971294403\n",
      "Epoch[0], Val loss: 0.05494978651404381\n",
      "Epoch[0], Batch[90], Train loss: 0.06434477865695953\n",
      "Epoch[0], Val loss: 0.055703870952129364\n",
      "Epoch[0], Batch[91], Train loss: 0.06187884137034416\n",
      "Epoch[0], Val loss: 0.0601288266479969\n",
      "Epoch[0], Batch[92], Train loss: 0.06339653581380844\n",
      "Epoch[0], Val loss: 0.05679263919591904\n",
      "Epoch[0], Batch[93], Train loss: 0.0617692805826664\n",
      "Epoch[0], Val loss: 0.060349930077791214\n",
      "Epoch[0], Batch[94], Train loss: 0.06412637233734131\n",
      "Epoch[0], Val loss: 0.055312760174274445\n",
      "Epoch[0], Batch[95], Train loss: 0.06139685586094856\n",
      "Epoch[0], Val loss: 0.05573026090860367\n",
      "Epoch[0], Batch[96], Train loss: 0.0657336488366127\n",
      "Epoch[0], Val loss: 0.05718828737735748\n",
      "Epoch[0], Batch[97], Train loss: 0.06436513364315033\n",
      "Epoch[0], Val loss: 0.052426960319280624\n",
      "Epoch[0], Batch[98], Train loss: 0.061770860105752945\n",
      "Epoch[0], Val loss: 0.055788554251194\n",
      "Epoch[0], Batch[99], Train loss: 0.06196359917521477\n",
      "Epoch[0], Val loss: 0.056584011763334274\n",
      "Epoch[0], Batch[100], Train loss: 0.06021280214190483\n",
      "Epoch[0], Val loss: 0.06337085366249084\n",
      "Epoch[0], Batch[101], Train loss: 0.061271343380212784\n",
      "Epoch[0], Val loss: 0.06214553490281105\n",
      "Epoch[0], Batch[102], Train loss: 0.06506690382957458\n",
      "Epoch[0], Val loss: 0.0584539920091629\n",
      "Epoch[0], Batch[103], Train loss: 0.058047134429216385\n",
      "Epoch[0], Val loss: 0.057138457894325256\n",
      "Epoch[0], Batch[104], Train loss: 0.05974118784070015\n",
      "Epoch[0], Val loss: 0.05757633596658707\n",
      "Epoch[0], Batch[105], Train loss: 0.06444292515516281\n",
      "Epoch[0], Val loss: 0.058410175144672394\n",
      "Epoch[0], Batch[106], Train loss: 0.06168625131249428\n",
      "Epoch[0], Val loss: 0.05544969066977501\n",
      "Epoch[0], Batch[107], Train loss: 0.06007327139377594\n",
      "Epoch[0], Val loss: 0.05243565887212753\n",
      "Epoch[0], Batch[108], Train loss: 0.06276050955057144\n",
      "Epoch[0], Val loss: 0.052679259330034256\n",
      "Epoch[0], Batch[109], Train loss: 0.06282550096511841\n",
      "Epoch[0], Val loss: 0.05837683379650116\n",
      "Epoch[0], Batch[110], Train loss: 0.06473487615585327\n",
      "Epoch[0], Val loss: 0.053952619433403015\n",
      "Epoch[0], Batch[111], Train loss: 0.06621085852384567\n",
      "Epoch[0], Val loss: 0.05984785780310631\n",
      "Epoch[0], Batch[112], Train loss: 0.05787861719727516\n",
      "Epoch[0], Val loss: 0.055025119334459305\n",
      "Epoch[0], Batch[113], Train loss: 0.0599304623901844\n",
      "Epoch[0], Val loss: 0.05603319779038429\n",
      "Epoch[0], Batch[114], Train loss: 0.062353167682886124\n",
      "Epoch[0], Val loss: 0.057474102824926376\n",
      "Epoch[0], Batch[115], Train loss: 0.06218560039997101\n",
      "Epoch[0], Val loss: 0.05570725351572037\n",
      "Epoch[0], Batch[116], Train loss: 0.06145665422081947\n",
      "Epoch[0], Val loss: 0.05787138640880585\n",
      "Epoch[0], Batch[117], Train loss: 0.062398046255111694\n",
      "Epoch[0], Val loss: 0.056199073791503906\n",
      "Epoch[0], Batch[118], Train loss: 0.0576019287109375\n",
      "Epoch[0], Val loss: 0.06004521623253822\n",
      "Epoch[0], Batch[119], Train loss: 0.06990274041891098\n",
      "Epoch[0], Val loss: 0.05673256143927574\n",
      "Epoch[0], Batch[120], Train loss: 0.05740119144320488\n",
      "Epoch[0], Val loss: 0.056994710117578506\n",
      "Epoch[0], Batch[121], Train loss: 0.06063832342624664\n",
      "Epoch[0], Val loss: 0.052160561084747314\n",
      "Epoch[0], Batch[122], Train loss: 0.06380865722894669\n",
      "Epoch[0], Val loss: 0.056302741169929504\n",
      "Epoch[0], Batch[123], Train loss: 0.06179932504892349\n",
      "Epoch[0], Val loss: 0.05787113681435585\n",
      "Epoch[0], Batch[124], Train loss: 0.06783387809991837\n",
      "Epoch[0], Val loss: 0.05444164201617241\n",
      "Epoch[0], Batch[125], Train loss: 0.06396357715129852\n",
      "Epoch[0], Val loss: 0.0550442636013031\n",
      "Epoch[0], Batch[126], Train loss: 0.058172281831502914\n",
      "Epoch[0], Val loss: 0.05226673558354378\n",
      "Epoch[0], Batch[127], Train loss: 0.06226221099495888\n",
      "Epoch[0], Val loss: 0.05881145969033241\n",
      "Epoch[0], Batch[128], Train loss: 0.06077839434146881\n",
      "Epoch[0], Val loss: 0.06051696091890335\n",
      "Epoch[0], Batch[129], Train loss: 0.05974207818508148\n",
      "Epoch[0], Val loss: 0.05581852048635483\n",
      "Epoch[0], Batch[130], Train loss: 0.06315690279006958\n",
      "Epoch[0], Val loss: 0.05724196881055832\n",
      "Epoch[0], Batch[131], Train loss: 0.05760779604315758\n",
      "Epoch[0], Val loss: 0.05173354595899582\n",
      "Epoch[0], Batch[132], Train loss: 0.061351228505373\n",
      "Epoch[0], Val loss: 0.05566282942891121\n",
      "Epoch[0], Batch[133], Train loss: 0.061076559126377106\n",
      "Epoch[0], Val loss: 0.05573521926999092\n",
      "Epoch[0], Batch[134], Train loss: 0.06305669993162155\n",
      "Epoch[0], Val loss: 0.05459762364625931\n",
      "Epoch[0], Batch[135], Train loss: 0.06310281902551651\n",
      "Epoch[0], Val loss: 0.057065945118665695\n",
      "Epoch[0], Batch[136], Train loss: 0.06259647756814957\n",
      "Epoch[0], Val loss: 0.06216777116060257\n",
      "Epoch[0], Batch[137], Train loss: 0.0589650422334671\n",
      "Epoch[0], Val loss: 0.053784288465976715\n",
      "Epoch[0], Batch[138], Train loss: 0.06314929574728012\n",
      "Epoch[0], Val loss: 0.05894317850470543\n",
      "Epoch[0], Batch[139], Train loss: 0.06293067336082458\n",
      "Epoch[0], Val loss: 0.05913200601935387\n",
      "Epoch[0], Batch[140], Train loss: 0.06619346141815186\n",
      "Epoch[0], Val loss: 0.05965491011738777\n",
      "Epoch[0], Batch[141], Train loss: 0.06807354092597961\n",
      "Epoch[0], Val loss: 0.056892029941082\n",
      "Epoch[0], Batch[142], Train loss: 0.06719699501991272\n",
      "Epoch[0], Val loss: 0.06208506599068642\n",
      "Epoch[0], Batch[143], Train loss: 0.05962791293859482\n",
      "Epoch[0], Val loss: 0.05528746917843819\n",
      "Epoch[0], Batch[144], Train loss: 0.06195366010069847\n",
      "Epoch[0], Val loss: 0.055488985031843185\n",
      "Epoch[0], Batch[145], Train loss: 0.062319982796907425\n",
      "Epoch[0], Val loss: 0.05676327645778656\n",
      "Epoch[0], Batch[146], Train loss: 0.06136719137430191\n",
      "Epoch[0], Val loss: 0.055811502039432526\n",
      "Epoch[0], Batch[147], Train loss: 0.06061049550771713\n",
      "Epoch[0], Val loss: 0.052226558327674866\n",
      "Epoch[0], Batch[148], Train loss: 0.058332763612270355\n",
      "Epoch[0], Val loss: 0.05834031477570534\n",
      "Epoch[0], Batch[149], Train loss: 0.061750378459692\n",
      "Epoch[0], Val loss: 0.05525689199566841\n",
      "Epoch[0], Batch[150], Train loss: 0.06141999363899231\n",
      "Epoch[0], Val loss: 0.05208642780780792\n",
      "Epoch[0], Batch[151], Train loss: 0.057407163083553314\n",
      "Epoch[0], Val loss: 0.05486885458230972\n",
      "Epoch[0], Batch[152], Train loss: 0.06046253815293312\n",
      "Epoch[0], Val loss: 0.05805707350373268\n",
      "Epoch[0], Batch[153], Train loss: 0.06017224118113518\n",
      "Epoch[0], Val loss: 0.05631953477859497\n",
      "Epoch[0], Batch[154], Train loss: 0.058302026242017746\n",
      "Epoch[0], Val loss: 0.055521346628665924\n",
      "Epoch[0], Batch[155], Train loss: 0.06254588067531586\n",
      "Epoch[0], Val loss: 0.05684323236346245\n",
      "Epoch[0], Batch[156], Train loss: 0.06235353276133537\n",
      "Epoch[0], Val loss: 0.055130284279584885\n",
      "Epoch[0], Batch[157], Train loss: 0.05925871059298515\n",
      "Epoch[0], Val loss: 0.05464385822415352\n",
      "Epoch[0], Batch[158], Train loss: 0.0599258691072464\n",
      "Epoch[0], Val loss: 0.05605359375476837\n",
      "Epoch[0], Batch[159], Train loss: 0.0643145889043808\n",
      "Epoch[0], Val loss: 0.058525923639535904\n",
      "Epoch[0], Batch[160], Train loss: 0.06159427389502525\n",
      "Epoch[0], Val loss: 0.05877179279923439\n",
      "Epoch[0], Batch[161], Train loss: 0.05834529921412468\n",
      "Epoch[0], Val loss: 0.05716106668114662\n",
      "Epoch[0], Batch[162], Train loss: 0.06105323135852814\n",
      "Epoch[0], Val loss: 0.053020577877759933\n",
      "Epoch[0], Batch[163], Train loss: 0.06592994928359985\n",
      "Epoch[0], Val loss: 0.051778074353933334\n",
      "Epoch[0], Batch[164], Train loss: 0.05580582469701767\n",
      "Epoch[0], Val loss: 0.056298766285181046\n",
      "Epoch[0], Batch[165], Train loss: 0.059939466416835785\n",
      "Epoch[0], Val loss: 0.05699494481086731\n",
      "Epoch[0], Batch[166], Train loss: 0.05928036943078041\n",
      "Epoch[0], Val loss: 0.0551408976316452\n",
      "Epoch[0], Batch[167], Train loss: 0.05897151306271553\n",
      "Epoch[0], Val loss: 0.05376424267888069\n",
      "Epoch[0], Batch[168], Train loss: 0.06054399535059929\n",
      "Epoch[0], Val loss: 0.05469759926199913\n",
      "Epoch[0], Batch[169], Train loss: 0.06151152774691582\n",
      "Epoch[0], Val loss: 0.060063205659389496\n",
      "Epoch[0], Batch[170], Train loss: 0.05953468754887581\n",
      "Epoch[0], Val loss: 0.05625784769654274\n",
      "Epoch[0], Batch[171], Train loss: 0.06311089545488358\n",
      "Epoch[0], Val loss: 0.053832896053791046\n",
      "Epoch[0], Batch[172], Train loss: 0.061451081186532974\n",
      "Epoch[0], Val loss: 0.05668732523918152\n",
      "Epoch[0], Batch[173], Train loss: 0.059809863567352295\n",
      "Epoch[0], Val loss: 0.05096166208386421\n",
      "Epoch[0], Batch[174], Train loss: 0.06190585345029831\n",
      "Epoch[0], Val loss: 0.05440768226981163\n",
      "Epoch[0], Batch[175], Train loss: 0.061725717037916183\n",
      "Epoch[0], Val loss: 0.056298382580280304\n",
      "Epoch[0], Batch[176], Train loss: 0.0651419386267662\n",
      "Epoch[0], Val loss: 0.05417618900537491\n",
      "Epoch[0], Batch[177], Train loss: 0.06189543008804321\n",
      "Epoch[0], Val loss: 0.0541551411151886\n",
      "Epoch[0], Batch[178], Train loss: 0.060763128101825714\n",
      "Epoch[0], Val loss: 0.05610434338450432\n",
      "Epoch[0], Batch[179], Train loss: 0.06400754302740097\n",
      "Epoch[0], Val loss: 0.056682199239730835\n",
      "Epoch[0], Batch[180], Train loss: 0.05899285897612572\n",
      "Epoch[0], Val loss: 0.05485958606004715\n",
      "Epoch[0], Batch[181], Train loss: 0.06022712215781212\n",
      "Epoch[0], Val loss: 0.0525461807847023\n",
      "Epoch[0], Batch[182], Train loss: 0.059497687965631485\n",
      "Epoch[0], Val loss: 0.06266837567090988\n",
      "Epoch[0], Batch[183], Train loss: 0.058376435190439224\n",
      "Epoch[0], Val loss: 0.05583665147423744\n",
      "Epoch[0], Batch[184], Train loss: 0.06040344759821892\n",
      "Epoch[0], Val loss: 0.05279407650232315\n",
      "Epoch[0], Batch[185], Train loss: 0.06432630866765976\n",
      "Epoch[0], Val loss: 0.057362623512744904\n",
      "Epoch[0], Batch[186], Train loss: 0.06094423308968544\n",
      "Epoch[0], Val loss: 0.05703328922390938\n",
      "Epoch[0], Batch[187], Train loss: 0.05947700887918472\n",
      "Epoch[0], Val loss: 0.054198574274778366\n",
      "Epoch[0], Batch[188], Train loss: 0.0566331185400486\n",
      "Epoch[0], Val loss: 0.05732191726565361\n",
      "Epoch[0], Batch[189], Train loss: 0.06237000972032547\n",
      "Epoch[0], Val loss: 0.054181892424821854\n",
      "Epoch[0], Batch[190], Train loss: 0.05775018408894539\n",
      "Epoch[0], Val loss: 0.05082288011908531\n",
      "Epoch[0], Batch[191], Train loss: 0.05564890429377556\n",
      "Epoch[0], Val loss: 0.05747763067483902\n",
      "Epoch[0], Batch[192], Train loss: 0.06387659162282944\n",
      "Epoch[0], Val loss: 0.049725912511348724\n",
      "Epoch[0], Batch[193], Train loss: 0.06002364307641983\n",
      "Epoch[0], Val loss: 0.05386222526431084\n",
      "Epoch[0], Batch[194], Train loss: 0.059437692165374756\n",
      "Epoch[0], Val loss: 0.05358231067657471\n",
      "Epoch[0], Batch[195], Train loss: 0.059651292860507965\n",
      "Epoch[0], Val loss: 0.05395015701651573\n",
      "Epoch[0], Batch[196], Train loss: 0.05987308919429779\n",
      "Epoch[0], Val loss: 0.055359192192554474\n",
      "Epoch[0], Batch[197], Train loss: 0.058636315166950226\n",
      "Epoch[0], Val loss: 0.052209556102752686\n",
      "Epoch[0], Batch[198], Train loss: 0.06007957458496094\n",
      "Epoch[0], Val loss: 0.05561760067939758\n",
      "Epoch[0], Batch[199], Train loss: 0.05584639683365822\n",
      "Epoch[0], Val loss: 0.05366991460323334\n",
      "Epoch[0], Batch[200], Train loss: 0.061673082411289215\n",
      "Epoch[0], Val loss: 0.05503910407423973\n",
      "Epoch[0], Batch[201], Train loss: 0.060537535697221756\n",
      "Epoch[0], Val loss: 0.05372471362352371\n",
      "Epoch[0], Batch[202], Train loss: 0.05995623394846916\n",
      "Epoch[0], Val loss: 0.049558017402887344\n",
      "Epoch[0], Batch[203], Train loss: 0.06105375289916992\n",
      "Epoch[0], Val loss: 0.052646562457084656\n",
      "Epoch[0], Batch[204], Train loss: 0.059054095298051834\n",
      "Epoch[0], Val loss: 0.05220730975270271\n",
      "Epoch[0], Batch[205], Train loss: 0.06377236545085907\n",
      "Epoch[0], Val loss: 0.052556395530700684\n",
      "Epoch[0], Batch[206], Train loss: 0.06004570424556732\n",
      "Epoch[0], Val loss: 0.056519702076911926\n",
      "Epoch[0], Batch[207], Train loss: 0.06541487574577332\n",
      "Epoch[0], Val loss: 0.05345713719725609\n",
      "Epoch[0], Batch[208], Train loss: 0.05827337130904198\n",
      "Epoch[0], Val loss: 0.05457475408911705\n",
      "Epoch[0], Batch[209], Train loss: 0.06760258972644806\n",
      "Epoch[0], Val loss: 0.05411473289132118\n",
      "Epoch[0], Batch[210], Train loss: 0.05899389460682869\n",
      "Epoch[0], Val loss: 0.051826491951942444\n",
      "Epoch[0], Batch[211], Train loss: 0.05658917501568794\n",
      "Epoch[0], Val loss: 0.05248738080263138\n",
      "Epoch[0], Batch[212], Train loss: 0.05930237099528313\n",
      "Epoch[0], Val loss: 0.05514312908053398\n",
      "Epoch[0], Batch[213], Train loss: 0.06220576539635658\n",
      "Epoch[0], Val loss: 0.05427708104252815\n",
      "Epoch[0], Batch[214], Train loss: 0.058299463242292404\n",
      "Epoch[0], Val loss: 0.05591123551130295\n",
      "Epoch[0], Batch[215], Train loss: 0.06204286217689514\n",
      "Epoch[0], Val loss: 0.05277543514966965\n",
      "Epoch[0], Batch[216], Train loss: 0.060463063418865204\n",
      "Epoch[0], Val loss: 0.058810289949178696\n",
      "Epoch[0], Batch[217], Train loss: 0.05990706384181976\n",
      "Epoch[0], Val loss: 0.056195296347141266\n",
      "Epoch[0], Batch[218], Train loss: 0.05951115861535072\n",
      "Epoch[0], Val loss: 0.05467069894075394\n",
      "Epoch[0], Batch[219], Train loss: 0.060309283435344696\n",
      "Epoch[0], Val loss: 0.053437940776348114\n",
      "Epoch[0], Batch[220], Train loss: 0.059304602444171906\n",
      "Epoch[0], Val loss: 0.05371803417801857\n",
      "Epoch[0], Batch[221], Train loss: 0.059709858149290085\n",
      "Epoch[0], Val loss: 0.05489275977015495\n",
      "Epoch[0], Batch[222], Train loss: 0.06298189610242844\n",
      "Epoch[0], Val loss: 0.056613802909851074\n",
      "Epoch[0], Batch[223], Train loss: 0.05902259796857834\n",
      "Epoch[0], Val loss: 0.05477533116936684\n",
      "Epoch[0], Batch[224], Train loss: 0.05782688036561012\n",
      "Epoch[0], Val loss: 0.05410998314619064\n",
      "Epoch[0], Batch[225], Train loss: 0.062419794499874115\n",
      "Epoch[0], Val loss: 0.05349932610988617\n",
      "Epoch[0], Batch[226], Train loss: 0.059097133576869965\n",
      "Epoch[0], Val loss: 0.05426692217588425\n",
      "Epoch[0], Batch[227], Train loss: 0.05964095890522003\n",
      "Epoch[0], Val loss: 0.05730295553803444\n",
      "Epoch[0], Batch[228], Train loss: 0.056472960859537125\n",
      "Epoch[0], Val loss: 0.05503983423113823\n",
      "Epoch[0], Batch[229], Train loss: 0.058881066739559174\n",
      "Epoch[0], Val loss: 0.05674907565116882\n",
      "Epoch[0], Batch[230], Train loss: 0.05816381052136421\n",
      "Epoch[0], Val loss: 0.05474914237856865\n",
      "Epoch[0], Batch[231], Train loss: 0.06182309612631798\n",
      "Epoch[0], Val loss: 0.05146083980798721\n",
      "Epoch[0], Batch[232], Train loss: 0.05807871371507645\n",
      "Epoch[0], Val loss: 0.05713331326842308\n",
      "Epoch[0], Batch[233], Train loss: 0.05740157514810562\n",
      "Epoch[0], Val loss: 0.05231760814785957\n",
      "Epoch[0], Batch[234], Train loss: 0.05796441808342934\n",
      "Epoch[0], Val loss: 0.053047485649585724\n",
      "Epoch[0], Batch[235], Train loss: 0.060269344598054886\n",
      "Epoch[0], Val loss: 0.05929558351635933\n",
      "Epoch[0], Batch[236], Train loss: 0.05724423751235008\n",
      "Epoch[0], Val loss: 0.05319530516862869\n",
      "Epoch[0], Batch[237], Train loss: 0.059730350971221924\n",
      "Epoch[0], Val loss: 0.05199321731925011\n",
      "Epoch[0], Batch[238], Train loss: 0.05908777192234993\n",
      "Epoch[0], Val loss: 0.052091073244810104\n",
      "Epoch[0], Batch[239], Train loss: 0.055122874677181244\n",
      "Epoch[0], Val loss: 0.05312439426779747\n",
      "Epoch[0], Batch[240], Train loss: 0.062404222786426544\n",
      "Epoch[0], Val loss: 0.058971863240003586\n",
      "Epoch[0], Batch[241], Train loss: 0.05780434608459473\n",
      "Epoch[0], Val loss: 0.05452294647693634\n",
      "Epoch[0], Batch[242], Train loss: 0.05587078630924225\n",
      "Epoch[0], Val loss: 0.05335373058915138\n",
      "Epoch[0], Batch[243], Train loss: 0.05958284065127373\n",
      "Epoch[0], Val loss: 0.050449371337890625\n",
      "Epoch[0], Batch[244], Train loss: 0.05891204997897148\n",
      "Epoch[0], Val loss: 0.05497229844331741\n",
      "Epoch[0], Batch[245], Train loss: 0.06268157064914703\n",
      "Epoch[0], Val loss: 0.05844985693693161\n",
      "Epoch[0], Batch[246], Train loss: 0.056370604783296585\n",
      "Epoch[0], Val loss: 0.05367047339677811\n",
      "Epoch[0], Batch[247], Train loss: 0.05853685736656189\n",
      "Epoch[0], Val loss: 0.05528698116540909\n",
      "Epoch[0], Batch[248], Train loss: 0.06245217099785805\n",
      "Epoch[0], Val loss: 0.05603906884789467\n",
      "Epoch[0], Batch[249], Train loss: 0.05901257321238518\n",
      "Epoch[0], Val loss: 0.0577564463019371\n",
      "Epoch[0], Batch[250], Train loss: 0.06069917231798172\n",
      "Epoch[0], Val loss: 0.056092675775289536\n",
      "Epoch[0], Batch[251], Train loss: 0.05584859475493431\n",
      "Epoch[0], Val loss: 0.05437711253762245\n",
      "Epoch[0], Batch[252], Train loss: 0.055062465369701385\n",
      "Epoch[0], Val loss: 0.05417843908071518\n",
      "Epoch[0], Batch[253], Train loss: 0.05931834131479263\n",
      "Epoch[0], Val loss: 0.05765768885612488\n",
      "Epoch[0], Batch[254], Train loss: 0.05598221719264984\n",
      "Epoch[0], Val loss: 0.053875185549259186\n",
      "Epoch[0], Batch[255], Train loss: 0.057500552386045456\n",
      "Epoch[0], Val loss: 0.0560428723692894\n",
      "Epoch[0], Batch[256], Train loss: 0.05855618789792061\n",
      "Epoch[0], Val loss: 0.05299569293856621\n",
      "Epoch[0], Batch[257], Train loss: 0.05916117876768112\n",
      "Epoch[0], Val loss: 0.055674660950899124\n",
      "Epoch[0], Batch[258], Train loss: 0.057763151824474335\n",
      "Epoch[0], Val loss: 0.05049075186252594\n",
      "Epoch[0], Batch[259], Train loss: 0.058238156139850616\n",
      "Epoch[0], Val loss: 0.04969694837927818\n",
      "Epoch[0], Batch[260], Train loss: 0.059118568897247314\n",
      "Epoch[0], Val loss: 0.054530929774045944\n",
      "Epoch[0], Batch[261], Train loss: 0.057245902717113495\n",
      "Epoch[0], Val loss: 0.0543573834002018\n",
      "Epoch[0], Batch[262], Train loss: 0.05911848321557045\n",
      "Epoch[0], Val loss: 0.05408153310418129\n",
      "Epoch[0], Batch[263], Train loss: 0.05628316476941109\n",
      "Epoch[0], Val loss: 0.05204029381275177\n",
      "Epoch[0], Batch[264], Train loss: 0.05740809813141823\n",
      "Epoch[0], Val loss: 0.053060512989759445\n",
      "Epoch[0], Batch[265], Train loss: 0.05367974191904068\n",
      "Epoch[0], Val loss: 0.0561501607298851\n",
      "Epoch[0], Batch[266], Train loss: 0.05611572414636612\n",
      "Epoch[0], Val loss: 0.0526626892387867\n",
      "Epoch[0], Batch[267], Train loss: 0.06079026311635971\n",
      "Epoch[0], Val loss: 0.05146343633532524\n",
      "Epoch[0], Batch[268], Train loss: 0.063319630920887\n",
      "Epoch[0], Val loss: 0.054691288620233536\n",
      "Epoch[0], Batch[269], Train loss: 0.05734439566731453\n",
      "Epoch[0], Val loss: 0.052436985075473785\n",
      "Epoch[0], Batch[270], Train loss: 0.056578122079372406\n",
      "Epoch[0], Val loss: 0.05685877427458763\n",
      "Epoch[0], Batch[271], Train loss: 0.05617065355181694\n",
      "Epoch[0], Val loss: 0.05841631069779396\n",
      "Epoch[0], Batch[272], Train loss: 0.05756482854485512\n",
      "Epoch[0], Val loss: 0.05544571205973625\n",
      "Epoch[0], Batch[273], Train loss: 0.05865471810102463\n",
      "Epoch[0], Val loss: 0.04867012798786163\n",
      "Epoch[0], Batch[274], Train loss: 0.0617002435028553\n",
      "Epoch[0], Val loss: 0.05280289053916931\n",
      "Epoch[0], Batch[275], Train loss: 0.061265356838703156\n",
      "Epoch[0], Val loss: 0.05306055396795273\n",
      "Epoch[0], Batch[276], Train loss: 0.060914866626262665\n",
      "Epoch[0], Val loss: 0.051742419600486755\n",
      "Epoch[0], Batch[277], Train loss: 0.05582808330655098\n",
      "Epoch[0], Val loss: 0.05645079165697098\n",
      "Epoch[0], Batch[278], Train loss: 0.056601785123348236\n",
      "Epoch[0], Val loss: 0.05508881062269211\n",
      "Epoch[0], Batch[279], Train loss: 0.0604780875146389\n",
      "Epoch[0], Val loss: 0.053801532834768295\n",
      "Epoch[0], Batch[280], Train loss: 0.05559874325990677\n",
      "Epoch[0], Val loss: 0.05239952355623245\n",
      "Epoch[0], Batch[281], Train loss: 0.05567087605595589\n",
      "Epoch[0], Val loss: 0.052689723670482635\n",
      "Epoch[0], Batch[282], Train loss: 0.05616533383727074\n",
      "Epoch[0], Val loss: 0.050311796367168427\n",
      "Epoch[0], Batch[283], Train loss: 0.05676255002617836\n",
      "Epoch[0], Val loss: 0.05452796071767807\n",
      "Epoch[0], Batch[284], Train loss: 0.05826054513454437\n",
      "Epoch[0], Val loss: 0.055126313120126724\n",
      "Epoch[0], Batch[285], Train loss: 0.058305155485868454\n",
      "Epoch[0], Val loss: 0.05167543143033981\n",
      "Epoch[0], Batch[286], Train loss: 0.05674053728580475\n",
      "Epoch[0], Val loss: 0.0547790490090847\n",
      "Epoch[0], Batch[287], Train loss: 0.059863679111003876\n",
      "Epoch[0], Val loss: 0.05217183008790016\n",
      "Epoch[0], Batch[288], Train loss: 0.06270688772201538\n",
      "Epoch[0], Val loss: 0.05301355570554733\n",
      "Epoch[0], Batch[289], Train loss: 0.057639092206954956\n",
      "Epoch[0], Val loss: 0.05276932194828987\n",
      "Epoch[0], Batch[290], Train loss: 0.05951425060629845\n",
      "Epoch[0], Val loss: 0.052082810550928116\n",
      "Epoch[0], Batch[291], Train loss: 0.05745492875576019\n",
      "Epoch[0], Val loss: 0.0551053024828434\n",
      "Epoch[0], Batch[292], Train loss: 0.05764766037464142\n",
      "Epoch[0], Val loss: 0.04976562783122063\n",
      "Epoch[0], Batch[293], Train loss: 0.06045394390821457\n",
      "Epoch[0], Val loss: 0.05201916769146919\n",
      "Epoch[0], Batch[294], Train loss: 0.0625099390745163\n",
      "Epoch[0], Val loss: 0.05572712793946266\n",
      "Epoch[0], Batch[295], Train loss: 0.05689395219087601\n",
      "Epoch[0], Val loss: 0.05235082656145096\n",
      "Epoch[0], Batch[296], Train loss: 0.05600768327713013\n",
      "Epoch[0], Val loss: 0.05331575870513916\n",
      "Epoch[0], Batch[297], Train loss: 0.05800474062561989\n",
      "Epoch[0], Val loss: 0.05147140100598335\n",
      "Epoch[0], Batch[298], Train loss: 0.05757851153612137\n",
      "Epoch[0], Val loss: 0.05387846753001213\n",
      "Epoch[0], Batch[299], Train loss: 0.05888470262289047\n",
      "Epoch[0], Val loss: 0.05629926919937134\n",
      "Epoch[0], Batch[300], Train loss: 0.05882863700389862\n",
      "Epoch[0], Val loss: 0.05263349413871765\n",
      "Epoch[0], Batch[301], Train loss: 0.05866695195436478\n",
      "Epoch[0], Val loss: 0.051033489406108856\n",
      "Epoch[0], Batch[302], Train loss: 0.057050783187150955\n",
      "Epoch[0], Val loss: 0.05168106034398079\n",
      "Epoch[0], Batch[303], Train loss: 0.059409260749816895\n",
      "Epoch[0], Val loss: 0.05311236530542374\n",
      "Epoch[0], Batch[304], Train loss: 0.059935592114925385\n",
      "Epoch[0], Val loss: 0.05032774806022644\n",
      "Epoch[0], Batch[305], Train loss: 0.05950804054737091\n",
      "Epoch[0], Val loss: 0.053613875061273575\n",
      "Epoch[0], Batch[306], Train loss: 0.05551036447286606\n",
      "Epoch[0], Val loss: 0.052570998668670654\n",
      "Epoch[0], Batch[307], Train loss: 0.0577068068087101\n",
      "Epoch[0], Val loss: 0.050853680819272995\n",
      "Epoch[0], Batch[308], Train loss: 0.05727015435695648\n",
      "Epoch[0], Val loss: 0.05155511200428009\n",
      "Epoch[0], Batch[309], Train loss: 0.054222963750362396\n",
      "Epoch[0], Val loss: 0.056361496448516846\n",
      "Epoch[0], Batch[310], Train loss: 0.05767800286412239\n",
      "Epoch[0], Val loss: 0.0520852692425251\n",
      "Epoch[0], Batch[311], Train loss: 0.058362528681755066\n",
      "Epoch[0], Val loss: 0.054327383637428284\n",
      "Epoch[0], Batch[312], Train loss: 0.0614054873585701\n",
      "Epoch[0], Val loss: 0.055875420570373535\n",
      "Epoch[0], Batch[313], Train loss: 0.05901395529508591\n",
      "Epoch[0], Val loss: 0.054130248725414276\n",
      "Epoch[0], Batch[314], Train loss: 0.0628540962934494\n",
      "Epoch[0], Val loss: 0.04986705631017685\n",
      "Epoch[0], Batch[315], Train loss: 0.056937992572784424\n",
      "Epoch[0], Val loss: 0.05463400110602379\n",
      "Epoch[0], Batch[316], Train loss: 0.06121581792831421\n",
      "Epoch[0], Val loss: 0.053796298801898956\n",
      "Epoch[0], Batch[317], Train loss: 0.05811791494488716\n",
      "Epoch[0], Val loss: 0.05239926651120186\n",
      "Epoch[0], Batch[318], Train loss: 0.0585370808839798\n",
      "Epoch[0], Val loss: 0.05410243198275566\n",
      "Epoch[0], Batch[319], Train loss: 0.06038638949394226\n",
      "Epoch[0], Val loss: 0.05199754610657692\n",
      "Epoch[0], Batch[320], Train loss: 0.05499417334794998\n",
      "Epoch[0], Val loss: 0.05408049002289772\n",
      "Epoch[0], Batch[321], Train loss: 0.05848470702767372\n",
      "Epoch[0], Val loss: 0.053957343101501465\n",
      "Epoch[0], Batch[322], Train loss: 0.05457371100783348\n",
      "Epoch[0], Val loss: 0.05394235998392105\n",
      "Epoch[0], Batch[323], Train loss: 0.058916669338941574\n",
      "Epoch[0], Val loss: 0.054686274379491806\n",
      "Epoch[0], Batch[324], Train loss: 0.056076403707265854\n",
      "Epoch[0], Val loss: 0.05207329988479614\n",
      "Epoch[0], Batch[325], Train loss: 0.05884777009487152\n",
      "Epoch[0], Val loss: 0.05070924386382103\n",
      "Epoch[0], Batch[326], Train loss: 0.06093687191605568\n",
      "Epoch[0], Val loss: 0.04974987357854843\n",
      "Epoch[0], Batch[327], Train loss: 0.057632867246866226\n",
      "Epoch[0], Val loss: 0.051322706043720245\n",
      "Epoch[0], Batch[328], Train loss: 0.05509388446807861\n",
      "Epoch[0], Val loss: 0.053838327527046204\n",
      "Epoch[0], Batch[329], Train loss: 0.054627835750579834\n",
      "Epoch[0], Val loss: 0.05148851498961449\n",
      "Epoch[0], Batch[330], Train loss: 0.05958090350031853\n",
      "Epoch[0], Val loss: 0.05296560376882553\n",
      "Epoch[0], Batch[331], Train loss: 0.06202850118279457\n",
      "Epoch[0], Val loss: 0.05243578925728798\n",
      "Epoch[0], Batch[332], Train loss: 0.057588476687669754\n",
      "Epoch[0], Val loss: 0.051592934876680374\n",
      "Epoch[0], Batch[333], Train loss: 0.060156431049108505\n",
      "Epoch[0], Val loss: 0.055100031197071075\n",
      "Epoch[0], Batch[334], Train loss: 0.05572846531867981\n",
      "Epoch[0], Val loss: 0.0530196875333786\n",
      "Epoch[0], Batch[335], Train loss: 0.0539771243929863\n",
      "Epoch[0], Val loss: 0.04921827092766762\n",
      "Epoch[0], Batch[336], Train loss: 0.05779809504747391\n",
      "Epoch[0], Val loss: 0.04889664426445961\n",
      "Epoch[0], Batch[337], Train loss: 0.056924887001514435\n",
      "Epoch[0], Val loss: 0.05029384046792984\n",
      "Epoch[0], Batch[338], Train loss: 0.060239680111408234\n",
      "Epoch[0], Val loss: 0.04896247386932373\n",
      "Epoch[0], Batch[339], Train loss: 0.05987317115068436\n",
      "Epoch[0], Val loss: 0.05434969440102577\n",
      "Epoch[0], Batch[340], Train loss: 0.05617949366569519\n",
      "Epoch[0], Val loss: 0.04957408085465431\n",
      "Epoch[0], Batch[341], Train loss: 0.05746794119477272\n",
      "Epoch[0], Val loss: 0.051626402884721756\n",
      "Epoch[0], Batch[342], Train loss: 0.05943664535880089\n",
      "Epoch[0], Val loss: 0.05309431999921799\n",
      "Epoch[0], Batch[343], Train loss: 0.06051590293645859\n",
      "Epoch[0], Val loss: 0.052193786948919296\n",
      "Epoch[0], Batch[344], Train loss: 0.05633673444390297\n",
      "Epoch[0], Val loss: 0.0533006452023983\n",
      "Epoch[0], Batch[345], Train loss: 0.05708800628781319\n",
      "Epoch[0], Val loss: 0.04844358190894127\n",
      "Epoch[0], Batch[346], Train loss: 0.0532553568482399\n",
      "Epoch[0], Val loss: 0.050358545035123825\n",
      "Epoch[0], Batch[347], Train loss: 0.05832213535904884\n",
      "Epoch[0], Val loss: 0.05141085758805275\n",
      "Epoch[0], Batch[348], Train loss: 0.05423698574304581\n",
      "Epoch[0], Val loss: 0.053424905985593796\n",
      "Epoch[0], Batch[349], Train loss: 0.05769691616296768\n",
      "Epoch[0], Val loss: 0.054949093610048294\n",
      "Epoch[0], Batch[350], Train loss: 0.058403026312589645\n",
      "Epoch[0], Val loss: 0.05170566588640213\n",
      "Epoch[0], Batch[351], Train loss: 0.058697376400232315\n",
      "Epoch[0], Val loss: 0.05149846151471138\n",
      "Epoch[0], Batch[352], Train loss: 0.05440323054790497\n",
      "Epoch[0], Val loss: 0.0526825450360775\n",
      "Epoch[0], Batch[353], Train loss: 0.055597737431526184\n",
      "Epoch[0], Val loss: 0.05116315931081772\n",
      "Epoch[0], Batch[354], Train loss: 0.05553502216935158\n",
      "Epoch[0], Val loss: 0.05139705166220665\n",
      "Epoch[0], Batch[355], Train loss: 0.06007101759314537\n",
      "Epoch[0], Val loss: 0.054315995424985886\n",
      "Epoch[0], Batch[356], Train loss: 0.060174956917762756\n",
      "Epoch[0], Val loss: 0.05182555317878723\n",
      "Epoch[0], Batch[357], Train loss: 0.05669395998120308\n",
      "Epoch[0], Val loss: 0.05224099010229111\n",
      "Epoch[0], Batch[358], Train loss: 0.05428623408079147\n",
      "Epoch[0], Val loss: 0.05356440320611\n",
      "Epoch[0], Batch[359], Train loss: 0.057018887251615524\n",
      "Epoch[0], Val loss: 0.0512075237929821\n",
      "Epoch[0], Batch[360], Train loss: 0.058750566095113754\n",
      "Epoch[0], Val loss: 0.05785749852657318\n",
      "Epoch[0], Batch[361], Train loss: 0.05579540878534317\n",
      "Epoch[0], Val loss: 0.050612326711416245\n",
      "Epoch[0], Batch[362], Train loss: 0.05798543617129326\n",
      "Epoch[0], Val loss: 0.05424610525369644\n",
      "Epoch[0], Batch[363], Train loss: 0.057515569031238556\n",
      "Epoch[0], Val loss: 0.0581323504447937\n",
      "Epoch[0], Batch[364], Train loss: 0.056807637214660645\n",
      "Epoch[0], Val loss: 0.047207072377204895\n",
      "Epoch[0], Batch[365], Train loss: 0.054158758372068405\n",
      "Epoch[0], Val loss: 0.05079619586467743\n",
      "Epoch[0], Batch[366], Train loss: 0.05813269317150116\n",
      "Epoch[0], Val loss: 0.05149786174297333\n",
      "Epoch[0], Batch[367], Train loss: 0.05699550732970238\n",
      "Epoch[0], Val loss: 0.054153215140104294\n",
      "Epoch[0], Batch[368], Train loss: 0.05333292484283447\n",
      "Epoch[0], Val loss: 0.050349920988082886\n",
      "Epoch[0], Batch[369], Train loss: 0.058369867503643036\n",
      "Epoch[0], Val loss: 0.049289051443338394\n",
      "Epoch[0], Batch[370], Train loss: 0.0567871630191803\n",
      "Epoch[0], Val loss: 0.05404713749885559\n",
      "Epoch[0], Batch[371], Train loss: 0.05888683721423149\n",
      "Epoch[0], Val loss: 0.0513356551527977\n",
      "Epoch[0], Batch[372], Train loss: 0.05727430433034897\n",
      "Epoch[0], Val loss: 0.05333254113793373\n",
      "Epoch[0], Batch[373], Train loss: 0.05855034291744232\n",
      "Epoch[0], Val loss: 0.05444766953587532\n",
      "Epoch[0], Batch[374], Train loss: 0.051950421184301376\n",
      "Epoch[0], Val loss: 0.05394525080919266\n",
      "Epoch[0], Batch[375], Train loss: 0.056635476648807526\n",
      "Epoch[0], Val loss: 0.052382662892341614\n",
      "Epoch[0], Batch[376], Train loss: 0.059679437428712845\n",
      "Epoch[0], Val loss: 0.0513000525534153\n",
      "Epoch[0], Batch[377], Train loss: 0.054203715175390244\n",
      "Epoch[0], Val loss: 0.05033659189939499\n",
      "Epoch[0], Batch[378], Train loss: 0.0538986399769783\n",
      "Epoch[0], Val loss: 0.0552646666765213\n",
      "Epoch[0], Batch[379], Train loss: 0.054527267813682556\n",
      "Epoch[0], Val loss: 0.05525368079543114\n",
      "Epoch[0], Batch[380], Train loss: 0.055321890860795975\n",
      "Epoch[0], Val loss: 0.04946925863623619\n",
      "Epoch[0], Batch[381], Train loss: 0.055417340248823166\n",
      "Epoch[0], Val loss: 0.05262158438563347\n",
      "Epoch[0], Batch[382], Train loss: 0.05593523010611534\n",
      "Epoch[0], Val loss: 0.051456138491630554\n",
      "Epoch[0], Batch[383], Train loss: 0.05713273212313652\n",
      "Epoch[0], Val loss: 0.05347878113389015\n",
      "Epoch[0], Batch[384], Train loss: 0.05389084294438362\n",
      "Epoch[0], Val loss: 0.0521368570625782\n",
      "Epoch[0], Batch[385], Train loss: 0.05807914212346077\n",
      "Epoch[0], Val loss: 0.052753034979104996\n",
      "Epoch[0], Batch[386], Train loss: 0.05472993850708008\n",
      "Epoch[0], Val loss: 0.05415404960513115\n",
      "Epoch[0], Batch[387], Train loss: 0.06126776710152626\n",
      "Epoch[0], Val loss: 0.05402621626853943\n",
      "Epoch[0], Batch[388], Train loss: 0.05611591786146164\n",
      "Epoch[0], Val loss: 0.05443890392780304\n",
      "Epoch[0], Batch[389], Train loss: 0.056066613644361496\n",
      "Epoch[0], Val loss: 0.053497035056352615\n",
      "Epoch[0], Batch[390], Train loss: 0.05666203051805496\n",
      "Epoch[0], Val loss: 0.05401061102747917\n",
      "Epoch[0], Batch[391], Train loss: 0.058958448469638824\n",
      "Epoch[0], Val loss: 0.05479194223880768\n",
      "Epoch[0], Batch[392], Train loss: 0.053959161043167114\n",
      "Epoch[0], Val loss: 0.047494277358055115\n",
      "Epoch[0], Batch[393], Train loss: 0.05539416894316673\n",
      "Epoch[0], Val loss: 0.05052775517106056\n",
      "Epoch[0], Batch[394], Train loss: 0.05889207869768143\n",
      "Epoch[0], Val loss: 0.04691990464925766\n",
      "Epoch[0], Batch[395], Train loss: 0.056431859731674194\n",
      "Epoch[0], Val loss: 0.05118226259946823\n",
      "Epoch[0], Batch[396], Train loss: 0.05755607783794403\n",
      "Epoch[0], Val loss: 0.050886038690805435\n",
      "Epoch[0], Batch[397], Train loss: 0.055712807923555374\n",
      "Epoch[0], Val loss: 0.05216749384999275\n",
      "Epoch[0], Batch[398], Train loss: 0.05687646195292473\n",
      "Epoch[0], Val loss: 0.051408637315034866\n",
      "Epoch[0], Batch[399], Train loss: 0.05515311658382416\n",
      "Epoch[0], Val loss: 0.05252378433942795\n",
      "Epoch[0], Batch[400], Train loss: 0.053280193358659744\n",
      "Epoch[0], Val loss: 0.05397670343518257\n",
      "Epoch[0], Batch[401], Train loss: 0.055595796555280685\n",
      "Epoch[0], Val loss: 0.050872553139925\n",
      "Epoch[0], Batch[402], Train loss: 0.05703257769346237\n",
      "Epoch[0], Val loss: 0.0504155233502388\n",
      "Epoch[0], Batch[403], Train loss: 0.05691175535321236\n",
      "Epoch[0], Val loss: 0.0495324581861496\n",
      "Epoch[0], Batch[404], Train loss: 0.05345458909869194\n",
      "Epoch[0], Val loss: 0.05074092373251915\n",
      "Epoch[0], Batch[405], Train loss: 0.05742918699979782\n",
      "Epoch[0], Val loss: 0.05179079249501228\n",
      "Epoch[0], Batch[406], Train loss: 0.057311754673719406\n",
      "Epoch[0], Val loss: 0.048744041472673416\n",
      "Epoch[0], Batch[407], Train loss: 0.05605952441692352\n",
      "Epoch[0], Val loss: 0.05339004471898079\n",
      "Epoch[0], Batch[408], Train loss: 0.05681672692298889\n",
      "Epoch[0], Val loss: 0.050042107701301575\n",
      "Epoch[0], Batch[409], Train loss: 0.051841553300619125\n",
      "Epoch[0], Val loss: 0.0500849187374115\n",
      "Epoch[0], Batch[410], Train loss: 0.055681128054857254\n",
      "Epoch[0], Val loss: 0.052598364651203156\n",
      "Epoch[0], Batch[411], Train loss: 0.05971144512295723\n",
      "Epoch[0], Val loss: 0.05322457104921341\n",
      "Epoch[0], Batch[412], Train loss: 0.05489635095000267\n",
      "Epoch[0], Val loss: 0.055657289922237396\n",
      "Epoch[0], Batch[413], Train loss: 0.05665390193462372\n",
      "Epoch[0], Val loss: 0.05250565707683563\n",
      "Epoch[0], Batch[414], Train loss: 0.056201860308647156\n",
      "Epoch[0], Val loss: 0.05302400141954422\n",
      "Epoch[0], Batch[415], Train loss: 0.05715871974825859\n",
      "Epoch[0], Val loss: 0.049859046936035156\n",
      "Epoch[0], Batch[416], Train loss: 0.05632202327251434\n",
      "Epoch[0], Val loss: 0.055416516959667206\n",
      "Epoch[0], Batch[417], Train loss: 0.05313527211546898\n",
      "Epoch[0], Val loss: 0.050668589770793915\n",
      "Epoch[0], Batch[418], Train loss: 0.05298364907503128\n",
      "Epoch[0], Val loss: 0.05109415203332901\n",
      "Epoch[0], Batch[419], Train loss: 0.057630300521850586\n",
      "Epoch[0], Val loss: 0.05243126302957535\n",
      "Epoch[0], Batch[420], Train loss: 0.058827273547649384\n",
      "Epoch[0], Val loss: 0.05304495617747307\n",
      "Epoch[0], Batch[421], Train loss: 0.056732144206762314\n",
      "Epoch[0], Val loss: 0.051733944565057755\n",
      "Epoch[0], Batch[422], Train loss: 0.05717294663190842\n",
      "Epoch[0], Val loss: 0.04843634366989136\n",
      "Epoch[0], Batch[423], Train loss: 0.05514996498823166\n",
      "Epoch[0], Val loss: 0.049999840557575226\n",
      "Epoch[0], Batch[424], Train loss: 0.054112572222948074\n",
      "Epoch[0], Val loss: 0.050958868116140366\n",
      "Epoch[0], Batch[425], Train loss: 0.058498166501522064\n",
      "Epoch[0], Val loss: 0.05339421331882477\n",
      "Epoch[0], Batch[426], Train loss: 0.057119470089673996\n",
      "Epoch[0], Val loss: 0.0503305085003376\n",
      "Epoch[0], Batch[427], Train loss: 0.052690040320158005\n",
      "Epoch[0], Val loss: 0.05106283724308014\n",
      "Epoch[0], Batch[428], Train loss: 0.05264119431376457\n",
      "Epoch[0], Val loss: 0.0547410249710083\n",
      "Epoch[0], Batch[429], Train loss: 0.05421382933855057\n",
      "Epoch[0], Val loss: 0.05226625129580498\n",
      "Epoch[0], Batch[430], Train loss: 0.05480483919382095\n",
      "Epoch[0], Val loss: 0.054425112903118134\n",
      "Epoch[0], Batch[431], Train loss: 0.053085505962371826\n",
      "Epoch[0], Val loss: 0.05333380773663521\n",
      "Epoch[0], Batch[432], Train loss: 0.05266410857439041\n",
      "Epoch[0], Val loss: 0.051285870373249054\n",
      "Epoch[0], Batch[433], Train loss: 0.05449896305799484\n",
      "Epoch[0], Val loss: 0.05323115363717079\n",
      "Epoch[0], Batch[434], Train loss: 0.055791232734918594\n",
      "Epoch[0], Val loss: 0.05175744742155075\n",
      "Epoch[0], Batch[435], Train loss: 0.05553226172924042\n",
      "Epoch[0], Val loss: 0.04979245364665985\n",
      "Epoch[0], Batch[436], Train loss: 0.05538475885987282\n",
      "Epoch[0], Val loss: 0.050628662109375\n",
      "Epoch[0], Batch[437], Train loss: 0.059043798595666885\n",
      "Epoch[0], Val loss: 0.053741756826639175\n",
      "Epoch[0], Batch[438], Train loss: 0.057149045169353485\n",
      "Epoch[0], Val loss: 0.054271385073661804\n",
      "Epoch[0], Batch[439], Train loss: 0.05504625290632248\n",
      "Epoch[0], Val loss: 0.047474756836891174\n",
      "Epoch[0], Batch[440], Train loss: 0.05616527423262596\n",
      "Epoch[0], Val loss: 0.049401458352804184\n",
      "Epoch[0], Batch[441], Train loss: 0.0582423135638237\n",
      "Epoch[0], Val loss: 0.05014462023973465\n",
      "Epoch[0], Batch[442], Train loss: 0.056366778910160065\n",
      "Epoch[0], Val loss: 0.04829639941453934\n",
      "Epoch[0], Batch[443], Train loss: 0.051818180829286575\n",
      "Epoch[0], Val loss: 0.0499502569437027\n",
      "Epoch[0], Batch[444], Train loss: 0.057334888726472855\n",
      "Epoch[0], Val loss: 0.051258936524391174\n",
      "Epoch[0], Batch[445], Train loss: 0.05286600440740585\n",
      "Epoch[0], Val loss: 0.050793737173080444\n",
      "Epoch[0], Batch[446], Train loss: 0.05498708784580231\n",
      "Epoch[0], Val loss: 0.052361998707056046\n",
      "Epoch[0], Batch[447], Train loss: 0.05535581707954407\n",
      "Epoch[0], Val loss: 0.052139297127723694\n",
      "Epoch[0], Batch[448], Train loss: 0.05610346794128418\n",
      "Epoch[0], Val loss: 0.050407301634550095\n",
      "Epoch[0], Batch[449], Train loss: 0.05829089507460594\n",
      "Epoch[0], Val loss: 0.05466903746128082\n",
      "Epoch[0], Batch[450], Train loss: 0.055347222834825516\n",
      "Epoch[0], Val loss: 0.05127730220556259\n",
      "Epoch[0], Batch[451], Train loss: 0.05622220039367676\n",
      "Epoch[0], Val loss: 0.05092550069093704\n",
      "Epoch[0], Batch[452], Train loss: 0.05676747485995293\n",
      "Epoch[0], Val loss: 0.04990091547369957\n",
      "Epoch[0], Batch[453], Train loss: 0.05815157666802406\n",
      "Epoch[0], Val loss: 0.0515441857278347\n",
      "Epoch[0], Batch[454], Train loss: 0.05461129546165466\n",
      "Epoch[0], Val loss: 0.05137117579579353\n",
      "Epoch[0], Batch[455], Train loss: 0.054975200444459915\n",
      "Epoch[0], Val loss: 0.05327199772000313\n",
      "Epoch[0], Batch[456], Train loss: 0.05774214118719101\n",
      "Epoch[0], Val loss: 0.05653955787420273\n",
      "Epoch[0], Batch[457], Train loss: 0.054283108562231064\n",
      "Epoch[0], Val loss: 0.05280759185552597\n",
      "Epoch[0], Batch[458], Train loss: 0.05520058423280716\n",
      "Epoch[0], Val loss: 0.05202880874276161\n",
      "Epoch[0], Batch[459], Train loss: 0.05679231509566307\n",
      "Epoch[0], Val loss: 0.05210111662745476\n",
      "Epoch[0], Batch[460], Train loss: 0.05649767071008682\n",
      "Epoch[0], Val loss: 0.051768265664577484\n",
      "Epoch[0], Batch[461], Train loss: 0.05843425169587135\n",
      "Epoch[0], Val loss: 0.05416154861450195\n",
      "Epoch[0], Batch[462], Train loss: 0.0572977177798748\n",
      "Epoch[0], Val loss: 0.051621340215206146\n",
      "Epoch[0], Batch[463], Train loss: 0.05315358191728592\n",
      "Epoch[0], Val loss: 0.05300569534301758\n",
      "Epoch[0], Batch[464], Train loss: 0.05522005632519722\n",
      "Epoch[0], Val loss: 0.05184439569711685\n",
      "Epoch[0], Batch[465], Train loss: 0.054249975830316544\n",
      "Epoch[0], Val loss: 0.05107731372117996\n",
      "Epoch[0], Batch[466], Train loss: 0.05664371699094772\n",
      "Epoch[0], Val loss: 0.05274522677063942\n",
      "Epoch[0], Batch[467], Train loss: 0.05807806923985481\n",
      "Epoch[0], Val loss: 0.05084054172039032\n",
      "Epoch[0], Batch[468], Train loss: 0.05437268689274788\n",
      "Epoch[0], Val loss: 0.05218476057052612\n",
      "Epoch[0], Batch[469], Train loss: 0.05335218831896782\n",
      "Epoch[0], Val loss: 0.052504442632198334\n",
      "Epoch[0], Batch[470], Train loss: 0.0537746399641037\n",
      "Epoch[0], Val loss: 0.051845502108335495\n",
      "Epoch[0], Batch[471], Train loss: 0.055721573531627655\n",
      "Epoch[0], Val loss: 0.049755923449993134\n",
      "Epoch[0], Batch[472], Train loss: 0.051923878490924835\n",
      "Epoch[0], Val loss: 0.05191412195563316\n",
      "Epoch[0], Batch[473], Train loss: 0.05400353670120239\n",
      "Epoch[0], Val loss: 0.05219317600131035\n",
      "Epoch[0], Batch[474], Train loss: 0.05564909055829048\n",
      "Epoch[0], Val loss: 0.051916949450969696\n",
      "Epoch[0], Batch[475], Train loss: 0.05441097915172577\n",
      "Epoch[0], Val loss: 0.05046103894710541\n",
      "Epoch[0], Batch[476], Train loss: 0.05497859790921211\n",
      "Epoch[0], Val loss: 0.050565481185913086\n",
      "Epoch[0], Batch[477], Train loss: 0.053182024508714676\n",
      "Epoch[0], Val loss: 0.04994107037782669\n",
      "Epoch[0], Batch[478], Train loss: 0.05558481812477112\n",
      "Epoch[0], Val loss: 0.05397389829158783\n",
      "Epoch[0], Batch[479], Train loss: 0.05571509897708893\n",
      "Epoch[0], Val loss: 0.05284963175654411\n",
      "Epoch[0], Batch[480], Train loss: 0.05402382090687752\n",
      "Epoch[0], Val loss: 0.05060287192463875\n",
      "Epoch[0], Batch[481], Train loss: 0.05656680837273598\n",
      "Epoch[0], Val loss: 0.04924960806965828\n",
      "Epoch[0], Batch[482], Train loss: 0.052349671721458435\n",
      "Epoch[0], Val loss: 0.05206656828522682\n",
      "Epoch[0], Batch[483], Train loss: 0.05779319629073143\n",
      "Epoch[0], Val loss: 0.05081314593553543\n",
      "Epoch[0], Batch[484], Train loss: 0.05523969233036041\n",
      "Epoch[0], Val loss: 0.050089579075574875\n",
      "Epoch[0], Batch[485], Train loss: 0.052388809621334076\n",
      "Epoch[0], Val loss: 0.04628406837582588\n",
      "Epoch[0], Batch[486], Train loss: 0.056378770619630814\n",
      "Epoch[0], Val loss: 0.05330522358417511\n",
      "Epoch[0], Batch[487], Train loss: 0.05536024644970894\n",
      "Epoch[0], Val loss: 0.04921974241733551\n",
      "Epoch[0], Batch[488], Train loss: 0.05835133045911789\n",
      "Epoch[0], Val loss: 0.051599908620119095\n",
      "Epoch[0], Batch[489], Train loss: 0.05205316096544266\n",
      "Epoch[0], Val loss: 0.052574947476387024\n",
      "Epoch[0], Batch[490], Train loss: 0.05659837648272514\n",
      "Epoch[0], Val loss: 0.05220702663064003\n",
      "Epoch[0], Batch[491], Train loss: 0.054299548268318176\n",
      "Epoch[0], Val loss: 0.049613241106271744\n",
      "Epoch[0], Batch[492], Train loss: 0.052852362394332886\n",
      "Epoch[0], Val loss: 0.05189939960837364\n",
      "Epoch[0], Batch[493], Train loss: 0.05602365732192993\n",
      "Epoch[0], Val loss: 0.05083910748362541\n",
      "Epoch[0], Batch[494], Train loss: 0.05462837964296341\n",
      "Epoch[0], Val loss: 0.04982803761959076\n",
      "Epoch[0], Batch[495], Train loss: 0.05468311905860901\n",
      "Epoch[0], Val loss: 0.051105089485645294\n",
      "Epoch[0], Batch[496], Train loss: 0.054494962096214294\n",
      "Epoch[0], Val loss: 0.04970507323741913\n",
      "Epoch[0], Batch[497], Train loss: 0.056842491030693054\n",
      "Epoch[0], Val loss: 0.053940318524837494\n",
      "Epoch[0], Batch[498], Train loss: 0.055337678641080856\n",
      "Epoch[0], Val loss: 0.04792264848947525\n",
      "Epoch[0], Batch[499], Train loss: 0.05286567658185959\n",
      "Epoch[0], Val loss: 0.04834526404738426\n",
      "Epoch[0], Batch[500], Train loss: 0.05702664703130722\n",
      "Epoch[0], Val loss: 0.051620569080114365\n",
      "Epoch[0], Batch[501], Train loss: 0.05159221589565277\n",
      "Epoch[0], Val loss: 0.050961628556251526\n",
      "Epoch[0], Batch[502], Train loss: 0.05471469834446907\n",
      "Epoch[0], Val loss: 0.052159927785396576\n",
      "Epoch[0], Batch[503], Train loss: 0.05512828752398491\n",
      "Epoch[0], Val loss: 0.05265719071030617\n",
      "Epoch[0], Batch[504], Train loss: 0.05370163172483444\n",
      "Epoch[0], Val loss: 0.050216179341077805\n",
      "Epoch[0], Batch[505], Train loss: 0.053215403109788895\n",
      "Epoch[0], Val loss: 0.04795067384839058\n",
      "Epoch[0], Batch[506], Train loss: 0.053502969443798065\n",
      "Epoch[0], Val loss: 0.04993932321667671\n",
      "Epoch[0], Batch[507], Train loss: 0.05492160841822624\n",
      "Epoch[0], Val loss: 0.052468400448560715\n",
      "Epoch[0], Batch[508], Train loss: 0.05188592150807381\n",
      "Epoch[0], Val loss: 0.048236142843961716\n",
      "Epoch[0], Batch[509], Train loss: 0.05420936271548271\n",
      "Epoch[0], Val loss: 0.04840002581477165\n",
      "Epoch[0], Batch[510], Train loss: 0.053914569318294525\n",
      "Epoch[0], Val loss: 0.048898909240961075\n",
      "Epoch[0], Batch[511], Train loss: 0.053872592747211456\n",
      "Epoch[0], Val loss: 0.052807364612817764\n",
      "Epoch[0], Batch[512], Train loss: 0.05990497022867203\n",
      "Epoch[0], Val loss: 0.04930545389652252\n",
      "Epoch[0], Batch[513], Train loss: 0.05921858176589012\n",
      "Epoch[0], Val loss: 0.04857150465250015\n",
      "Epoch[0], Batch[514], Train loss: 0.05440760403871536\n",
      "Epoch[0], Val loss: 0.048648688942193985\n",
      "Epoch[0], Batch[515], Train loss: 0.05305466800928116\n",
      "Epoch[0], Val loss: 0.050060566514730453\n",
      "Epoch[0], Batch[516], Train loss: 0.05497531220316887\n",
      "Epoch[0], Val loss: 0.04691314324736595\n",
      "Epoch[0], Batch[517], Train loss: 0.05110015720129013\n",
      "Epoch[0], Val loss: 0.055762115865945816\n",
      "Epoch[0], Batch[518], Train loss: 0.05108876898884773\n",
      "Epoch[0], Val loss: 0.04816236346960068\n",
      "Epoch[0], Batch[519], Train loss: 0.05399779975414276\n",
      "Epoch[0], Val loss: 0.053270772099494934\n",
      "Epoch[0], Batch[520], Train loss: 0.05450579524040222\n",
      "Epoch[0], Val loss: 0.05171169713139534\n",
      "Epoch[0], Batch[521], Train loss: 0.05436548963189125\n",
      "Epoch[0], Val loss: 0.05233798548579216\n",
      "Epoch[0], Batch[522], Train loss: 0.05180130898952484\n",
      "Epoch[0], Val loss: 0.05353180691599846\n",
      "Epoch[0], Batch[523], Train loss: 0.05395906791090965\n",
      "Epoch[0], Val loss: 0.05209988355636597\n",
      "Epoch[0], Batch[524], Train loss: 0.05371030047535896\n",
      "Epoch[0], Val loss: 0.051975566893815994\n",
      "Epoch[0], Batch[525], Train loss: 0.05291450396180153\n",
      "Epoch[0], Val loss: 0.05388939008116722\n",
      "Epoch[0], Batch[526], Train loss: 0.054573118686676025\n",
      "Epoch[0], Val loss: 0.051698215305805206\n",
      "Epoch[0], Batch[527], Train loss: 0.04972107708454132\n",
      "Epoch[0], Val loss: 0.04893820360302925\n",
      "Epoch[0], Batch[528], Train loss: 0.054621849209070206\n",
      "Epoch[0], Val loss: 0.05196039751172066\n",
      "Epoch[0], Batch[529], Train loss: 0.05429019033908844\n",
      "Epoch[0], Val loss: 0.04764258861541748\n",
      "Epoch[0], Batch[530], Train loss: 0.05847851186990738\n",
      "Epoch[0], Val loss: 0.04873376712203026\n",
      "Epoch[0], Batch[531], Train loss: 0.05435940995812416\n",
      "Epoch[0], Val loss: 0.051080383360385895\n",
      "Epoch[0], Batch[532], Train loss: 0.05489261448383331\n",
      "Epoch[0], Val loss: 0.04847808927297592\n",
      "Epoch[0], Batch[533], Train loss: 0.05754022300243378\n",
      "Epoch[0], Val loss: 0.04833991080522537\n",
      "Epoch[0], Batch[534], Train loss: 0.051925379782915115\n",
      "Epoch[0], Val loss: 0.04952903091907501\n",
      "Epoch[0], Batch[535], Train loss: 0.055158451199531555\n",
      "Epoch[0], Val loss: 0.04950467869639397\n",
      "Epoch[0], Batch[536], Train loss: 0.056375082582235336\n",
      "Epoch[0], Val loss: 0.04825955629348755\n",
      "Epoch[0], Batch[537], Train loss: 0.05427416414022446\n",
      "Epoch[0], Val loss: 0.05223517492413521\n",
      "Epoch[0], Batch[538], Train loss: 0.05502553656697273\n",
      "Epoch[0], Val loss: 0.04773177206516266\n",
      "Epoch[0], Batch[539], Train loss: 0.0506112240254879\n",
      "Epoch[0], Val loss: 0.05431094393134117\n",
      "Epoch[0], Batch[540], Train loss: 0.05379167199134827\n",
      "Epoch[0], Val loss: 0.04844945669174194\n",
      "Epoch[0], Batch[541], Train loss: 0.05499039217829704\n",
      "Epoch[0], Val loss: 0.05406196042895317\n",
      "Epoch[0], Batch[542], Train loss: 0.053390275686979294\n",
      "Epoch[0], Val loss: 0.04812943935394287\n",
      "Epoch[0], Batch[543], Train loss: 0.052240435034036636\n",
      "Epoch[0], Val loss: 0.04668813943862915\n",
      "Epoch[0], Batch[544], Train loss: 0.05101493373513222\n",
      "Epoch[0], Val loss: 0.04985528066754341\n",
      "Epoch[0], Batch[545], Train loss: 0.05763673782348633\n",
      "Epoch[0], Val loss: 0.05386526882648468\n",
      "Epoch[0], Batch[546], Train loss: 0.05659450590610504\n",
      "Epoch[0], Val loss: 0.04910719394683838\n",
      "Epoch[0], Batch[547], Train loss: 0.05232376977801323\n",
      "Epoch[0], Val loss: 0.048697732388973236\n",
      "Epoch[0], Batch[548], Train loss: 0.054343532770872116\n",
      "Epoch[0], Val loss: 0.04983197897672653\n",
      "Epoch[0], Batch[549], Train loss: 0.054375551640987396\n",
      "Epoch[0], Val loss: 0.05205559358000755\n",
      "Epoch[0], Batch[550], Train loss: 0.05476243421435356\n",
      "Epoch[0], Val loss: 0.050990767776966095\n",
      "Epoch[0], Batch[551], Train loss: 0.05606481432914734\n",
      "Epoch[0], Val loss: 0.05343811213970184\n",
      "Epoch[0], Batch[552], Train loss: 0.05373239144682884\n",
      "Epoch[0], Val loss: 0.04915836453437805\n",
      "Epoch[0], Batch[553], Train loss: 0.05350024998188019\n",
      "Epoch[0], Val loss: 0.04933823645114899\n",
      "Epoch[0], Batch[554], Train loss: 0.05497610941529274\n",
      "Epoch[0], Val loss: 0.05084734410047531\n",
      "Epoch[0], Batch[555], Train loss: 0.05122331902384758\n",
      "Epoch[0], Val loss: 0.05215882137417793\n",
      "Epoch[0], Batch[556], Train loss: 0.051867835223674774\n",
      "Epoch[0], Val loss: 0.04997242987155914\n",
      "Epoch[0], Batch[557], Train loss: 0.051535677164793015\n",
      "Epoch[0], Val loss: 0.05002142861485481\n",
      "Epoch[0], Batch[558], Train loss: 0.05187748372554779\n",
      "Epoch[0], Val loss: 0.04960674047470093\n",
      "Epoch[0], Batch[559], Train loss: 0.05388396978378296\n",
      "Epoch[0], Val loss: 0.04945134371519089\n",
      "Epoch[0], Batch[560], Train loss: 0.053156547248363495\n",
      "Epoch[0], Val loss: 0.05153832584619522\n",
      "Epoch[0], Batch[561], Train loss: 0.053651660680770874\n",
      "Epoch[0], Val loss: 0.049287132918834686\n",
      "Epoch[0], Batch[562], Train loss: 0.05678000673651695\n",
      "Epoch[0], Val loss: 0.04744600132107735\n",
      "Epoch[0], Batch[563], Train loss: 0.05685161426663399\n",
      "Epoch[0], Val loss: 0.054558880627155304\n",
      "Epoch[0], Batch[564], Train loss: 0.05275497958064079\n",
      "Epoch[0], Val loss: 0.04839466139674187\n",
      "Epoch[0], Batch[565], Train loss: 0.05526282638311386\n",
      "Epoch[0], Val loss: 0.05091508477926254\n",
      "Epoch[0], Batch[566], Train loss: 0.051956355571746826\n",
      "Epoch[0], Val loss: 0.047654103487730026\n",
      "Epoch[0], Batch[567], Train loss: 0.052894242107868195\n",
      "Epoch[0], Val loss: 0.04849807545542717\n",
      "Epoch[0], Batch[568], Train loss: 0.053164031356573105\n",
      "Epoch[0], Val loss: 0.05202050879597664\n",
      "Epoch[0], Batch[569], Train loss: 0.052749160677194595\n",
      "Epoch[0], Val loss: 0.05139001086354256\n",
      "Epoch[0], Batch[570], Train loss: 0.05544020235538483\n",
      "Epoch[0], Val loss: 0.051903508603572845\n",
      "Epoch[0], Batch[571], Train loss: 0.05230754241347313\n",
      "Epoch[0], Val loss: 0.04874721169471741\n",
      "Epoch[0], Batch[572], Train loss: 0.049738917499780655\n",
      "Epoch[0], Val loss: 0.05151045322418213\n",
      "Epoch[0], Batch[573], Train loss: 0.05161989852786064\n",
      "Epoch[0], Val loss: 0.04857395961880684\n",
      "Epoch[0], Batch[574], Train loss: 0.056645721197128296\n",
      "Epoch[0], Val loss: 0.046243056654930115\n",
      "Epoch[0], Batch[575], Train loss: 0.05322615057229996\n",
      "Epoch[0], Val loss: 0.04833311587572098\n",
      "Epoch[0], Batch[576], Train loss: 0.05458279326558113\n",
      "Epoch[0], Val loss: 0.048788413405418396\n",
      "Epoch[0], Batch[577], Train loss: 0.05189032852649689\n",
      "Epoch[0], Val loss: 0.04702068120241165\n",
      "Epoch[0], Batch[578], Train loss: 0.05128803849220276\n",
      "Epoch[0], Val loss: 0.0497874915599823\n",
      "Epoch[0], Batch[579], Train loss: 0.051035623997449875\n",
      "Epoch[0], Val loss: 0.05244800075888634\n",
      "Epoch[0], Batch[580], Train loss: 0.05526004359126091\n",
      "Epoch[0], Val loss: 0.047673922032117844\n",
      "Epoch[0], Batch[581], Train loss: 0.05082191526889801\n",
      "Epoch[0], Val loss: 0.05103077366948128\n",
      "Epoch[0], Batch[582], Train loss: 0.05039733275771141\n",
      "Epoch[0], Val loss: 0.04942227900028229\n",
      "Epoch[0], Batch[583], Train loss: 0.053430285304784775\n",
      "Epoch[0], Val loss: 0.050085779279470444\n",
      "Epoch[0], Batch[584], Train loss: 0.053455088287591934\n",
      "Epoch[0], Val loss: 0.04956307262182236\n",
      "Epoch[0], Batch[585], Train loss: 0.052434708923101425\n",
      "Epoch[0], Val loss: 0.04949784651398659\n",
      "Epoch[0], Batch[586], Train loss: 0.05520135909318924\n",
      "Epoch[0], Val loss: 0.050275664776563644\n",
      "Epoch[0], Batch[587], Train loss: 0.052131377160549164\n",
      "Epoch[0], Val loss: 0.048886388540267944\n",
      "Epoch[0], Batch[588], Train loss: 0.05228818207979202\n",
      "Epoch[0], Val loss: 0.04667101055383682\n",
      "Epoch[0], Batch[589], Train loss: 0.050790298730134964\n",
      "Epoch[0], Val loss: 0.048590563237667084\n",
      "Epoch[0], Batch[590], Train loss: 0.051974933594465256\n",
      "Epoch[0], Val loss: 0.05421420559287071\n",
      "Epoch[0], Batch[591], Train loss: 0.049546193331480026\n",
      "Epoch[0], Val loss: 0.047971367835998535\n",
      "Epoch[0], Batch[592], Train loss: 0.05243134871125221\n",
      "Epoch[0], Val loss: 0.04789365082979202\n",
      "Epoch[0], Batch[593], Train loss: 0.05421438440680504\n",
      "Epoch[0], Val loss: 0.051513951271772385\n",
      "Epoch[0], Batch[594], Train loss: 0.05617917701601982\n",
      "Epoch[0], Val loss: 0.04877346381545067\n",
      "Epoch[0], Batch[595], Train loss: 0.05153834447264671\n",
      "Epoch[0], Val loss: 0.051016949117183685\n",
      "Epoch[0], Batch[596], Train loss: 0.057534728199243546\n",
      "Epoch[0], Val loss: 0.04724811762571335\n",
      "Epoch[0], Batch[597], Train loss: 0.053129881620407104\n",
      "Epoch[0], Val loss: 0.04942170903086662\n",
      "Epoch[0], Batch[598], Train loss: 0.052553609013557434\n",
      "Epoch[0], Val loss: 0.04738026484847069\n",
      "Epoch[0], Batch[599], Train loss: 0.05225607380270958\n",
      "Epoch[0], Val loss: 0.04938177764415741\n",
      "Epoch[0], Batch[600], Train loss: 0.054855089634656906\n",
      "Epoch[0], Val loss: 0.047556646168231964\n",
      "Epoch[0], Batch[601], Train loss: 0.052184585481882095\n",
      "Epoch[0], Val loss: 0.051338668912649155\n",
      "Epoch[0], Batch[602], Train loss: 0.05415477976202965\n",
      "Epoch[0], Val loss: 0.052244871854782104\n",
      "Epoch[0], Batch[603], Train loss: 0.05430954322218895\n",
      "Epoch[0], Val loss: 0.051615189760923386\n",
      "Epoch[0], Batch[604], Train loss: 0.05340446159243584\n",
      "Epoch[0], Val loss: 0.05390593037009239\n",
      "Epoch[0], Batch[605], Train loss: 0.050263721495866776\n",
      "Epoch[0], Val loss: 0.04885105416178703\n",
      "Epoch[0], Batch[606], Train loss: 0.05261055380105972\n",
      "Epoch[0], Val loss: 0.05059017241001129\n",
      "Epoch[0], Batch[607], Train loss: 0.053107477724552155\n",
      "Epoch[0], Val loss: 0.050643183290958405\n",
      "Epoch[0], Batch[608], Train loss: 0.05034128949046135\n",
      "Epoch[0], Val loss: 0.048212289810180664\n",
      "Epoch[0], Batch[609], Train loss: 0.05590822547674179\n",
      "Epoch[0], Val loss: 0.05089655891060829\n",
      "Epoch[0], Batch[610], Train loss: 0.05160483717918396\n",
      "Epoch[0], Val loss: 0.0509432815015316\n",
      "Epoch[0], Batch[611], Train loss: 0.05295978859066963\n",
      "Epoch[0], Val loss: 0.05185787007212639\n",
      "Epoch[0], Batch[612], Train loss: 0.0526212640106678\n",
      "Epoch[0], Val loss: 0.04953517019748688\n",
      "Epoch[0], Batch[613], Train loss: 0.055104244500398636\n",
      "Epoch[0], Val loss: 0.04686376824975014\n",
      "Epoch[0], Batch[614], Train loss: 0.05309613049030304\n",
      "Epoch[0], Val loss: 0.0507190003991127\n",
      "Epoch[0], Batch[615], Train loss: 0.05154931917786598\n",
      "Epoch[0], Val loss: 0.04809693619608879\n",
      "Epoch[0], Batch[616], Train loss: 0.051354315131902695\n",
      "Epoch[0], Val loss: 0.048066843301057816\n",
      "Epoch[0], Batch[617], Train loss: 0.052357833832502365\n",
      "Epoch[0], Val loss: 0.047567930072546005\n",
      "Epoch[0], Batch[618], Train loss: 0.050858840346336365\n",
      "Epoch[0], Val loss: 0.0475773960351944\n",
      "Epoch[0], Batch[619], Train loss: 0.05491890758275986\n",
      "Epoch[0], Val loss: 0.04882578179240227\n",
      "Epoch[0], Batch[620], Train loss: 0.05452815443277359\n",
      "Epoch[0], Val loss: 0.04954446107149124\n",
      "Epoch[0], Batch[621], Train loss: 0.05453711003065109\n",
      "Epoch[0], Val loss: 0.05086634308099747\n",
      "Epoch[0], Batch[622], Train loss: 0.05518968775868416\n",
      "Epoch[0], Val loss: 0.04798366501927376\n",
      "Epoch[0], Batch[623], Train loss: 0.05000973492860794\n",
      "Epoch[0], Val loss: 0.049840886145830154\n",
      "Epoch[0], Batch[624], Train loss: 0.052042629569768906\n",
      "Epoch[0], Val loss: 0.04960823804140091\n",
      "Epoch[0], Batch[625], Train loss: 0.051530621945858\n",
      "Epoch[0], Val loss: 0.04786645248532295\n",
      "Epoch[0], Batch[626], Train loss: 0.0534636527299881\n",
      "Epoch[0], Val loss: 0.047043297439813614\n",
      "Epoch[0], Batch[627], Train loss: 0.05470458045601845\n",
      "Epoch[0], Val loss: 0.04645515978336334\n",
      "Epoch[0], Batch[628], Train loss: 0.05181455612182617\n",
      "Epoch[0], Val loss: 0.05008108541369438\n",
      "Epoch[0], Batch[629], Train loss: 0.052446454763412476\n",
      "Epoch[0], Val loss: 0.05040754750370979\n",
      "Epoch[0], Batch[630], Train loss: 0.053058672696352005\n",
      "Epoch[0], Val loss: 0.0463128425180912\n",
      "Epoch[0], Batch[631], Train loss: 0.052213866263628006\n",
      "Epoch[0], Val loss: 0.05106683075428009\n",
      "Epoch[0], Batch[632], Train loss: 0.05376226454973221\n",
      "Epoch[0], Val loss: 0.047523826360702515\n",
      "Epoch[0], Batch[633], Train loss: 0.05106256157159805\n",
      "Epoch[0], Val loss: 0.04960307478904724\n",
      "Epoch[0], Batch[634], Train loss: 0.0508221872150898\n",
      "Epoch[0], Val loss: 0.04841198772192001\n",
      "Epoch[0], Batch[635], Train loss: 0.05300173908472061\n",
      "Epoch[0], Val loss: 0.04924433305859566\n",
      "Epoch[0], Batch[636], Train loss: 0.05356166884303093\n",
      "Epoch[0], Val loss: 0.04902683198451996\n",
      "Epoch[0], Batch[637], Train loss: 0.05548974871635437\n",
      "Epoch[0], Val loss: 0.047640684992074966\n",
      "Epoch[0], Batch[638], Train loss: 0.05450296774506569\n",
      "Epoch[0], Val loss: 0.04866906255483627\n",
      "Epoch[0], Batch[639], Train loss: 0.04973484203219414\n",
      "Epoch[0], Val loss: 0.04606132581830025\n",
      "Epoch[0], Batch[640], Train loss: 0.049872417002916336\n",
      "Epoch[0], Val loss: 0.04689650237560272\n",
      "Epoch[0], Batch[641], Train loss: 0.05205647647380829\n",
      "Epoch[0], Val loss: 0.04769427701830864\n",
      "Epoch[0], Batch[642], Train loss: 0.05161675438284874\n",
      "Epoch[0], Val loss: 0.04630937799811363\n",
      "Epoch[0], Batch[643], Train loss: 0.05148019269108772\n",
      "Epoch[0], Val loss: 0.047924503684043884\n",
      "Epoch[0], Batch[644], Train loss: 0.05501188710331917\n",
      "Epoch[0], Val loss: 0.04762084409594536\n",
      "Epoch[0], Batch[645], Train loss: 0.05652748793363571\n",
      "Epoch[0], Val loss: 0.04745697230100632\n",
      "Epoch[0], Batch[646], Train loss: 0.051462892442941666\n",
      "Epoch[0], Val loss: 0.04637962952256203\n",
      "Epoch[0], Batch[647], Train loss: 0.054555438458919525\n",
      "Epoch[0], Val loss: 0.04935743287205696\n",
      "Epoch[0], Batch[648], Train loss: 0.04922889173030853\n",
      "Epoch[0], Val loss: 0.05108112841844559\n",
      "Epoch[0], Batch[649], Train loss: 0.054506100714206696\n",
      "Epoch[0], Val loss: 0.04710796847939491\n",
      "Epoch[0], Batch[650], Train loss: 0.05328534543514252\n",
      "Epoch[0], Val loss: 0.048817578703165054\n",
      "Epoch[0], Batch[651], Train loss: 0.05203861743211746\n",
      "Epoch[0], Val loss: 0.048871513456106186\n",
      "Epoch[0], Batch[652], Train loss: 0.0532914474606514\n",
      "Epoch[0], Val loss: 0.049577802419662476\n",
      "Epoch[0], Batch[653], Train loss: 0.052820492535829544\n",
      "Epoch[0], Val loss: 0.05142180994153023\n",
      "Epoch[0], Batch[654], Train loss: 0.05062105879187584\n",
      "Epoch[0], Val loss: 0.05088524520397186\n",
      "Epoch[0], Batch[655], Train loss: 0.05342143401503563\n",
      "Epoch[0], Val loss: 0.045693930238485336\n",
      "Epoch[0], Batch[656], Train loss: 0.05131619796156883\n",
      "Epoch[0], Val loss: 0.047730762511491776\n",
      "Epoch[0], Batch[657], Train loss: 0.054812364280223846\n",
      "Epoch[0], Val loss: 0.04945271462202072\n",
      "Epoch[0], Batch[658], Train loss: 0.05496630072593689\n",
      "Epoch[0], Val loss: 0.04544893279671669\n",
      "Epoch[0], Batch[659], Train loss: 0.050999630242586136\n",
      "Epoch[0], Val loss: 0.04830940440297127\n",
      "Epoch[0], Batch[660], Train loss: 0.05287506803870201\n",
      "Epoch[0], Val loss: 0.05029555782675743\n",
      "Epoch[0], Batch[661], Train loss: 0.051663968712091446\n",
      "Epoch[0], Val loss: 0.050498656928539276\n",
      "Epoch[0], Batch[662], Train loss: 0.053460851311683655\n",
      "Epoch[0], Val loss: 0.047408074140548706\n",
      "Epoch[0], Batch[663], Train loss: 0.05320419371128082\n",
      "Epoch[0], Val loss: 0.045824892818927765\n",
      "Epoch[0], Batch[664], Train loss: 0.05161401256918907\n",
      "Epoch[0], Val loss: 0.04707542434334755\n",
      "Epoch[0], Batch[665], Train loss: 0.05583922564983368\n",
      "Epoch[0], Val loss: 0.04565883055329323\n",
      "Epoch[0], Batch[666], Train loss: 0.05277659744024277\n",
      "Epoch[0], Val loss: 0.0493517741560936\n",
      "Epoch[0], Batch[667], Train loss: 0.05444253236055374\n",
      "Epoch[0], Val loss: 0.048740677535533905\n",
      "Epoch[0], Batch[668], Train loss: 0.05103490129113197\n",
      "Epoch[0], Val loss: 0.048609860241413116\n",
      "Epoch[0], Batch[669], Train loss: 0.05042111873626709\n",
      "Epoch[0], Val loss: 0.050668202340602875\n",
      "Epoch[0], Batch[670], Train loss: 0.05058896169066429\n",
      "Epoch[0], Val loss: 0.04822206497192383\n",
      "Epoch[0], Batch[671], Train loss: 0.053506139665842056\n",
      "Epoch[0], Val loss: 0.04649452865123749\n",
      "Epoch[0], Batch[672], Train loss: 0.05172964558005333\n",
      "Epoch[0], Val loss: 0.04821879416704178\n",
      "Epoch[0], Batch[673], Train loss: 0.05154768377542496\n",
      "Epoch[0], Val loss: 0.04812507703900337\n",
      "Epoch[0], Batch[674], Train loss: 0.05323372781276703\n",
      "Epoch[0], Val loss: 0.049688614904880524\n",
      "Epoch[0], Batch[675], Train loss: 0.054372016340494156\n",
      "Epoch[0], Val loss: 0.05392284691333771\n",
      "Epoch[0], Batch[676], Train loss: 0.05244996026158333\n",
      "Epoch[0], Val loss: 0.051466669887304306\n",
      "Epoch[0], Batch[677], Train loss: 0.055917248129844666\n",
      "Epoch[0], Val loss: 0.050086379051208496\n",
      "Epoch[0], Batch[678], Train loss: 0.052749112248420715\n",
      "Epoch[0], Val loss: 0.04740135371685028\n",
      "Epoch[0], Batch[679], Train loss: 0.050796180963516235\n",
      "Epoch[0], Val loss: 0.04888906702399254\n",
      "Epoch[0], Batch[680], Train loss: 0.055779363960027695\n",
      "Epoch[0], Val loss: 0.04844893142580986\n",
      "Epoch[0], Batch[681], Train loss: 0.050250910222530365\n",
      "Epoch[0], Val loss: 0.04902702569961548\n",
      "Epoch[0], Batch[682], Train loss: 0.052415404468774796\n",
      "Epoch[0], Val loss: 0.048184871673583984\n",
      "Epoch[0], Batch[683], Train loss: 0.05213798210024834\n",
      "Epoch[0], Val loss: 0.04935213923454285\n",
      "Epoch[0], Batch[684], Train loss: 0.05092858895659447\n",
      "Epoch[0], Val loss: 0.04781389608979225\n",
      "Epoch[0], Batch[685], Train loss: 0.051380593329668045\n",
      "Epoch[0], Val loss: 0.04665398225188255\n",
      "Epoch[0], Batch[686], Train loss: 0.051036760210990906\n",
      "Epoch[0], Val loss: 0.05069934204220772\n",
      "Epoch[0], Batch[687], Train loss: 0.052672483026981354\n",
      "Epoch[0], Val loss: 0.051376741379499435\n",
      "Epoch[0], Batch[688], Train loss: 0.05411136895418167\n",
      "Epoch[0], Val loss: 0.04732559993863106\n",
      "Epoch[0], Batch[689], Train loss: 0.05186540633440018\n",
      "Epoch[0], Val loss: 0.04773794487118721\n",
      "Epoch[0], Batch[690], Train loss: 0.05348484590649605\n",
      "Epoch[0], Val loss: 0.050924647599458694\n",
      "Epoch[0], Batch[691], Train loss: 0.0498504713177681\n",
      "Epoch[0], Val loss: 0.04866509139537811\n",
      "Epoch[0], Batch[692], Train loss: 0.05234003812074661\n",
      "Epoch[0], Val loss: 0.05024005100131035\n",
      "Epoch[0], Batch[693], Train loss: 0.051638416945934296\n",
      "Epoch[0], Val loss: 0.04968307539820671\n",
      "Epoch[0], Batch[694], Train loss: 0.04922850430011749\n",
      "Epoch[0], Val loss: 0.046545516699552536\n",
      "Epoch[0], Batch[695], Train loss: 0.05468497425317764\n",
      "Epoch[0], Val loss: 0.047763973474502563\n",
      "Epoch[0], Batch[696], Train loss: 0.051848601549863815\n",
      "Epoch[0], Val loss: 0.04954752326011658\n",
      "Epoch[0], Batch[697], Train loss: 0.04966488853096962\n",
      "Epoch[0], Val loss: 0.048780009150505066\n",
      "Epoch[0], Batch[698], Train loss: 0.05231647938489914\n",
      "Epoch[0], Val loss: 0.044267117977142334\n",
      "Epoch[0], Batch[699], Train loss: 0.053272005170583725\n",
      "Epoch[0], Val loss: 0.0471627376973629\n",
      "Epoch[0], Batch[700], Train loss: 0.05318601056933403\n",
      "Epoch[0], Val loss: 0.049687378108501434\n",
      "Epoch[0], Batch[701], Train loss: 0.051095150411129\n",
      "Epoch[0], Val loss: 0.047093816101551056\n",
      "Epoch[0], Batch[702], Train loss: 0.05216868594288826\n",
      "Epoch[0], Val loss: 0.047422535717487335\n",
      "Epoch[0], Batch[703], Train loss: 0.05034828558564186\n",
      "Epoch[0], Val loss: 0.0495346374809742\n",
      "Epoch[0], Batch[704], Train loss: 0.05273795500397682\n",
      "Epoch[0], Val loss: 0.048648059368133545\n",
      "Epoch[0], Batch[705], Train loss: 0.05282653123140335\n",
      "Epoch[0], Val loss: 0.04663263261318207\n",
      "Epoch[0], Batch[706], Train loss: 0.05394211784005165\n",
      "Epoch[0], Val loss: 0.050564225763082504\n",
      "Epoch[0], Batch[707], Train loss: 0.05052560567855835\n",
      "Epoch[0], Val loss: 0.047480542212724686\n",
      "Epoch[0], Batch[708], Train loss: 0.05317625403404236\n",
      "Epoch[0], Val loss: 0.050277482718229294\n",
      "Epoch[0], Batch[709], Train loss: 0.05099482089281082\n",
      "Epoch[0], Val loss: 0.05074817314743996\n",
      "Epoch[0], Batch[710], Train loss: 0.052625320851802826\n",
      "Epoch[0], Val loss: 0.047081537544727325\n",
      "Epoch[0], Batch[711], Train loss: 0.05232526361942291\n",
      "Epoch[0], Val loss: 0.048394713550806046\n",
      "Epoch[0], Batch[712], Train loss: 0.054887186735868454\n",
      "Epoch[0], Val loss: 0.04942769557237625\n",
      "Epoch[0], Batch[713], Train loss: 0.05422297492623329\n",
      "Epoch[0], Val loss: 0.045572325587272644\n",
      "Epoch[0], Batch[714], Train loss: 0.05154452100396156\n",
      "Epoch[0], Val loss: 0.04794226586818695\n",
      "Epoch[0], Batch[715], Train loss: 0.05517785623669624\n",
      "Epoch[0], Val loss: 0.049038469791412354\n",
      "Epoch[0], Batch[716], Train loss: 0.05274759978055954\n",
      "Epoch[0], Val loss: 0.04559324309229851\n",
      "Epoch[0], Batch[717], Train loss: 0.05573606118559837\n",
      "Epoch[0], Val loss: 0.04902102053165436\n",
      "Epoch[0], Batch[718], Train loss: 0.05364299938082695\n",
      "Epoch[0], Val loss: 0.0478249154984951\n",
      "Epoch[0], Batch[719], Train loss: 0.04923674091696739\n",
      "Epoch[0], Val loss: 0.048018597066402435\n",
      "Epoch[0], Batch[720], Train loss: 0.05215931683778763\n",
      "Epoch[0], Val loss: 0.04600631818175316\n",
      "Epoch[0], Batch[721], Train loss: 0.05379793047904968\n",
      "Epoch[0], Val loss: 0.04582695662975311\n",
      "Epoch[0], Batch[722], Train loss: 0.0501888282597065\n",
      "Epoch[0], Val loss: 0.04862420633435249\n",
      "Epoch[0], Batch[723], Train loss: 0.05069993808865547\n",
      "Epoch[0], Val loss: 0.04537034407258034\n",
      "Epoch[0], Batch[724], Train loss: 0.05335612595081329\n",
      "Epoch[0], Val loss: 0.04854515567421913\n",
      "Epoch[0], Batch[725], Train loss: 0.05125001445412636\n",
      "Epoch[0], Val loss: 0.04759584739804268\n",
      "Epoch[0], Batch[726], Train loss: 0.05103066563606262\n",
      "Epoch[0], Val loss: 0.04705650731921196\n",
      "Epoch[0], Batch[727], Train loss: 0.053590040653944016\n",
      "Epoch[0], Val loss: 0.04500417783856392\n",
      "Epoch[0], Batch[728], Train loss: 0.05039622634649277\n",
      "Epoch[0], Val loss: 0.045517697930336\n",
      "Epoch[0], Batch[729], Train loss: 0.05036969855427742\n",
      "Epoch[0], Val loss: 0.04805811494588852\n",
      "Epoch[0], Batch[730], Train loss: 0.050506312400102615\n",
      "Epoch[0], Val loss: 0.04957418888807297\n",
      "Epoch[0], Batch[731], Train loss: 0.05041774734854698\n",
      "Epoch[0], Val loss: 0.04749803617596626\n",
      "Epoch[0], Batch[732], Train loss: 0.05186131224036217\n",
      "Epoch[0], Val loss: 0.048554763197898865\n",
      "Epoch[0], Batch[733], Train loss: 0.05160917714238167\n",
      "Epoch[0], Val loss: 0.047791171818971634\n",
      "Epoch[0], Batch[734], Train loss: 0.05494091287255287\n",
      "Epoch[0], Val loss: 0.04832646995782852\n",
      "Epoch[0], Batch[735], Train loss: 0.05582800507545471\n",
      "Epoch[0], Val loss: 0.0480308011174202\n",
      "Epoch[0], Batch[736], Train loss: 0.052452169358730316\n",
      "Epoch[0], Val loss: 0.0447002649307251\n",
      "Epoch[0], Batch[737], Train loss: 0.051271602511405945\n",
      "Epoch[0], Val loss: 0.04989172890782356\n",
      "Epoch[0], Batch[738], Train loss: 0.05163130909204483\n",
      "Epoch[0], Val loss: 0.04994778707623482\n",
      "Epoch[0], Batch[739], Train loss: 0.05017751082777977\n",
      "Epoch[0], Val loss: 0.04989839345216751\n",
      "Epoch[0], Batch[740], Train loss: 0.05315249040722847\n",
      "Epoch[0], Val loss: 0.046269048005342484\n",
      "Epoch[0], Batch[741], Train loss: 0.05033227428793907\n",
      "Epoch[0], Val loss: 0.048716407269239426\n",
      "Epoch[0], Batch[742], Train loss: 0.05131888389587402\n",
      "Epoch[0], Val loss: 0.04656645283102989\n",
      "Epoch[0], Batch[743], Train loss: 0.054132647812366486\n",
      "Epoch[0], Val loss: 0.046522971242666245\n",
      "Epoch[0], Batch[744], Train loss: 0.05157966539263725\n",
      "Epoch[0], Val loss: 0.05302160233259201\n",
      "Epoch[0], Batch[745], Train loss: 0.050456702709198\n",
      "Epoch[0], Val loss: 0.04505009204149246\n",
      "Epoch[0], Batch[746], Train loss: 0.0517028272151947\n",
      "Epoch[0], Val loss: 0.04815065860748291\n",
      "Epoch[0], Batch[747], Train loss: 0.05050167068839073\n",
      "Epoch[0], Val loss: 0.04891440272331238\n",
      "Epoch[0], Batch[748], Train loss: 0.05196186155080795\n",
      "Epoch[0], Val loss: 0.048846881836652756\n",
      "Epoch[0], Batch[749], Train loss: 0.04827766492962837\n",
      "Epoch[0], Val loss: 0.046699825674295425\n",
      "Epoch[0], Batch[750], Train loss: 0.05298430845141411\n",
      "Epoch[0], Val loss: 0.04860429838299751\n",
      "Epoch[0], Batch[751], Train loss: 0.05261967331171036\n",
      "Epoch[0], Val loss: 0.04900466650724411\n",
      "Epoch[0], Batch[752], Train loss: 0.051885008811950684\n",
      "Epoch[0], Val loss: 0.04851698502898216\n",
      "Epoch[0], Batch[753], Train loss: 0.05259629711508751\n",
      "Epoch[0], Val loss: 0.04802780598402023\n",
      "Epoch[0], Batch[754], Train loss: 0.05014221370220184\n",
      "Epoch[0], Val loss: 0.051026422530412674\n",
      "Epoch[0], Batch[755], Train loss: 0.04916347563266754\n",
      "Epoch[0], Val loss: 0.04863995686173439\n",
      "Epoch[0], Batch[756], Train loss: 0.05264180153608322\n",
      "Epoch[0], Val loss: 0.048572566360235214\n",
      "Epoch[0], Batch[757], Train loss: 0.049748487770557404\n",
      "Epoch[0], Val loss: 0.048467881977558136\n",
      "Epoch[0], Batch[758], Train loss: 0.050325602293014526\n",
      "Epoch[0], Val loss: 0.047105733305215836\n",
      "Epoch[0], Batch[759], Train loss: 0.056031931191682816\n",
      "Epoch[0], Val loss: 0.04913666099309921\n",
      "Epoch[0], Batch[760], Train loss: 0.050312310457229614\n",
      "Epoch[0], Val loss: 0.0448613241314888\n",
      "Epoch[0], Batch[761], Train loss: 0.04882857948541641\n",
      "Epoch[0], Val loss: 0.04859696701169014\n",
      "Epoch[0], Batch[762], Train loss: 0.05235133320093155\n",
      "Epoch[0], Val loss: 0.048330117017030716\n",
      "Epoch[0], Batch[763], Train loss: 0.05048682540655136\n",
      "Epoch[0], Val loss: 0.047314003109931946\n",
      "Epoch[0], Batch[764], Train loss: 0.050205253064632416\n",
      "Epoch[0], Val loss: 0.048606108874082565\n",
      "Epoch[0], Batch[765], Train loss: 0.05361102893948555\n",
      "Epoch[0], Val loss: 0.05017484351992607\n",
      "Epoch[0], Batch[766], Train loss: 0.04896989092230797\n",
      "Epoch[0], Val loss: 0.046747609972953796\n",
      "Epoch[0], Batch[767], Train loss: 0.04938454553484917\n",
      "Epoch[0], Val loss: 0.05159691721200943\n",
      "Epoch[0], Batch[768], Train loss: 0.05030588433146477\n",
      "Epoch[0], Val loss: 0.04922604560852051\n",
      "Epoch[0], Batch[769], Train loss: 0.05160786956548691\n",
      "Epoch[0], Val loss: 0.04795579984784126\n",
      "Epoch[0], Batch[770], Train loss: 0.04709282144904137\n",
      "Epoch[0], Val loss: 0.0485377237200737\n",
      "Epoch[0], Batch[771], Train loss: 0.050867959856987\n",
      "Epoch[0], Val loss: 0.04940037429332733\n",
      "Epoch[0], Batch[772], Train loss: 0.05247819796204567\n",
      "Epoch[0], Val loss: 0.04493981972336769\n",
      "Epoch[0], Batch[773], Train loss: 0.05259394273161888\n",
      "Epoch[0], Val loss: 0.049629613757133484\n",
      "Epoch[0], Batch[774], Train loss: 0.052812669426202774\n",
      "Epoch[0], Val loss: 0.04624306038022041\n",
      "Epoch[0], Batch[775], Train loss: 0.05346037819981575\n",
      "Epoch[0], Val loss: 0.046923909336328506\n",
      "Epoch[0], Batch[776], Train loss: 0.05114153027534485\n",
      "Epoch[0], Val loss: 0.048066627234220505\n",
      "Epoch[0], Batch[777], Train loss: 0.05202951654791832\n",
      "Epoch[0], Val loss: 0.0456773042678833\n",
      "Epoch[0], Batch[778], Train loss: 0.04833164066076279\n",
      "Epoch[0], Val loss: 0.04866056516766548\n",
      "Epoch[0], Batch[779], Train loss: 0.050760667771101\n",
      "Epoch[0], Val loss: 0.04824244976043701\n",
      "Epoch[0], Batch[780], Train loss: 0.052899111062288284\n",
      "Epoch[0], Val loss: 0.049136850982904434\n",
      "Epoch[0], Batch[781], Train loss: 0.04939596727490425\n",
      "Epoch[0], Val loss: 0.05008934438228607\n",
      "Epoch[0], Batch[782], Train loss: 0.04863822087645531\n",
      "Epoch[0], Val loss: 0.04448980465531349\n",
      "Epoch[0], Batch[783], Train loss: 0.04944034293293953\n",
      "Epoch[0], Val loss: 0.04810241982340813\n",
      "Epoch[0], Batch[784], Train loss: 0.05309554189443588\n",
      "Epoch[0], Val loss: 0.04884723946452141\n",
      "Epoch[0], Batch[785], Train loss: 0.050169602036476135\n",
      "Epoch[0], Val loss: 0.04779580235481262\n",
      "Epoch[0], Batch[786], Train loss: 0.05007198825478554\n",
      "Epoch[0], Val loss: 0.046612851321697235\n",
      "Epoch[0], Batch[787], Train loss: 0.05078338831663132\n",
      "Epoch[0], Val loss: 0.04698563739657402\n",
      "Epoch[0], Batch[788], Train loss: 0.051152847707271576\n",
      "Epoch[0], Val loss: 0.04574436694383621\n",
      "Epoch[0], Batch[789], Train loss: 0.04985642805695534\n",
      "Epoch[0], Val loss: 0.048453189432621\n",
      "Epoch[0], Batch[790], Train loss: 0.05301733687520027\n",
      "Epoch[0], Val loss: 0.045328203588724136\n",
      "Epoch[0], Batch[791], Train loss: 0.049577727913856506\n",
      "Epoch[0], Val loss: 0.047500159591436386\n",
      "Epoch[0], Batch[792], Train loss: 0.05097075551748276\n",
      "Epoch[0], Val loss: 0.045866552740335464\n",
      "Epoch[0], Batch[793], Train loss: 0.04857610538601875\n",
      "Epoch[0], Val loss: 0.04638982564210892\n",
      "Epoch[0], Batch[794], Train loss: 0.05220472440123558\n",
      "Epoch[0], Val loss: 0.04851517826318741\n",
      "Epoch[0], Batch[795], Train loss: 0.050501059740781784\n",
      "Epoch[0], Val loss: 0.04452386870980263\n",
      "Epoch[0], Batch[796], Train loss: 0.05019126459956169\n",
      "Epoch[0], Val loss: 0.04786109924316406\n",
      "Epoch[0], Batch[797], Train loss: 0.04829774796962738\n",
      "Epoch[0], Val loss: 0.047596462070941925\n",
      "Epoch[0], Batch[798], Train loss: 0.04935760423541069\n",
      "Epoch[0], Val loss: 0.045625656843185425\n",
      "Epoch[0], Batch[799], Train loss: 0.04872735217213631\n",
      "Epoch[0], Val loss: 0.045949794352054596\n",
      "Epoch[0], Batch[800], Train loss: 0.048937905579805374\n",
      "Epoch[0], Val loss: 0.04831063002347946\n",
      "Epoch[0], Batch[801], Train loss: 0.050919048488140106\n",
      "Epoch[0], Val loss: 0.04799202084541321\n",
      "Epoch[0], Batch[802], Train loss: 0.05027363449335098\n",
      "Epoch[0], Val loss: 0.046508703380823135\n",
      "Epoch[0], Batch[803], Train loss: 0.04593963548541069\n",
      "Epoch[0], Val loss: 0.04486348107457161\n",
      "Epoch[0], Batch[804], Train loss: 0.048218272626399994\n",
      "Epoch[0], Val loss: 0.04787923023104668\n",
      "Epoch[0], Batch[805], Train loss: 0.051492854952812195\n",
      "Epoch[0], Val loss: 0.047499414533376694\n",
      "Epoch[0], Batch[806], Train loss: 0.04789457842707634\n",
      "Epoch[0], Val loss: 0.04750799760222435\n",
      "Epoch[0], Batch[807], Train loss: 0.04688636586070061\n",
      "Epoch[0], Val loss: 0.0474560372531414\n",
      "Epoch[0], Batch[808], Train loss: 0.04816492646932602\n",
      "Epoch[0], Val loss: 0.04781758785247803\n",
      "Epoch[0], Batch[809], Train loss: 0.05527884513139725\n",
      "Epoch[0], Val loss: 0.04723137244582176\n",
      "Epoch[0], Batch[810], Train loss: 0.05030570551753044\n",
      "Epoch[0], Val loss: 0.04702110216021538\n",
      "Epoch[0], Batch[811], Train loss: 0.04936676472425461\n",
      "Epoch[0], Val loss: 0.04467293247580528\n",
      "Epoch[0], Batch[812], Train loss: 0.051875751465559006\n",
      "Epoch[0], Val loss: 0.046145785599946976\n",
      "Epoch[0], Batch[813], Train loss: 0.04873792454600334\n",
      "Epoch[0], Val loss: 0.04831032082438469\n",
      "Epoch[0], Batch[814], Train loss: 0.05443909764289856\n",
      "Epoch[0], Val loss: 0.049591485410928726\n",
      "Epoch[0], Batch[815], Train loss: 0.05208262428641319\n",
      "Epoch[0], Val loss: 0.05023590102791786\n",
      "Epoch[0], Batch[816], Train loss: 0.048895593732595444\n",
      "Epoch[0], Val loss: 0.0438380241394043\n",
      "Epoch[0], Batch[817], Train loss: 0.04952732101082802\n",
      "Epoch[0], Val loss: 0.04367562755942345\n",
      "Epoch[0], Batch[818], Train loss: 0.04923095554113388\n",
      "Epoch[0], Val loss: 0.045708801597356796\n",
      "Epoch[0], Batch[819], Train loss: 0.04919285327196121\n",
      "Epoch[0], Val loss: 0.044505003839731216\n",
      "Epoch[0], Batch[820], Train loss: 0.0530557781457901\n",
      "Epoch[0], Val loss: 0.047706738114356995\n",
      "Epoch[0], Batch[821], Train loss: 0.05177215486764908\n",
      "Epoch[0], Val loss: 0.045923247933387756\n",
      "Epoch[0], Batch[822], Train loss: 0.04957137256860733\n",
      "Epoch[0], Val loss: 0.04402824118733406\n",
      "Epoch[0], Batch[823], Train loss: 0.04851440340280533\n",
      "Epoch[0], Val loss: 0.047636475414037704\n",
      "Epoch[0], Batch[824], Train loss: 0.05028655380010605\n",
      "Epoch[0], Val loss: 0.047393471002578735\n",
      "Epoch[0], Batch[825], Train loss: 0.05079283192753792\n",
      "Epoch[0], Val loss: 0.0486319437623024\n",
      "Epoch[0], Batch[826], Train loss: 0.051984239369630814\n",
      "Epoch[0], Val loss: 0.04770829156041145\n",
      "Epoch[0], Batch[827], Train loss: 0.050989631563425064\n",
      "Epoch[0], Val loss: 0.04749981313943863\n",
      "Epoch[0], Batch[828], Train loss: 0.05243783816695213\n",
      "Epoch[0], Val loss: 0.049508411437273026\n",
      "Epoch[0], Batch[829], Train loss: 0.052271995693445206\n",
      "Epoch[0], Val loss: 0.04814007505774498\n",
      "Epoch[0], Batch[830], Train loss: 0.050084128975868225\n",
      "Epoch[0], Val loss: 0.04820789396762848\n",
      "Epoch[0], Batch[831], Train loss: 0.05043300986289978\n",
      "Epoch[0], Val loss: 0.04703178629279137\n",
      "Epoch[0], Batch[832], Train loss: 0.05073156952857971\n",
      "Epoch[0], Val loss: 0.04732043296098709\n",
      "Epoch[0], Batch[833], Train loss: 0.05168486014008522\n",
      "Epoch[0], Val loss: 0.050044652074575424\n",
      "Epoch[0], Batch[834], Train loss: 0.05202179402112961\n",
      "Epoch[0], Val loss: 0.04916606470942497\n",
      "Epoch[0], Batch[835], Train loss: 0.05196114256978035\n",
      "Epoch[0], Val loss: 0.050494056195020676\n",
      "Epoch[0], Batch[836], Train loss: 0.05053279548883438\n",
      "Epoch[0], Val loss: 0.04415588453412056\n",
      "Epoch[0], Batch[837], Train loss: 0.048438891768455505\n",
      "Epoch[0], Val loss: 0.04578552767634392\n",
      "Epoch[0], Batch[838], Train loss: 0.04925184324383736\n",
      "Epoch[0], Val loss: 0.04855639114975929\n",
      "Epoch[0], Batch[839], Train loss: 0.05095440894365311\n",
      "Epoch[0], Val loss: 0.04590662941336632\n",
      "Epoch[0], Batch[840], Train loss: 0.04958273470401764\n",
      "Epoch[0], Val loss: 0.04544854909181595\n",
      "Epoch[0], Batch[841], Train loss: 0.04933779314160347\n",
      "Epoch[0], Val loss: 0.04513901099562645\n",
      "Epoch[0], Batch[842], Train loss: 0.048461828380823135\n",
      "Epoch[0], Val loss: 0.047034405171871185\n",
      "Epoch[0], Batch[843], Train loss: 0.05133340507745743\n",
      "Epoch[0], Val loss: 0.04574769362807274\n",
      "Epoch[0], Batch[844], Train loss: 0.05266116186976433\n",
      "Epoch[0], Val loss: 0.05124181509017944\n",
      "Epoch[0], Batch[845], Train loss: 0.05178511142730713\n",
      "Epoch[0], Val loss: 0.04828830808401108\n",
      "Epoch[0], Batch[846], Train loss: 0.05238135904073715\n",
      "Epoch[0], Val loss: 0.04805587977170944\n",
      "Epoch[0], Batch[847], Train loss: 0.051608894020318985\n",
      "Epoch[0], Val loss: 0.047119367867708206\n",
      "Epoch[0], Batch[848], Train loss: 0.050886332988739014\n",
      "Epoch[0], Val loss: 0.04622199758887291\n",
      "Epoch[0], Batch[849], Train loss: 0.052458539605140686\n",
      "Epoch[0], Val loss: 0.0488242544233799\n",
      "Epoch[0], Batch[850], Train loss: 0.04904479905962944\n",
      "Epoch[0], Val loss: 0.048316992819309235\n",
      "Epoch[0], Batch[851], Train loss: 0.052355121821165085\n",
      "Epoch[0], Val loss: 0.04600059986114502\n",
      "Epoch[0], Batch[852], Train loss: 0.05194760486483574\n",
      "Epoch[0], Val loss: 0.046754684299230576\n",
      "Epoch[0], Batch[853], Train loss: 0.05023794621229172\n",
      "Epoch[0], Val loss: 0.04431037977337837\n",
      "Epoch[0], Batch[854], Train loss: 0.04929957911372185\n",
      "Epoch[0], Val loss: 0.04735994338989258\n",
      "Epoch[0], Batch[855], Train loss: 0.04989590123295784\n",
      "Epoch[0], Val loss: 0.04749751836061478\n",
      "Epoch[0], Batch[856], Train loss: 0.04916691407561302\n",
      "Epoch[0], Val loss: 0.04561970755457878\n",
      "Epoch[0], Batch[857], Train loss: 0.049609068781137466\n",
      "Epoch[0], Val loss: 0.048169273883104324\n",
      "Epoch[0], Batch[858], Train loss: 0.052638497203588486\n",
      "Epoch[0], Val loss: 0.04472453147172928\n",
      "Epoch[0], Batch[859], Train loss: 0.0501813143491745\n",
      "Epoch[0], Val loss: 0.04957728460431099\n",
      "Epoch[0], Batch[860], Train loss: 0.04757491126656532\n",
      "Epoch[0], Val loss: 0.04916050285100937\n",
      "Epoch[0], Batch[861], Train loss: 0.0470820777118206\n",
      "Epoch[0], Val loss: 0.04886382818222046\n",
      "Epoch[0], Batch[862], Train loss: 0.05090026184916496\n",
      "Epoch[0], Val loss: 0.04790492355823517\n",
      "Epoch[0], Batch[863], Train loss: 0.04925750195980072\n",
      "Epoch[0], Val loss: 0.046101462095975876\n",
      "Epoch[0], Batch[864], Train loss: 0.05106547847390175\n",
      "Epoch[0], Val loss: 0.04757828265428543\n",
      "Epoch[0], Batch[865], Train loss: 0.04871312156319618\n",
      "Epoch[0], Val loss: 0.04854969307780266\n",
      "Epoch[0], Batch[866], Train loss: 0.04899878054857254\n",
      "Epoch[0], Val loss: 0.048577189445495605\n",
      "Epoch[0], Batch[867], Train loss: 0.05041415989398956\n",
      "Epoch[0], Val loss: 0.04875495284795761\n",
      "Epoch[0], Batch[868], Train loss: 0.050553735345602036\n",
      "Epoch[0], Val loss: 0.048497844487428665\n",
      "Epoch[0], Batch[869], Train loss: 0.047852616757154465\n",
      "Epoch[0], Val loss: 0.04645593836903572\n",
      "Epoch[0], Batch[870], Train loss: 0.05067422240972519\n",
      "Epoch[0], Val loss: 0.045693326741456985\n",
      "Epoch[0], Batch[871], Train loss: 0.050658464431762695\n",
      "Epoch[0], Val loss: 0.04588291794061661\n",
      "Epoch[0], Batch[872], Train loss: 0.048366300761699677\n",
      "Epoch[0], Val loss: 0.046320877969264984\n",
      "Epoch[0], Batch[873], Train loss: 0.04832959547638893\n",
      "Epoch[0], Val loss: 0.04531056433916092\n",
      "Epoch[0], Batch[874], Train loss: 0.049991779029369354\n",
      "Epoch[0], Val loss: 0.044896550476551056\n",
      "Epoch[0], Batch[875], Train loss: 0.05189373344182968\n",
      "Epoch[0], Val loss: 0.04527095705270767\n",
      "Epoch[0], Batch[876], Train loss: 0.05194593593478203\n",
      "Epoch[0], Val loss: 0.0466768853366375\n",
      "Epoch[0], Batch[877], Train loss: 0.050881218165159225\n",
      "Epoch[0], Val loss: 0.047154102474451065\n",
      "Epoch[0], Batch[878], Train loss: 0.051550451666116714\n",
      "Epoch[0], Val loss: 0.04583866521716118\n",
      "Epoch[0], Batch[879], Train loss: 0.052610233426094055\n",
      "Epoch[0], Val loss: 0.047063376754522324\n",
      "Epoch[0], Batch[880], Train loss: 0.04964994266629219\n",
      "Epoch[0], Val loss: 0.04879879206418991\n",
      "Epoch[0], Batch[881], Train loss: 0.05087440833449364\n",
      "Epoch[0], Val loss: 0.04562756419181824\n",
      "Epoch[0], Batch[882], Train loss: 0.04861212149262428\n",
      "Epoch[0], Val loss: 0.04713178798556328\n",
      "Epoch[0], Batch[883], Train loss: 0.04867539182305336\n",
      "Epoch[0], Val loss: 0.046962376683950424\n",
      "Epoch[0], Batch[884], Train loss: 0.04818161204457283\n",
      "Epoch[0], Val loss: 0.04528278484940529\n",
      "Epoch[0], Batch[885], Train loss: 0.051412202417850494\n",
      "Epoch[0], Val loss: 0.0478009469807148\n",
      "Epoch[0], Batch[886], Train loss: 0.04850427433848381\n",
      "Epoch[0], Val loss: 0.04569334536790848\n",
      "Epoch[0], Batch[887], Train loss: 0.050014201551675797\n",
      "Epoch[0], Val loss: 0.050572916865348816\n",
      "Epoch[0], Batch[888], Train loss: 0.05008852109313011\n",
      "Epoch[0], Val loss: 0.04363777115941048\n",
      "Epoch[0], Batch[889], Train loss: 0.049924589693546295\n",
      "Epoch[0], Val loss: 0.04678909108042717\n",
      "Epoch[0], Batch[890], Train loss: 0.04775642976164818\n",
      "Epoch[0], Val loss: 0.046285536140203476\n",
      "Epoch[0], Batch[891], Train loss: 0.051526088267564774\n",
      "Epoch[0], Val loss: 0.04535266384482384\n",
      "Epoch[0], Batch[892], Train loss: 0.048822417855262756\n",
      "Epoch[0], Val loss: 0.0456785224378109\n",
      "Epoch[0], Batch[893], Train loss: 0.04919099807739258\n",
      "Epoch[0], Val loss: 0.046281538903713226\n",
      "Epoch[0], Batch[894], Train loss: 0.050094932317733765\n",
      "Epoch[0], Val loss: 0.04536423459649086\n",
      "Epoch[0], Batch[895], Train loss: 0.048571906983852386\n",
      "Epoch[0], Val loss: 0.04814498871564865\n",
      "Epoch[0], Batch[896], Train loss: 0.05221642926335335\n",
      "Epoch[0], Val loss: 0.04597752168774605\n",
      "Epoch[0], Batch[897], Train loss: 0.04972011595964432\n",
      "Epoch[0], Val loss: 0.04636802151799202\n",
      "Epoch[0], Batch[898], Train loss: 0.047606341540813446\n",
      "Epoch[0], Val loss: 0.04643213003873825\n",
      "Epoch[0], Batch[899], Train loss: 0.0504569411277771\n",
      "Epoch[0], Val loss: 0.047024548053741455\n",
      "Epoch[0], Batch[900], Train loss: 0.05100136250257492\n",
      "Epoch[0], Val loss: 0.042175520211458206\n",
      "Epoch[0], Batch[901], Train loss: 0.05186593160033226\n",
      "Epoch[0], Val loss: 0.04641330987215042\n",
      "Epoch[0], Batch[902], Train loss: 0.0481514036655426\n",
      "Epoch[0], Val loss: 0.04831873998045921\n",
      "Epoch[0], Batch[903], Train loss: 0.050372663885354996\n",
      "Epoch[0], Val loss: 0.04641731083393097\n",
      "Epoch[0], Batch[904], Train loss: 0.05129547789692879\n",
      "Epoch[0], Val loss: 0.045829080045223236\n",
      "Epoch[0], Batch[905], Train loss: 0.04898076504468918\n",
      "Epoch[0], Val loss: 0.04757736250758171\n",
      "Epoch[0], Batch[906], Train loss: 0.04927608743309975\n",
      "Epoch[0], Val loss: 0.04825901612639427\n",
      "Epoch[0], Batch[907], Train loss: 0.04613219201564789\n",
      "Epoch[0], Val loss: 0.047674912959337234\n",
      "Epoch[0], Batch[908], Train loss: 0.0512777604162693\n",
      "Epoch[0], Val loss: 0.04536000266671181\n",
      "Epoch[0], Batch[909], Train loss: 0.04769447073340416\n",
      "Epoch[0], Val loss: 0.04491844028234482\n",
      "Epoch[0], Batch[910], Train loss: 0.04953940212726593\n",
      "Epoch[0], Val loss: 0.047357331961393356\n",
      "Epoch[0], Batch[911], Train loss: 0.051151175051927567\n",
      "Epoch[0], Val loss: 0.04603850841522217\n",
      "Epoch[0], Batch[912], Train loss: 0.05358995497226715\n",
      "Epoch[0], Val loss: 0.04568314924836159\n",
      "Epoch[0], Batch[913], Train loss: 0.04957699030637741\n",
      "Epoch[0], Val loss: 0.04844867065548897\n",
      "Epoch[0], Batch[914], Train loss: 0.05043187737464905\n",
      "Epoch[0], Val loss: 0.044932588934898376\n",
      "Epoch[0], Batch[915], Train loss: 0.05438626930117607\n",
      "Epoch[0], Val loss: 0.045830827206373215\n",
      "Epoch[0], Batch[916], Train loss: 0.05127802863717079\n",
      "Epoch[0], Val loss: 0.04472958296537399\n",
      "Epoch[0], Batch[917], Train loss: 0.047819580882787704\n",
      "Epoch[0], Val loss: 0.04843425750732422\n",
      "Epoch[0], Batch[918], Train loss: 0.05069415271282196\n",
      "Epoch[0], Val loss: 0.048736684024333954\n",
      "Epoch[0], Batch[919], Train loss: 0.048373859375715256\n",
      "Epoch[0], Val loss: 0.044255856424570084\n",
      "Epoch[0], Batch[920], Train loss: 0.048671264201402664\n",
      "Epoch[0], Val loss: 0.04856230691075325\n",
      "Epoch[0], Batch[921], Train loss: 0.05069352686405182\n",
      "Epoch[0], Val loss: 0.04629676789045334\n",
      "Epoch[0], Batch[922], Train loss: 0.04976971074938774\n",
      "Epoch[0], Val loss: 0.04553528130054474\n",
      "Epoch[0], Batch[923], Train loss: 0.04932013154029846\n",
      "Epoch[0], Val loss: 0.047279659658670425\n",
      "Epoch[0], Batch[924], Train loss: 0.047841187566518784\n",
      "Epoch[0], Val loss: 0.04461871087551117\n",
      "Epoch[0], Batch[925], Train loss: 0.04812287911772728\n",
      "Epoch[0], Val loss: 0.046343281865119934\n",
      "Epoch[0], Batch[926], Train loss: 0.048634257167577744\n",
      "Epoch[0], Val loss: 0.047954872250556946\n",
      "Epoch[0], Batch[927], Train loss: 0.05140290409326553\n",
      "Epoch[0], Val loss: 0.046686142683029175\n",
      "Epoch[0], Batch[928], Train loss: 0.05126253142952919\n",
      "Epoch[0], Val loss: 0.045641280710697174\n",
      "Epoch[0], Batch[929], Train loss: 0.04848300293087959\n",
      "Epoch[0], Val loss: 0.04682592302560806\n",
      "Epoch[0], Batch[930], Train loss: 0.0477418452501297\n",
      "Epoch[0], Val loss: 0.04560040682554245\n",
      "Epoch[0], Batch[931], Train loss: 0.047851428389549255\n",
      "Epoch[0], Val loss: 0.04477638751268387\n",
      "Epoch[0], Batch[932], Train loss: 0.04933037608861923\n",
      "Epoch[0], Val loss: 0.04505659267306328\n",
      "Epoch[0], Batch[933], Train loss: 0.05049309879541397\n",
      "Epoch[0], Val loss: 0.04717002063989639\n",
      "Epoch[0], Batch[934], Train loss: 0.04890952631831169\n",
      "Epoch[0], Val loss: 0.04665333032608032\n",
      "Epoch[0], Batch[935], Train loss: 0.049433983862400055\n",
      "Epoch[0], Val loss: 0.046928875148296356\n",
      "Epoch[0], Batch[936], Train loss: 0.05028977245092392\n",
      "Epoch[0], Val loss: 0.04493434727191925\n",
      "Epoch[0], Batch[937], Train loss: 0.050055064260959625\n",
      "Epoch[0], Val loss: 0.04574486240744591\n",
      "Epoch[0], Batch[938], Train loss: 0.047466304153203964\n",
      "Epoch[0], Val loss: 0.04564118757843971\n",
      "Epoch[0], Batch[939], Train loss: 0.05198601633310318\n",
      "Epoch[0], Val loss: 0.04479476809501648\n",
      "Epoch[0], Batch[940], Train loss: 0.048243578523397446\n",
      "Epoch[0], Val loss: 0.04646774008870125\n",
      "Epoch[0], Batch[941], Train loss: 0.04859035834670067\n",
      "Epoch[0], Val loss: 0.043307796120643616\n",
      "Epoch[0], Batch[942], Train loss: 0.048273466527462006\n",
      "Epoch[0], Val loss: 0.045229099690914154\n",
      "Epoch[0], Batch[943], Train loss: 0.04823625087738037\n",
      "Epoch[0], Val loss: 0.04283446446061134\n",
      "Epoch[0], Batch[944], Train loss: 0.052547283470630646\n",
      "Epoch[0], Val loss: 0.04780219867825508\n",
      "Epoch[0], Batch[945], Train loss: 0.049803346395492554\n",
      "Epoch[0], Val loss: 0.043788641691207886\n",
      "Epoch[0], Batch[946], Train loss: 0.04802800714969635\n",
      "Epoch[0], Val loss: 0.04454377293586731\n",
      "Epoch[0], Batch[947], Train loss: 0.04686610773205757\n",
      "Epoch[0], Val loss: 0.04460703581571579\n",
      "Epoch[0], Batch[948], Train loss: 0.048746258020401\n",
      "Epoch[0], Val loss: 0.044404298067092896\n",
      "Epoch[0], Batch[949], Train loss: 0.048774756491184235\n",
      "Epoch[0], Val loss: 0.0451669842004776\n",
      "Epoch[0], Batch[950], Train loss: 0.05299974977970123\n",
      "Epoch[0], Val loss: 0.046544332057237625\n",
      "Epoch[0], Batch[951], Train loss: 0.04999043047428131\n",
      "Epoch[0], Val loss: 0.047900352627038956\n",
      "Epoch[0], Batch[952], Train loss: 0.05034562572836876\n",
      "Epoch[0], Val loss: 0.044534895569086075\n",
      "Epoch[0], Batch[953], Train loss: 0.04878902807831764\n",
      "Epoch[0], Val loss: 0.043496742844581604\n",
      "Epoch[0], Batch[954], Train loss: 0.04856978356838226\n",
      "Epoch[0], Val loss: 0.050205256789922714\n",
      "Epoch[0], Batch[955], Train loss: 0.0479375496506691\n",
      "Epoch[0], Val loss: 0.04463516175746918\n",
      "Epoch[0], Batch[956], Train loss: 0.04991912841796875\n",
      "Epoch[0], Val loss: 0.046234045177698135\n",
      "Epoch[0], Batch[957], Train loss: 0.049143753945827484\n",
      "Epoch[0], Val loss: 0.045127466320991516\n",
      "Epoch[0], Batch[958], Train loss: 0.0509878471493721\n",
      "Epoch[0], Val loss: 0.04389146715402603\n",
      "Epoch[0], Batch[959], Train loss: 0.047094181180000305\n",
      "Epoch[0], Val loss: 0.04462510719895363\n",
      "Epoch[0], Batch[960], Train loss: 0.05208108574151993\n",
      "Epoch[0], Val loss: 0.047673165798187256\n",
      "Epoch[0], Batch[961], Train loss: 0.04783111810684204\n",
      "Epoch[0], Val loss: 0.04478370398283005\n",
      "Epoch[0], Batch[962], Train loss: 0.04875876381993294\n",
      "Epoch[0], Val loss: 0.04487372189760208\n",
      "Epoch[0], Batch[963], Train loss: 0.05276252329349518\n",
      "Epoch[0], Val loss: 0.045974936336278915\n",
      "Epoch[0], Batch[964], Train loss: 0.04954810440540314\n",
      "Epoch[0], Val loss: 0.04558132216334343\n",
      "Epoch[0], Batch[965], Train loss: 0.05028631165623665\n",
      "Epoch[0], Val loss: 0.045771799981594086\n",
      "Epoch[0], Batch[966], Train loss: 0.047128334641456604\n",
      "Epoch[0], Val loss: 0.044620051980018616\n",
      "Epoch[0], Batch[967], Train loss: 0.05061584711074829\n",
      "Epoch[0], Val loss: 0.04576833173632622\n",
      "Epoch[0], Batch[968], Train loss: 0.04677267745137215\n",
      "Epoch[0], Val loss: 0.046991340816020966\n",
      "Epoch[0], Batch[969], Train loss: 0.04632798954844475\n",
      "Epoch[0], Val loss: 0.048863403499126434\n",
      "Epoch[0], Batch[970], Train loss: 0.049721475690603256\n",
      "Epoch[0], Val loss: 0.044087596237659454\n",
      "Epoch[0], Batch[971], Train loss: 0.04728180542588234\n",
      "Epoch[0], Val loss: 0.04552280530333519\n",
      "Epoch[0], Batch[972], Train loss: 0.05048724263906479\n",
      "Epoch[0], Val loss: 0.049564018845558167\n",
      "Epoch[0], Batch[973], Train loss: 0.0528552271425724\n",
      "Epoch[0], Val loss: 0.04223562777042389\n",
      "Epoch[0], Batch[974], Train loss: 0.05172470957040787\n",
      "Epoch[0], Val loss: 0.045239146798849106\n",
      "Epoch[0], Batch[975], Train loss: 0.04510479047894478\n",
      "Epoch[0], Val loss: 0.05004321411252022\n",
      "Epoch[0], Batch[976], Train loss: 0.04825766757130623\n",
      "Epoch[0], Val loss: 0.046495817601680756\n",
      "Epoch[0], Batch[977], Train loss: 0.04772967845201492\n",
      "Epoch[0], Val loss: 0.04601627215743065\n",
      "Epoch[0], Batch[978], Train loss: 0.045714396983385086\n",
      "Epoch[0], Val loss: 0.04427008703351021\n",
      "Epoch[0], Batch[979], Train loss: 0.0455368347465992\n",
      "Epoch[0], Val loss: 0.043781183660030365\n",
      "Epoch[0], Batch[980], Train loss: 0.04653910920023918\n",
      "Epoch[0], Val loss: 0.04593248292803764\n",
      "Epoch[0], Batch[981], Train loss: 0.0500185564160347\n",
      "Epoch[0], Val loss: 0.04545370489358902\n",
      "Epoch[0], Batch[982], Train loss: 0.05042757838964462\n",
      "Epoch[0], Val loss: 0.04667900502681732\n",
      "Epoch[0], Batch[983], Train loss: 0.048107512295246124\n",
      "Epoch[0], Val loss: 0.04384150728583336\n",
      "Epoch[0], Batch[984], Train loss: 0.04582503065466881\n",
      "Epoch[0], Val loss: 0.04672984778881073\n",
      "Epoch[0], Batch[985], Train loss: 0.05068768933415413\n",
      "Epoch[0], Val loss: 0.043333448469638824\n",
      "Epoch[0], Batch[986], Train loss: 0.04911405220627785\n",
      "Epoch[0], Val loss: 0.04550163447856903\n",
      "Epoch[0], Batch[987], Train loss: 0.04759988188743591\n",
      "Epoch[0], Val loss: 0.04648885130882263\n",
      "Epoch[0], Batch[988], Train loss: 0.05001199245452881\n",
      "Epoch[0], Val loss: 0.04383322596549988\n",
      "Epoch[0], Batch[989], Train loss: 0.04938048869371414\n",
      "Epoch[0], Val loss: 0.04548459127545357\n",
      "Epoch[0], Batch[990], Train loss: 0.04602562636137009\n",
      "Epoch[0], Val loss: 0.043735504150390625\n",
      "Epoch[0], Batch[991], Train loss: 0.04983096197247505\n",
      "Epoch[0], Val loss: 0.04378050938248634\n",
      "Epoch[0], Batch[992], Train loss: 0.04551682248711586\n",
      "Epoch[0], Val loss: 0.04426330327987671\n",
      "Epoch[0], Batch[993], Train loss: 0.05065888538956642\n",
      "Epoch[0], Val loss: 0.0434940867125988\n",
      "Epoch[0], Batch[994], Train loss: 0.04887416586279869\n",
      "Epoch[0], Val loss: 0.04304800555109978\n",
      "Epoch[0], Batch[995], Train loss: 0.04553261026740074\n",
      "Epoch[0], Val loss: 0.044605642557144165\n",
      "Epoch[0], Batch[996], Train loss: 0.050618335604667664\n",
      "Epoch[0], Val loss: 0.046303655952215195\n",
      "Epoch[0], Batch[997], Train loss: 0.04948776587843895\n",
      "Epoch[0], Val loss: 0.04523792862892151\n",
      "Epoch[0], Batch[998], Train loss: 0.04695874825119972\n",
      "Epoch[0], Val loss: 0.04541970044374466\n",
      "Epoch[0], Batch[999], Train loss: 0.048693351447582245\n",
      "Epoch[0], Val loss: 0.0433490164577961\n",
      "Epoch[0], Batch[1000], Train loss: 0.045654404908418655\n",
      "Epoch[0], Val loss: 0.04664080590009689\n",
      "Epoch[0], Batch[1001], Train loss: 0.050369106233119965\n",
      "Epoch[0], Val loss: 0.04430420324206352\n",
      "Epoch[0], Batch[1002], Train loss: 0.04942543804645538\n",
      "Epoch[0], Val loss: 0.045991234481334686\n",
      "Epoch[0], Batch[1003], Train loss: 0.04617248475551605\n",
      "Epoch[0], Val loss: 0.04485602304339409\n",
      "Epoch[0], Batch[1004], Train loss: 0.0478978231549263\n",
      "Epoch[0], Val loss: 0.04518837481737137\n",
      "Epoch[0], Batch[1005], Train loss: 0.044486526399850845\n",
      "Epoch[0], Val loss: 0.0460408553481102\n",
      "Epoch[0], Batch[1006], Train loss: 0.04927412420511246\n",
      "Epoch[0], Val loss: 0.04511851817369461\n",
      "Epoch[0], Batch[1007], Train loss: 0.04931308701634407\n",
      "Epoch[0], Val loss: 0.04367445036768913\n",
      "Epoch[0], Batch[1008], Train loss: 0.049702323973178864\n",
      "Epoch[0], Val loss: 0.04509691148996353\n",
      "Epoch[0], Batch[1009], Train loss: 0.044948723167181015\n",
      "Epoch[0], Val loss: 0.04815321043133736\n",
      "Epoch[0], Batch[1010], Train loss: 0.04567040875554085\n",
      "Epoch[0], Val loss: 0.04510476067662239\n",
      "Epoch[0], Batch[1011], Train loss: 0.04572819918394089\n",
      "Epoch[0], Val loss: 0.04526855796575546\n",
      "Epoch[0], Batch[1012], Train loss: 0.048681337386369705\n",
      "Epoch[0], Val loss: 0.04578838124871254\n",
      "Epoch[0], Batch[1013], Train loss: 0.04723944142460823\n",
      "Epoch[0], Val loss: 0.045109834522008896\n",
      "Epoch[0], Batch[1014], Train loss: 0.04730464518070221\n",
      "Epoch[0], Val loss: 0.04562593623995781\n",
      "Epoch[0], Batch[1015], Train loss: 0.047314610332250595\n",
      "Epoch[0], Val loss: 0.04479430615901947\n",
      "Epoch[0], Batch[1016], Train loss: 0.049325522035360336\n",
      "Epoch[0], Val loss: 0.04251621663570404\n",
      "Epoch[0], Batch[1017], Train loss: 0.048273175954818726\n",
      "Epoch[0], Val loss: 0.044652510434389114\n",
      "Epoch[0], Batch[1018], Train loss: 0.04798994958400726\n",
      "Epoch[0], Val loss: 0.04636376351118088\n",
      "Epoch[0], Batch[1019], Train loss: 0.04764741286635399\n",
      "Epoch[0], Val loss: 0.045035190880298615\n",
      "Epoch[0], Batch[1020], Train loss: 0.04699651524424553\n",
      "Epoch[0], Val loss: 0.04543488472700119\n",
      "Epoch[0], Batch[1021], Train loss: 0.046773724257946014\n",
      "Epoch[0], Val loss: 0.047899290919303894\n",
      "Epoch[0], Batch[1022], Train loss: 0.05022889003157616\n",
      "Epoch[0], Val loss: 0.047621577978134155\n",
      "Epoch[0], Batch[1023], Train loss: 0.04654790088534355\n",
      "Epoch[0], Val loss: 0.04510490223765373\n",
      "Epoch[0], Batch[1024], Train loss: 0.04526616632938385\n",
      "Epoch[0], Val loss: 0.04401243105530739\n",
      "Epoch[0], Batch[1025], Train loss: 0.05077598616480827\n",
      "Epoch[0], Val loss: 0.04378478601574898\n",
      "Epoch[0], Batch[1026], Train loss: 0.046742551028728485\n",
      "Epoch[0], Val loss: 0.04633565992116928\n",
      "Epoch[0], Batch[1027], Train loss: 0.0475815087556839\n",
      "Epoch[0], Val loss: 0.04328799620270729\n",
      "Epoch[0], Batch[1028], Train loss: 0.04406191408634186\n",
      "Epoch[0], Val loss: 0.0456138476729393\n",
      "Epoch[0], Batch[1029], Train loss: 0.04564785584807396\n",
      "Epoch[0], Val loss: 0.04450193792581558\n",
      "Epoch[0], Batch[1030], Train loss: 0.04948877915740013\n",
      "Epoch[0], Val loss: 0.04701564833521843\n",
      "Epoch[0], Batch[1031], Train loss: 0.045862194150686264\n",
      "Epoch[0], Val loss: 0.04827676713466644\n",
      "Epoch[0], Batch[1032], Train loss: 0.04810197651386261\n",
      "Epoch[0], Val loss: 0.04562441259622574\n",
      "Epoch[0], Batch[1033], Train loss: 0.049583520740270615\n",
      "Epoch[0], Val loss: 0.042038071900606155\n",
      "Epoch[0], Batch[1034], Train loss: 0.04540819302201271\n",
      "Epoch[0], Val loss: 0.04222804307937622\n",
      "Epoch[0], Batch[1035], Train loss: 0.05007699877023697\n",
      "Epoch[0], Val loss: 0.048032667487859726\n",
      "Epoch[0], Batch[1036], Train loss: 0.05090651661157608\n",
      "Epoch[0], Val loss: 0.044670842587947845\n",
      "Epoch[0], Batch[1037], Train loss: 0.04529901221394539\n",
      "Epoch[0], Val loss: 0.044895295053720474\n",
      "Epoch[0], Batch[1038], Train loss: 0.04965456202626228\n",
      "Epoch[0], Val loss: 0.04442205652594566\n",
      "Epoch[0], Batch[1039], Train loss: 0.048267826437950134\n",
      "Epoch[0], Val loss: 0.04359300062060356\n",
      "Epoch[0], Batch[1040], Train loss: 0.047043900936841965\n",
      "Epoch[0], Val loss: 0.04133627936244011\n",
      "Epoch[0], Batch[1041], Train loss: 0.047812655568122864\n",
      "Epoch[0], Val loss: 0.040882717818021774\n",
      "Epoch[0], Batch[1042], Train loss: 0.04899156466126442\n",
      "Epoch[0], Val loss: 0.04492006078362465\n",
      "Epoch[0], Batch[1043], Train loss: 0.0473305843770504\n",
      "Epoch[0], Val loss: 0.042197391390800476\n",
      "Epoch[0], Batch[1044], Train loss: 0.05057374760508537\n",
      "Epoch[0], Val loss: 0.04758082702755928\n",
      "Epoch[0], Batch[1045], Train loss: 0.04711974412202835\n",
      "Epoch[0], Val loss: 0.04421697184443474\n",
      "Epoch[0], Batch[1046], Train loss: 0.04650552198290825\n",
      "Epoch[0], Val loss: 0.04545280337333679\n",
      "Epoch[0], Batch[1047], Train loss: 0.04685983061790466\n",
      "Epoch[0], Val loss: 0.0421973392367363\n",
      "Epoch[0], Batch[1048], Train loss: 0.047726888209581375\n",
      "Epoch[0], Val loss: 0.045161712914705276\n",
      "Epoch[0], Batch[1049], Train loss: 0.04987732321023941\n",
      "Epoch[0], Val loss: 0.04565172642469406\n",
      "Epoch[0], Batch[1050], Train loss: 0.052405741065740585\n",
      "Epoch[0], Val loss: 0.04174600914120674\n",
      "Epoch[0], Batch[1051], Train loss: 0.04692869260907173\n",
      "Epoch[0], Val loss: 0.04984454810619354\n",
      "Epoch[0], Batch[1052], Train loss: 0.04970123991370201\n",
      "Epoch[0], Val loss: 0.045617688447237015\n",
      "Epoch[0], Batch[1053], Train loss: 0.0475795604288578\n",
      "Epoch[0], Val loss: 0.04482586681842804\n",
      "Epoch[0], Batch[1054], Train loss: 0.05256860330700874\n",
      "Epoch[0], Val loss: 0.04580599442124367\n",
      "Epoch[0], Batch[1055], Train loss: 0.04778452217578888\n",
      "Epoch[0], Val loss: 0.045140232890844345\n",
      "Epoch[0], Batch[1056], Train loss: 0.045996181666851044\n",
      "Epoch[0], Val loss: 0.043455593287944794\n",
      "Epoch[0], Batch[1057], Train loss: 0.04552140831947327\n",
      "Epoch[0], Val loss: 0.04105966165661812\n",
      "Epoch[0], Batch[1058], Train loss: 0.048334479331970215\n",
      "Epoch[0], Val loss: 0.04529387876391411\n",
      "Epoch[0], Batch[1059], Train loss: 0.04832426458597183\n",
      "Epoch[0], Val loss: 0.04402582719922066\n",
      "Epoch[0], Batch[1060], Train loss: 0.04560632258653641\n",
      "Epoch[0], Val loss: 0.045426104217767715\n",
      "Epoch[0], Batch[1061], Train loss: 0.046594586223363876\n",
      "Epoch[0], Val loss: 0.044256653636693954\n",
      "Epoch[0], Batch[1062], Train loss: 0.0481560193002224\n",
      "Epoch[0], Val loss: 0.044334616512060165\n",
      "Epoch[0], Batch[1063], Train loss: 0.05011807009577751\n",
      "Epoch[0], Val loss: 0.04466108977794647\n",
      "Epoch[0], Batch[1064], Train loss: 0.04481806233525276\n",
      "Epoch[0], Val loss: 0.04470646753907204\n",
      "Epoch[0], Batch[1065], Train loss: 0.047687333077192307\n",
      "Epoch[0], Val loss: 0.04776208475232124\n",
      "Epoch[0], Batch[1066], Train loss: 0.04857916384935379\n",
      "Epoch[0], Val loss: 0.04412711784243584\n",
      "Epoch[0], Batch[1067], Train loss: 0.049124255776405334\n",
      "Epoch[0], Val loss: 0.044457562267780304\n",
      "Epoch[0], Batch[1068], Train loss: 0.048782046884298325\n",
      "Epoch[0], Val loss: 0.04564939811825752\n",
      "Epoch[0], Batch[1069], Train loss: 0.0468454584479332\n",
      "Epoch[0], Val loss: 0.04219174385070801\n",
      "Epoch[0], Batch[1070], Train loss: 0.042228471487760544\n",
      "Epoch[0], Val loss: 0.04698657989501953\n",
      "Epoch[0], Batch[1071], Train loss: 0.04622485488653183\n",
      "Epoch[0], Val loss: 0.0445370078086853\n",
      "Epoch[0], Batch[1072], Train loss: 0.04530126228928566\n",
      "Epoch[0], Val loss: 0.04501878097653389\n",
      "Epoch[0], Batch[1073], Train loss: 0.046405334025621414\n",
      "Epoch[0], Val loss: 0.045842427760362625\n",
      "Epoch[0], Batch[1074], Train loss: 0.04588963836431503\n",
      "Epoch[0], Val loss: 0.04541677236557007\n",
      "Epoch[0], Batch[1075], Train loss: 0.046064600348472595\n",
      "Epoch[0], Val loss: 0.046203527599573135\n",
      "Epoch[0], Batch[1076], Train loss: 0.04897753894329071\n",
      "Epoch[0], Val loss: 0.04287438094615936\n",
      "Epoch[0], Batch[1077], Train loss: 0.04730439558625221\n",
      "Epoch[0], Val loss: 0.04415016621351242\n",
      "Epoch[0], Batch[1078], Train loss: 0.04581213742494583\n",
      "Epoch[0], Val loss: 0.042558252811431885\n",
      "Epoch[0], Batch[1079], Train loss: 0.04648326337337494\n",
      "Epoch[0], Val loss: 0.04337746649980545\n",
      "Epoch[0], Batch[1080], Train loss: 0.04652822017669678\n",
      "Epoch[0], Val loss: 0.04219673201441765\n",
      "Epoch[0], Batch[1081], Train loss: 0.04885442554950714\n",
      "Epoch[0], Val loss: 0.0462372750043869\n",
      "Epoch[0], Batch[1082], Train loss: 0.045749880373477936\n",
      "Epoch[0], Val loss: 0.045101530849933624\n",
      "Epoch[0], Batch[1083], Train loss: 0.05467817559838295\n",
      "Epoch[0], Val loss: 0.0466310940682888\n",
      "Epoch[0], Batch[1084], Train loss: 0.048365239053964615\n",
      "Epoch[0], Val loss: 0.044095296412706375\n",
      "Epoch[0], Batch[1085], Train loss: 0.04513421282172203\n",
      "Epoch[0], Val loss: 0.044170357286930084\n",
      "Epoch[0], Batch[1086], Train loss: 0.05044933781027794\n",
      "Epoch[0], Val loss: 0.042330898344516754\n",
      "Epoch[0], Batch[1087], Train loss: 0.046674564480781555\n",
      "Epoch[0], Val loss: 0.04403572157025337\n",
      "Epoch[0], Batch[1088], Train loss: 0.04685826972126961\n",
      "Epoch[0], Val loss: 0.044281311333179474\n",
      "Epoch[0], Batch[1089], Train loss: 0.047496844083070755\n",
      "Epoch[0], Val loss: 0.043520815670490265\n",
      "Epoch[0], Batch[1090], Train loss: 0.04831811413168907\n",
      "Epoch[0], Val loss: 0.04542280361056328\n",
      "Epoch[0], Batch[1091], Train loss: 0.04859568178653717\n",
      "Epoch[0], Val loss: 0.047377318143844604\n",
      "Epoch[0], Batch[1092], Train loss: 0.049781639128923416\n",
      "Epoch[0], Val loss: 0.04623371735215187\n",
      "Epoch[0], Batch[1093], Train loss: 0.04663829505443573\n",
      "Epoch[0], Val loss: 0.04526538774371147\n",
      "Epoch[0], Batch[1094], Train loss: 0.04649510234594345\n",
      "Epoch[0], Val loss: 0.04446728155016899\n",
      "Epoch[0], Batch[1095], Train loss: 0.05118938162922859\n",
      "Epoch[0], Val loss: 0.0429394356906414\n",
      "Epoch[0], Batch[1096], Train loss: 0.04739377275109291\n",
      "Epoch[0], Val loss: 0.04546928033232689\n",
      "Epoch[0], Batch[1097], Train loss: 0.046399958431720734\n",
      "Epoch[0], Val loss: 0.04479018598794937\n",
      "Epoch[0], Batch[1098], Train loss: 0.046213340014219284\n",
      "Epoch[0], Val loss: 0.041513893753290176\n",
      "Epoch[0], Batch[1099], Train loss: 0.047553371638059616\n",
      "Epoch[0], Val loss: 0.0415167436003685\n",
      "Epoch[0], Batch[1100], Train loss: 0.047671567648649216\n",
      "Epoch[0], Val loss: 0.04361838474869728\n",
      "Epoch[0], Batch[1101], Train loss: 0.04780955612659454\n",
      "Epoch[0], Val loss: 0.04545912146568298\n",
      "Epoch[0], Batch[1102], Train loss: 0.04671679064631462\n",
      "Epoch[0], Val loss: 0.04555412009358406\n",
      "Epoch[0], Batch[1103], Train loss: 0.04718833416700363\n",
      "Epoch[0], Val loss: 0.04475899785757065\n",
      "Epoch[0], Batch[1104], Train loss: 0.046577394008636475\n",
      "Epoch[0], Val loss: 0.041892945766448975\n",
      "Epoch[0], Batch[1105], Train loss: 0.0488710030913353\n",
      "Epoch[0], Val loss: 0.04400923475623131\n",
      "Epoch[0], Batch[1106], Train loss: 0.0458320677280426\n",
      "Epoch[0], Val loss: 0.04359963536262512\n",
      "Epoch[0], Batch[1107], Train loss: 0.044407621026039124\n",
      "Epoch[0], Val loss: 0.04154330864548683\n",
      "Epoch[0], Batch[1108], Train loss: 0.05036817863583565\n",
      "Epoch[0], Val loss: 0.04469839483499527\n",
      "Epoch[0], Batch[1109], Train loss: 0.04620660841464996\n",
      "Epoch[0], Val loss: 0.040924083441495895\n",
      "Epoch[0], Batch[1110], Train loss: 0.045503780245780945\n",
      "Epoch[0], Val loss: 0.04625023528933525\n",
      "Epoch[0], Batch[1111], Train loss: 0.04395085200667381\n",
      "Epoch[0], Val loss: 0.04349980130791664\n",
      "Epoch[0], Batch[1112], Train loss: 0.04449412226676941\n",
      "Epoch[0], Val loss: 0.047029655426740646\n",
      "Epoch[0], Batch[1113], Train loss: 0.04600901156663895\n",
      "Epoch[0], Val loss: 0.04163757711648941\n",
      "Epoch[0], Batch[1114], Train loss: 0.04570489376783371\n",
      "Epoch[0], Val loss: 0.0443335622549057\n",
      "Epoch[0], Batch[1115], Train loss: 0.047691408544778824\n",
      "Epoch[0], Val loss: 0.043314650654792786\n",
      "Epoch[0], Batch[1116], Train loss: 0.04580080136656761\n",
      "Epoch[0], Val loss: 0.045044515281915665\n",
      "Epoch[0], Batch[1117], Train loss: 0.04814678803086281\n",
      "Epoch[0], Val loss: 0.04179735109210014\n",
      "Epoch[0], Batch[1118], Train loss: 0.04490070417523384\n",
      "Epoch[0], Val loss: 0.04502434656023979\n",
      "Epoch[0], Batch[1119], Train loss: 0.0465882271528244\n",
      "Epoch[0], Val loss: 0.045173279941082\n",
      "Epoch[0], Batch[1120], Train loss: 0.04438137635588646\n",
      "Epoch[0], Val loss: 0.043462395668029785\n",
      "Epoch[0], Batch[1121], Train loss: 0.0506603829562664\n",
      "Epoch[0], Val loss: 0.04457048326730728\n",
      "Epoch[0], Batch[1122], Train loss: 0.045274969190359116\n",
      "Epoch[0], Val loss: 0.041455186903476715\n",
      "Epoch[0], Batch[1123], Train loss: 0.04763687029480934\n",
      "Epoch[0], Val loss: 0.0448002815246582\n",
      "Epoch[0], Batch[1124], Train loss: 0.052498508244752884\n",
      "Epoch[0], Val loss: 0.04325759410858154\n",
      "Epoch[0], Batch[1125], Train loss: 0.046620409935712814\n",
      "Epoch[0], Val loss: 0.04421282187104225\n",
      "Epoch[0], Batch[1126], Train loss: 0.044502142816782\n",
      "Epoch[0], Val loss: 0.04316268861293793\n",
      "Epoch[0], Batch[1127], Train loss: 0.04543733969330788\n",
      "Epoch[0], Val loss: 0.04471532255411148\n",
      "Epoch[0], Batch[1128], Train loss: 0.0472169928252697\n",
      "Epoch[0], Val loss: 0.0407961942255497\n",
      "Epoch[0], Batch[1129], Train loss: 0.04453088715672493\n",
      "Epoch[0], Val loss: 0.044795721769332886\n",
      "Epoch[0], Batch[1130], Train loss: 0.049624521285295486\n",
      "Epoch[0], Val loss: 0.044969331473112106\n",
      "Epoch[0], Batch[1131], Train loss: 0.046431764960289\n",
      "Epoch[0], Val loss: 0.046194277703762054\n",
      "Epoch[0], Batch[1132], Train loss: 0.04580998793244362\n",
      "Epoch[0], Val loss: 0.04537040367722511\n",
      "Epoch[0], Batch[1133], Train loss: 0.04718545824289322\n",
      "Epoch[0], Val loss: 0.044360995292663574\n",
      "Epoch[0], Batch[1134], Train loss: 0.04391895979642868\n",
      "Epoch[0], Val loss: 0.04432494938373566\n",
      "Epoch[0], Batch[1135], Train loss: 0.04708356037735939\n",
      "Epoch[0], Val loss: 0.04526779428124428\n",
      "Epoch[0], Batch[1136], Train loss: 0.04773236811161041\n",
      "Epoch[0], Val loss: 0.042641233652830124\n",
      "Epoch[0], Batch[1137], Train loss: 0.04723651707172394\n",
      "Epoch[0], Val loss: 0.04397561773657799\n",
      "Epoch[0], Batch[1138], Train loss: 0.04760535806417465\n",
      "Epoch[0], Val loss: 0.04285511374473572\n",
      "Epoch[0], Batch[1139], Train loss: 0.04683242738246918\n",
      "Epoch[0], Val loss: 0.04295011982321739\n",
      "Epoch[0], Batch[1140], Train loss: 0.04745015874505043\n",
      "Epoch[0], Val loss: 0.04258889704942703\n",
      "Epoch[0], Batch[1141], Train loss: 0.04814071208238602\n",
      "Epoch[0], Val loss: 0.04550624266266823\n",
      "Epoch[0], Batch[1142], Train loss: 0.04524686560034752\n",
      "Epoch[0], Val loss: 0.04008530080318451\n",
      "Epoch[0], Batch[1143], Train loss: 0.048547349870204926\n",
      "Epoch[0], Val loss: 0.04610642418265343\n",
      "Epoch[0], Batch[1144], Train loss: 0.04752713441848755\n",
      "Epoch[0], Val loss: 0.045210130512714386\n",
      "Epoch[0], Batch[1145], Train loss: 0.04754796624183655\n",
      "Epoch[0], Val loss: 0.046569839119911194\n",
      "Epoch[0], Batch[1146], Train loss: 0.04704377427697182\n",
      "Epoch[0], Val loss: 0.041658807545900345\n",
      "Epoch[0], Batch[1147], Train loss: 0.04778558015823364\n",
      "Epoch[0], Val loss: 0.04269681125879288\n",
      "Epoch[0], Batch[1148], Train loss: 0.04574621841311455\n",
      "Epoch[0], Val loss: 0.04433620348572731\n",
      "Epoch[0], Batch[1149], Train loss: 0.045072104781866074\n",
      "Epoch[0], Val loss: 0.0439891591668129\n",
      "Epoch[0], Batch[1150], Train loss: 0.04798387363553047\n",
      "Epoch[0], Val loss: 0.04603899270296097\n",
      "Epoch[0], Batch[1151], Train loss: 0.044793788343667984\n",
      "Epoch[0], Val loss: 0.04141070321202278\n",
      "Epoch[0], Batch[1152], Train loss: 0.04720412194728851\n",
      "Epoch[0], Val loss: 0.04292650148272514\n",
      "Epoch[0], Batch[1153], Train loss: 0.050025999546051025\n",
      "Epoch[0], Val loss: 0.04433702304959297\n",
      "Epoch[0], Batch[1154], Train loss: 0.04632774367928505\n",
      "Epoch[0], Val loss: 0.04334498569369316\n",
      "Epoch[0], Batch[1155], Train loss: 0.04998137429356575\n",
      "Epoch[0], Val loss: 0.04158346354961395\n",
      "Epoch[0], Batch[1156], Train loss: 0.05135839805006981\n",
      "Epoch[0], Val loss: 0.04109896719455719\n",
      "Epoch[0], Batch[1157], Train loss: 0.04544210433959961\n",
      "Epoch[0], Val loss: 0.044382065534591675\n",
      "Epoch[0], Batch[1158], Train loss: 0.0449470579624176\n",
      "Epoch[0], Val loss: 0.04388831928372383\n",
      "Epoch[0], Batch[1159], Train loss: 0.045148540288209915\n",
      "Epoch[0], Val loss: 0.04188603162765503\n",
      "Epoch[0], Batch[1160], Train loss: 0.04794606938958168\n",
      "Epoch[0], Val loss: 0.04433741793036461\n",
      "Epoch[0], Batch[1161], Train loss: 0.04717404767870903\n",
      "Epoch[0], Val loss: 0.04541837051510811\n",
      "Epoch[0], Batch[1162], Train loss: 0.04852176457643509\n",
      "Epoch[0], Val loss: 0.04425786808133125\n",
      "Epoch[0], Batch[1163], Train loss: 0.04664554446935654\n",
      "Epoch[0], Val loss: 0.04091404005885124\n",
      "Epoch[0], Batch[1164], Train loss: 0.046137675642967224\n",
      "Epoch[0], Val loss: 0.045541003346443176\n",
      "Epoch[0], Batch[1165], Train loss: 0.04447980225086212\n",
      "Epoch[0], Val loss: 0.04716740548610687\n",
      "Epoch[0], Batch[1166], Train loss: 0.0479767769575119\n",
      "Epoch[0], Val loss: 0.043099477887153625\n",
      "Epoch[0], Batch[1167], Train loss: 0.049095939844846725\n",
      "Epoch[0], Val loss: 0.04731854423880577\n",
      "Epoch[0], Batch[1168], Train loss: 0.04909240081906319\n",
      "Epoch[0], Val loss: 0.043397314846515656\n",
      "Epoch[0], Batch[1169], Train loss: 0.048337653279304504\n",
      "Epoch[0], Val loss: 0.04433683678507805\n",
      "Epoch[0], Batch[1170], Train loss: 0.0471968837082386\n",
      "Epoch[0], Val loss: 0.04589183256030083\n",
      "Epoch[0], Batch[1171], Train loss: 0.048272158950567245\n",
      "Epoch[0], Val loss: 0.044491760432720184\n",
      "Epoch[0], Batch[1172], Train loss: 0.04927939549088478\n",
      "Epoch[0], Val loss: 0.0454496406018734\n",
      "Epoch[0], Batch[1173], Train loss: 0.04728613793849945\n",
      "Epoch[0], Val loss: 0.04631777107715607\n",
      "Epoch[0], Batch[1174], Train loss: 0.05139929801225662\n",
      "Epoch[0], Val loss: 0.042727261781692505\n",
      "Epoch[0], Batch[1175], Train loss: 0.047202128916978836\n",
      "Epoch[0], Val loss: 0.04430883377790451\n",
      "Epoch[0], Batch[1176], Train loss: 0.04614344611763954\n",
      "Epoch[0], Val loss: 0.04481443017721176\n",
      "Epoch[0], Batch[1177], Train loss: 0.047241367399692535\n",
      "Epoch[0], Val loss: 0.045047078281641006\n",
      "Epoch[0], Batch[1178], Train loss: 0.05050714313983917\n",
      "Epoch[0], Val loss: 0.04566650837659836\n",
      "Epoch[0], Batch[1179], Train loss: 0.04497598484158516\n",
      "Epoch[0], Val loss: 0.04361381754279137\n",
      "Epoch[0], Batch[1180], Train loss: 0.04861140623688698\n",
      "Epoch[0], Val loss: 0.04452614486217499\n",
      "Epoch[0], Batch[1181], Train loss: 0.046006616204977036\n",
      "Epoch[0], Val loss: 0.04454375058412552\n",
      "Epoch[0], Batch[1182], Train loss: 0.04632307589054108\n",
      "Epoch[0], Val loss: 0.04332321137189865\n",
      "Epoch[0], Batch[1183], Train loss: 0.045967601239681244\n",
      "Epoch[0], Val loss: 0.04560816287994385\n",
      "Epoch[0], Batch[1184], Train loss: 0.04861758276820183\n",
      "Epoch[0], Val loss: 0.043500643223524094\n",
      "Epoch[0], Batch[1185], Train loss: 0.04667291417717934\n",
      "Epoch[0], Val loss: 0.04514913260936737\n",
      "Epoch[0], Batch[1186], Train loss: 0.04951995611190796\n",
      "Epoch[0], Val loss: 0.044512733817100525\n",
      "Epoch[0], Batch[1187], Train loss: 0.04579252004623413\n",
      "Epoch[0], Val loss: 0.045593228191137314\n",
      "Epoch[0], Batch[1188], Train loss: 0.04701540991663933\n",
      "Epoch[0], Val loss: 0.04149181768298149\n",
      "Epoch[0], Batch[1189], Train loss: 0.047137901186943054\n",
      "Epoch[0], Val loss: 0.04201309755444527\n",
      "Epoch[0], Batch[1190], Train loss: 0.04819819703698158\n",
      "Epoch[0], Val loss: 0.04451049864292145\n",
      "Epoch[0], Batch[1191], Train loss: 0.047452412545681\n",
      "Epoch[0], Val loss: 0.04317094758152962\n",
      "Epoch[0], Batch[1192], Train loss: 0.04838337004184723\n",
      "Epoch[0], Val loss: 0.04533563554286957\n",
      "Epoch[0], Batch[1193], Train loss: 0.04343778267502785\n",
      "Epoch[0], Val loss: 0.043942809104919434\n",
      "Epoch[0], Batch[1194], Train loss: 0.04553660750389099\n",
      "Epoch[0], Val loss: 0.042353011667728424\n",
      "Epoch[0], Batch[1195], Train loss: 0.04516071453690529\n",
      "Epoch[0], Val loss: 0.046172693371772766\n",
      "Epoch[0], Batch[1196], Train loss: 0.04256998375058174\n",
      "Epoch[0], Val loss: 0.04318663477897644\n",
      "Epoch[0], Batch[1197], Train loss: 0.04465201869606972\n",
      "Epoch[0], Val loss: 0.04342687502503395\n",
      "Epoch[0], Batch[1198], Train loss: 0.04796947166323662\n",
      "Epoch[0], Val loss: 0.044147323817014694\n",
      "Epoch[0], Batch[1199], Train loss: 0.04625542461872101\n",
      "Epoch[0], Val loss: 0.043253861367702484\n",
      "Epoch[0], Batch[1200], Train loss: 0.042646776884794235\n",
      "Epoch[0], Val loss: 0.041552115231752396\n",
      "Epoch[0], Batch[1201], Train loss: 0.04807208105921745\n",
      "Epoch[0], Val loss: 0.0468839593231678\n",
      "Epoch[0], Batch[1202], Train loss: 0.044341400265693665\n",
      "Epoch[0], Val loss: 0.04245110973715782\n",
      "Epoch[0], Batch[1203], Train loss: 0.05110708624124527\n",
      "Epoch[0], Val loss: 0.04170851781964302\n",
      "Epoch[0], Batch[1204], Train loss: 0.046111442148685455\n",
      "Epoch[0], Val loss: 0.04315292462706566\n",
      "Epoch[0], Batch[1205], Train loss: 0.04852529242634773\n",
      "Epoch[0], Val loss: 0.04672110080718994\n",
      "Epoch[0], Batch[1206], Train loss: 0.04671312868595123\n",
      "Epoch[0], Val loss: 0.04376601800322533\n",
      "Epoch[0], Batch[1207], Train loss: 0.046113234013319016\n",
      "Epoch[0], Val loss: 0.04493768513202667\n",
      "Epoch[0], Batch[1208], Train loss: 0.04592340439558029\n",
      "Epoch[0], Val loss: 0.04638945311307907\n",
      "Epoch[0], Batch[1209], Train loss: 0.0444754995405674\n",
      "Epoch[0], Val loss: 0.0432359054684639\n",
      "Epoch[0], Batch[1210], Train loss: 0.045856986194849014\n",
      "Epoch[0], Val loss: 0.04406803846359253\n",
      "Epoch[0], Batch[1211], Train loss: 0.045596130192279816\n",
      "Epoch[0], Val loss: 0.044462062418460846\n",
      "Epoch[0], Batch[1212], Train loss: 0.04519578441977501\n",
      "Epoch[0], Val loss: 0.04558120295405388\n",
      "Epoch[0], Batch[1213], Train loss: 0.04422276094555855\n",
      "Epoch[0], Val loss: 0.04267728328704834\n",
      "Epoch[0], Batch[1214], Train loss: 0.04602512717247009\n",
      "Epoch[0], Val loss: 0.0464162677526474\n",
      "Epoch[0], Batch[1215], Train loss: 0.04459647089242935\n",
      "Epoch[0], Val loss: 0.043105632066726685\n",
      "Epoch[0], Batch[1216], Train loss: 0.048758912831544876\n",
      "Epoch[0], Val loss: 0.04374656081199646\n",
      "Epoch[0], Batch[1217], Train loss: 0.04448322206735611\n",
      "Epoch[0], Val loss: 0.04562893882393837\n",
      "Epoch[0], Batch[1218], Train loss: 0.04660547524690628\n",
      "Epoch[0], Val loss: 0.0455203540623188\n",
      "Epoch[0], Batch[1219], Train loss: 0.04591479152441025\n",
      "Epoch[0], Val loss: 0.04175432398915291\n",
      "Epoch[0], Batch[1220], Train loss: 0.04516429081559181\n",
      "Epoch[0], Val loss: 0.042976152151823044\n",
      "Epoch[0], Batch[1221], Train loss: 0.04695432260632515\n",
      "Epoch[0], Val loss: 0.04041086882352829\n",
      "Epoch[0], Batch[1222], Train loss: 0.04431503266096115\n",
      "Epoch[0], Val loss: 0.040706876665353775\n",
      "Epoch[0], Batch[1223], Train loss: 0.04516911506652832\n",
      "Epoch[0], Val loss: 0.04145779088139534\n",
      "Epoch[0], Batch[1224], Train loss: 0.046738430857658386\n",
      "Epoch[0], Val loss: 0.04327040910720825\n",
      "Epoch[0], Batch[1225], Train loss: 0.04707903414964676\n",
      "Epoch[0], Val loss: 0.042373936623334885\n",
      "Epoch[0], Batch[1226], Train loss: 0.049829985946416855\n",
      "Epoch[0], Val loss: 0.04070044308900833\n",
      "Epoch[0], Batch[1227], Train loss: 0.04523485526442528\n",
      "Epoch[0], Val loss: 0.042013075202703476\n",
      "Epoch[0], Batch[1228], Train loss: 0.045366160571575165\n",
      "Epoch[0], Val loss: 0.045794736593961716\n",
      "Epoch[0], Batch[1229], Train loss: 0.04395819082856178\n",
      "Epoch[0], Val loss: 0.04347814619541168\n",
      "Epoch[0], Batch[1230], Train loss: 0.04956304281949997\n",
      "Epoch[0], Val loss: 0.04213086515665054\n",
      "Epoch[0], Batch[1231], Train loss: 0.04573000967502594\n",
      "Epoch[0], Val loss: 0.045135077089071274\n",
      "Epoch[0], Batch[1232], Train loss: 0.04439322277903557\n",
      "Epoch[0], Val loss: 0.04551117494702339\n",
      "Epoch[0], Batch[1233], Train loss: 0.04790955036878586\n",
      "Epoch[0], Val loss: 0.0427287220954895\n",
      "Epoch[0], Batch[1234], Train loss: 0.04491133987903595\n",
      "Epoch[0], Val loss: 0.04286918416619301\n",
      "Epoch[0], Batch[1235], Train loss: 0.046337708830833435\n",
      "Epoch[0], Val loss: 0.04352334514260292\n",
      "Epoch[0], Batch[1236], Train loss: 0.04529384523630142\n",
      "Epoch[0], Val loss: 0.04241570457816124\n",
      "Epoch[0], Batch[1237], Train loss: 0.04492964595556259\n",
      "Epoch[0], Val loss: 0.04205404967069626\n",
      "Epoch[0], Batch[1238], Train loss: 0.04509519413113594\n",
      "Epoch[0], Val loss: 0.04482068121433258\n",
      "Epoch[0], Batch[1239], Train loss: 0.04695653170347214\n",
      "Epoch[0], Val loss: 0.04292529448866844\n",
      "Epoch[0], Batch[1240], Train loss: 0.04680132865905762\n",
      "Epoch[0], Val loss: 0.044075772166252136\n",
      "Epoch[0], Batch[1241], Train loss: 0.046507515013217926\n",
      "Epoch[0], Val loss: 0.039533235132694244\n",
      "Epoch[0], Batch[1242], Train loss: 0.04873688146471977\n",
      "Epoch[0], Val loss: 0.044102106243371964\n",
      "Epoch[0], Batch[1243], Train loss: 0.046753592789173126\n",
      "Epoch[0], Val loss: 0.04159662872552872\n",
      "Epoch[0], Batch[1244], Train loss: 0.04433465376496315\n",
      "Epoch[0], Val loss: 0.04489043727517128\n",
      "Epoch[0], Batch[1245], Train loss: 0.045418135821819305\n",
      "Epoch[0], Val loss: 0.043714363127946854\n",
      "Epoch[0], Batch[1246], Train loss: 0.046882081776857376\n",
      "Epoch[0], Val loss: 0.04257551208138466\n",
      "Epoch[0], Batch[1247], Train loss: 0.04702027142047882\n",
      "Epoch[0], Val loss: 0.044819485396146774\n",
      "Epoch[0], Batch[1248], Train loss: 0.043340958654880524\n",
      "Epoch[0], Val loss: 0.04337602108716965\n",
      "Epoch[0], Batch[1249], Train loss: 0.04703570529818535\n",
      "Epoch[0], Val loss: 0.04196211323142052\n",
      "Epoch[0], Batch[1250], Train loss: 0.04643047973513603\n",
      "Epoch[0], Val loss: 0.04147589951753616\n",
      "Epoch[0], Batch[1251], Train loss: 0.0453130304813385\n",
      "Epoch[0], Val loss: 0.04157654196023941\n",
      "Epoch[0], Batch[1252], Train loss: 0.044353801757097244\n",
      "Epoch[0], Val loss: 0.03933406621217728\n",
      "Epoch[0], Batch[1253], Train loss: 0.04540405794978142\n",
      "Epoch[0], Val loss: 0.04281284287571907\n",
      "Epoch[0], Batch[1254], Train loss: 0.04906715825200081\n",
      "Epoch[0], Val loss: 0.04439415782690048\n",
      "Epoch[0], Batch[1255], Train loss: 0.04361619055271149\n",
      "Epoch[0], Val loss: 0.044352736324071884\n",
      "Epoch[0], Batch[1256], Train loss: 0.04532531276345253\n",
      "Epoch[0], Val loss: 0.043280135840177536\n",
      "Epoch[0], Batch[1257], Train loss: 0.04772205650806427\n",
      "Epoch[0], Val loss: 0.041135482490062714\n",
      "Epoch[0], Batch[1258], Train loss: 0.04450434818863869\n",
      "Epoch[0], Val loss: 0.044139616191387177\n",
      "Epoch[0], Batch[1259], Train loss: 0.04478823393583298\n",
      "Epoch[0], Val loss: 0.0439470037817955\n",
      "Epoch[0], Batch[1260], Train loss: 0.046950388699769974\n",
      "Epoch[0], Val loss: 0.04880424961447716\n",
      "Epoch[0], Batch[1261], Train loss: 0.04642844572663307\n",
      "Epoch[0], Val loss: 0.04410330206155777\n",
      "Epoch[0], Batch[1262], Train loss: 0.04703601822257042\n",
      "Epoch[0], Val loss: 0.04327818378806114\n",
      "Epoch[0], Batch[1263], Train loss: 0.0456523597240448\n",
      "Epoch[0], Val loss: 0.04059864208102226\n",
      "Epoch[0], Batch[1264], Train loss: 0.04687024652957916\n",
      "Epoch[0], Val loss: 0.04112044721841812\n",
      "Epoch[0], Batch[1265], Train loss: 0.04418515786528587\n",
      "Epoch[0], Val loss: 0.04323536902666092\n",
      "Epoch[0], Batch[1266], Train loss: 0.045048635452985764\n",
      "Epoch[0], Val loss: 0.05002996325492859\n",
      "Epoch[0], Batch[1267], Train loss: 0.045438893139362335\n",
      "Epoch[0], Val loss: 0.04175972938537598\n",
      "Epoch[0], Batch[1268], Train loss: 0.04947333037853241\n",
      "Epoch[0], Val loss: 0.04102090746164322\n",
      "Epoch[0], Batch[1269], Train loss: 0.043132681399583817\n",
      "Epoch[0], Val loss: 0.04106803238391876\n",
      "Epoch[0], Batch[1270], Train loss: 0.044195905327796936\n",
      "Epoch[0], Val loss: 0.043211545795202255\n",
      "Epoch[0], Batch[1271], Train loss: 0.04565050080418587\n",
      "Epoch[0], Val loss: 0.04233622923493385\n",
      "Epoch[0], Batch[1272], Train loss: 0.044669490307569504\n",
      "Epoch[0], Val loss: 0.046185240149497986\n",
      "Epoch[0], Batch[1273], Train loss: 0.045650556683540344\n",
      "Epoch[0], Val loss: 0.044166192412376404\n",
      "Epoch[0], Batch[1274], Train loss: 0.045617375522851944\n",
      "Epoch[0], Val loss: 0.0452057421207428\n",
      "Epoch[0], Batch[1275], Train loss: 0.044984836131334305\n",
      "Epoch[0], Val loss: 0.04640011489391327\n",
      "Epoch[0], Batch[1276], Train loss: 0.046301551163196564\n",
      "Epoch[0], Val loss: 0.04175550490617752\n",
      "Epoch[0], Batch[1277], Train loss: 0.046244844794273376\n",
      "Epoch[0], Val loss: 0.044506967067718506\n",
      "Epoch[0], Batch[1278], Train loss: 0.04385519400238991\n",
      "Epoch[0], Val loss: 0.046167731285095215\n",
      "Epoch[0], Batch[1279], Train loss: 0.04539589211344719\n",
      "Epoch[0], Val loss: 0.04224735125899315\n",
      "Epoch[0], Batch[1280], Train loss: 0.04556964337825775\n",
      "Epoch[0], Val loss: 0.04272677004337311\n",
      "Epoch[0], Batch[1281], Train loss: 0.0473208911716938\n",
      "Epoch[0], Val loss: 0.043569985777139664\n",
      "Epoch[0], Batch[1282], Train loss: 0.04504906386137009\n",
      "Epoch[0], Val loss: 0.04301879182457924\n",
      "Epoch[0], Batch[1283], Train loss: 0.0489862821996212\n",
      "Epoch[0], Val loss: 0.045245684683322906\n",
      "Epoch[0], Batch[1284], Train loss: 0.04263891279697418\n",
      "Epoch[0], Val loss: 0.04147198423743248\n",
      "Epoch[0], Batch[1285], Train loss: 0.045416802167892456\n",
      "Epoch[0], Val loss: 0.04579179733991623\n",
      "Epoch[0], Batch[1286], Train loss: 0.04537827521562576\n",
      "Epoch[0], Val loss: 0.04393816739320755\n",
      "Epoch[0], Batch[1287], Train loss: 0.047226790338754654\n",
      "Epoch[0], Val loss: 0.044046662747859955\n",
      "Epoch[0], Batch[1288], Train loss: 0.04660317674279213\n",
      "Epoch[0], Val loss: 0.04301676154136658\n",
      "Epoch[0], Batch[1289], Train loss: 0.04516586288809776\n",
      "Epoch[0], Val loss: 0.042561646550893784\n",
      "Epoch[0], Batch[1290], Train loss: 0.04311785474419594\n",
      "Epoch[0], Val loss: 0.04268678277730942\n",
      "Epoch[0], Batch[1291], Train loss: 0.042517803609371185\n",
      "Epoch[0], Val loss: 0.041090499609708786\n",
      "Epoch[0], Batch[1292], Train loss: 0.04646451771259308\n",
      "Epoch[0], Val loss: 0.042236413806676865\n",
      "Epoch[0], Batch[1293], Train loss: 0.044034041464328766\n",
      "Epoch[0], Val loss: 0.043020520359277725\n",
      "Epoch[0], Batch[1294], Train loss: 0.04588634520769119\n",
      "Epoch[0], Val loss: 0.04448602721095085\n",
      "Epoch[0], Batch[1295], Train loss: 0.04672040417790413\n",
      "Epoch[0], Val loss: 0.04359227418899536\n",
      "Epoch[0], Batch[1296], Train loss: 0.04625687003135681\n",
      "Epoch[0], Val loss: 0.04361063241958618\n",
      "Epoch[0], Batch[1297], Train loss: 0.04459155350923538\n",
      "Epoch[0], Val loss: 0.04163781926035881\n",
      "Epoch[0], Batch[1298], Train loss: 0.04634413123130798\n",
      "Epoch[0], Val loss: 0.04245307296514511\n",
      "Epoch[0], Batch[1299], Train loss: 0.045729346573352814\n",
      "Epoch[0], Val loss: 0.040295135229825974\n",
      "Epoch[0], Batch[1300], Train loss: 0.04484241083264351\n",
      "Epoch[0], Val loss: 0.042326293885707855\n",
      "Epoch[0], Batch[1301], Train loss: 0.043890610337257385\n",
      "Epoch[0], Val loss: 0.040305376052856445\n",
      "Epoch[0], Batch[1302], Train loss: 0.04554209113121033\n",
      "Epoch[0], Val loss: 0.04222823679447174\n",
      "Epoch[0], Batch[1303], Train loss: 0.04576319828629494\n",
      "Epoch[0], Val loss: 0.042526379227638245\n",
      "Epoch[0], Batch[1304], Train loss: 0.04513263329863548\n",
      "Epoch[0], Val loss: 0.04217127710580826\n",
      "Epoch[0], Batch[1305], Train loss: 0.04651094600558281\n",
      "Epoch[0], Val loss: 0.04168044030666351\n",
      "Epoch[0], Batch[1306], Train loss: 0.04490746930241585\n",
      "Epoch[0], Val loss: 0.04235273599624634\n",
      "Epoch[0], Batch[1307], Train loss: 0.0462278351187706\n",
      "Epoch[0], Val loss: 0.04125199094414711\n",
      "Epoch[0], Batch[1308], Train loss: 0.046619515866041183\n",
      "Epoch[0], Val loss: 0.04297608882188797\n",
      "Epoch[0], Batch[1309], Train loss: 0.04408560320734978\n",
      "Epoch[0], Val loss: 0.04101055487990379\n",
      "Epoch[0], Batch[1310], Train loss: 0.046417005360126495\n",
      "Epoch[0], Val loss: 0.04586968570947647\n",
      "Epoch[0], Batch[1311], Train loss: 0.046127669513225555\n",
      "Epoch[0], Val loss: 0.04193413630127907\n",
      "Epoch[0], Batch[1312], Train loss: 0.04518318548798561\n",
      "Epoch[0], Val loss: 0.04273366928100586\n",
      "Epoch[0], Batch[1313], Train loss: 0.04547862336039543\n",
      "Epoch[0], Val loss: 0.03989711031317711\n",
      "Epoch[0], Batch[1314], Train loss: 0.04302937164902687\n",
      "Epoch[0], Val loss: 0.04019070789217949\n",
      "Epoch[0], Batch[1315], Train loss: 0.04870058596134186\n",
      "Epoch[0], Val loss: 0.04357480630278587\n",
      "Epoch[0], Batch[1316], Train loss: 0.04659905657172203\n",
      "Epoch[0], Val loss: 0.04210931435227394\n",
      "Epoch[0], Batch[1317], Train loss: 0.04416994750499725\n",
      "Epoch[0], Val loss: 0.04167691245675087\n",
      "Epoch[0], Batch[1318], Train loss: 0.04547615349292755\n",
      "Epoch[0], Val loss: 0.0449109748005867\n",
      "Epoch[0], Batch[1319], Train loss: 0.045156508684158325\n",
      "Epoch[0], Val loss: 0.0423758439719677\n",
      "Epoch[0], Batch[1320], Train loss: 0.045576099306344986\n",
      "Epoch[0], Val loss: 0.04034490883350372\n",
      "Epoch[0], Batch[1321], Train loss: 0.045564066618680954\n",
      "Epoch[0], Val loss: 0.0439741350710392\n",
      "Epoch[0], Batch[1322], Train loss: 0.047434087842702866\n",
      "Epoch[0], Val loss: 0.04150602966547012\n",
      "Epoch[0], Batch[1323], Train loss: 0.04381630942225456\n",
      "Epoch[0], Val loss: 0.042817797511816025\n",
      "Epoch[0], Batch[1324], Train loss: 0.042630914598703384\n",
      "Epoch[0], Val loss: 0.04289259389042854\n",
      "Epoch[0], Batch[1325], Train loss: 0.046328186988830566\n",
      "Epoch[0], Val loss: 0.039974652230739594\n",
      "Epoch[0], Batch[1326], Train loss: 0.044964514672756195\n",
      "Epoch[0], Val loss: 0.04145767167210579\n",
      "Epoch[0], Batch[1327], Train loss: 0.044346947222948074\n",
      "Epoch[0], Val loss: 0.03974226489663124\n",
      "Epoch[0], Batch[1328], Train loss: 0.0448739267885685\n",
      "Epoch[0], Val loss: 0.04635021090507507\n",
      "Epoch[0], Batch[1329], Train loss: 0.046233560889959335\n",
      "Epoch[0], Val loss: 0.04150839522480965\n",
      "Epoch[0], Batch[1330], Train loss: 0.041802991181612015\n",
      "Epoch[0], Val loss: 0.04403937980532646\n",
      "Epoch[0], Batch[1331], Train loss: 0.04364711791276932\n",
      "Epoch[0], Val loss: 0.04162411019206047\n",
      "Epoch[0], Batch[1332], Train loss: 0.045136671513319016\n",
      "Epoch[0], Val loss: 0.04093657433986664\n",
      "Epoch[0], Batch[1333], Train loss: 0.044349540024995804\n",
      "Epoch[0], Val loss: 0.04066775366663933\n",
      "Epoch[0], Batch[1334], Train loss: 0.044768281280994415\n",
      "Epoch[0], Val loss: 0.04227272793650627\n",
      "Epoch[0], Batch[1335], Train loss: 0.04456549137830734\n",
      "Epoch[0], Val loss: 0.04189230874180794\n",
      "Epoch[0], Batch[1336], Train loss: 0.04795088246464729\n",
      "Epoch[0], Val loss: 0.04146642982959747\n",
      "Epoch[0], Batch[1337], Train loss: 0.04372115433216095\n",
      "Epoch[0], Val loss: 0.04249320551753044\n",
      "Epoch[0], Batch[1338], Train loss: 0.04364130273461342\n",
      "Epoch[0], Val loss: 0.03913794830441475\n",
      "Epoch[0], Batch[1339], Train loss: 0.04609967768192291\n",
      "Epoch[0], Val loss: 0.04268167167901993\n",
      "Epoch[0], Batch[1340], Train loss: 0.04431771859526634\n",
      "Epoch[0], Val loss: 0.042881835252046585\n",
      "Epoch[0], Batch[1341], Train loss: 0.04399214684963226\n",
      "Epoch[0], Val loss: 0.04154939204454422\n",
      "Epoch[0], Batch[1342], Train loss: 0.04501686245203018\n",
      "Epoch[0], Val loss: 0.04186607524752617\n",
      "Epoch[0], Batch[1343], Train loss: 0.04519772529602051\n",
      "Epoch[0], Val loss: 0.04109947755932808\n",
      "Epoch[0], Batch[1344], Train loss: 0.046020425856113434\n",
      "Epoch[0], Val loss: 0.0446905791759491\n",
      "Epoch[0], Batch[1345], Train loss: 0.04396446794271469\n",
      "Epoch[0], Val loss: 0.04515364021062851\n",
      "Epoch[0], Batch[1346], Train loss: 0.04502185434103012\n",
      "Epoch[0], Val loss: 0.04390210658311844\n",
      "Epoch[0], Batch[1347], Train loss: 0.043609317392110825\n",
      "Epoch[0], Val loss: 0.04061384126543999\n",
      "Epoch[0], Batch[1348], Train loss: 0.04277303069829941\n",
      "Epoch[0], Val loss: 0.04138302803039551\n",
      "Epoch[0], Batch[1349], Train loss: 0.04622906073927879\n",
      "Epoch[0], Val loss: 0.043886952102184296\n",
      "Epoch[0], Batch[1350], Train loss: 0.04383157193660736\n",
      "Epoch[0], Val loss: 0.04127224534749985\n",
      "Epoch[0], Batch[1351], Train loss: 0.0450039841234684\n",
      "Epoch[0], Val loss: 0.0433863066136837\n",
      "Epoch[0], Batch[1352], Train loss: 0.04828657954931259\n",
      "Epoch[0], Val loss: 0.044629745185375214\n",
      "Epoch[0], Batch[1353], Train loss: 0.045707881450653076\n",
      "Epoch[0], Val loss: 0.04159407690167427\n",
      "Epoch[0], Batch[1354], Train loss: 0.04477192461490631\n",
      "Epoch[0], Val loss: 0.04478242248296738\n",
      "Epoch[0], Batch[1355], Train loss: 0.04427164047956467\n",
      "Epoch[0], Val loss: 0.04119052365422249\n",
      "Epoch[0], Batch[1356], Train loss: 0.04720953479409218\n",
      "Epoch[0], Val loss: 0.04199918359518051\n",
      "Epoch[0], Batch[1357], Train loss: 0.04493315890431404\n",
      "Epoch[0], Val loss: 0.041405756026506424\n",
      "Epoch[0], Batch[1358], Train loss: 0.044266942888498306\n",
      "Epoch[0], Val loss: 0.04597712308168411\n",
      "Epoch[0], Batch[1359], Train loss: 0.04482926428318024\n",
      "Epoch[0], Val loss: 0.04418173059821129\n",
      "Epoch[0], Batch[1360], Train loss: 0.04450349882245064\n",
      "Epoch[0], Val loss: 0.043242741376161575\n",
      "Epoch[0], Batch[1361], Train loss: 0.04659406095743179\n",
      "Epoch[0], Val loss: 0.04358123242855072\n",
      "Epoch[0], Batch[1362], Train loss: 0.04431088641285896\n",
      "Epoch[0], Val loss: 0.03815733268857002\n",
      "Epoch[0], Batch[1363], Train loss: 0.04839329421520233\n",
      "Epoch[0], Val loss: 0.04106191545724869\n",
      "Epoch[0], Batch[1364], Train loss: 0.04868823662400246\n",
      "Epoch[0], Val loss: 0.04580472409725189\n",
      "Epoch[0], Batch[1365], Train loss: 0.04999914765357971\n",
      "Epoch[0], Val loss: 0.039671674370765686\n",
      "Epoch[0], Batch[1366], Train loss: 0.04358706995844841\n",
      "Epoch[0], Val loss: 0.0417998731136322\n",
      "Epoch[0], Batch[1367], Train loss: 0.04196929559111595\n",
      "Epoch[0], Val loss: 0.03942778706550598\n",
      "Epoch[0], Batch[1368], Train loss: 0.043609485030174255\n",
      "Epoch[0], Val loss: 0.04078732058405876\n",
      "Epoch[0], Batch[1369], Train loss: 0.04602271690964699\n",
      "Epoch[0], Val loss: 0.04316532984375954\n",
      "Epoch[0], Batch[1370], Train loss: 0.043883707374334335\n",
      "Epoch[0], Val loss: 0.04050925746560097\n",
      "Epoch[0], Batch[1371], Train loss: 0.046298548579216\n",
      "Epoch[0], Val loss: 0.043515417724847794\n",
      "Epoch[0], Batch[1372], Train loss: 0.04637080430984497\n",
      "Epoch[0], Val loss: 0.04334581643342972\n",
      "Epoch[0], Batch[1373], Train loss: 0.04388505965471268\n",
      "Epoch[0], Val loss: 0.041376128792762756\n",
      "Epoch[0], Batch[1374], Train loss: 0.04503316432237625\n",
      "Epoch[0], Val loss: 0.040712688118219376\n",
      "Epoch[0], Batch[1375], Train loss: 0.049555979669094086\n",
      "Epoch[0], Val loss: 0.045513615012168884\n",
      "Epoch[0], Batch[1376], Train loss: 0.044558335095644\n",
      "Epoch[0], Val loss: 0.045802317559719086\n",
      "Epoch[0], Batch[1377], Train loss: 0.043869584798812866\n",
      "Epoch[0], Val loss: 0.04590494930744171\n",
      "Epoch[0], Batch[1378], Train loss: 0.043555017560720444\n",
      "Epoch[0], Val loss: 0.044243864715099335\n",
      "Epoch[0], Batch[1379], Train loss: 0.04660307243466377\n",
      "Epoch[0], Val loss: 0.0444076731801033\n",
      "Epoch[0], Batch[1380], Train loss: 0.043901994824409485\n",
      "Epoch[0], Val loss: 0.04290109872817993\n",
      "Epoch[0], Batch[1381], Train loss: 0.0476878359913826\n",
      "Epoch[0], Val loss: 0.039681099355220795\n",
      "Epoch[0], Batch[1382], Train loss: 0.04490972310304642\n",
      "Epoch[0], Val loss: 0.046733416616916656\n",
      "Epoch[0], Batch[1383], Train loss: 0.04364807903766632\n",
      "Epoch[0], Val loss: 0.04276464506983757\n",
      "Epoch[0], Batch[1384], Train loss: 0.0422588512301445\n",
      "Epoch[0], Val loss: 0.040574852377176285\n",
      "Epoch[0], Batch[1385], Train loss: 0.04557861015200615\n",
      "Epoch[0], Val loss: 0.04001183807849884\n",
      "Epoch[0], Batch[1386], Train loss: 0.04454405605792999\n",
      "Epoch[0], Val loss: 0.04467453807592392\n",
      "Epoch[0], Batch[1387], Train loss: 0.04670381173491478\n",
      "Epoch[0], Val loss: 0.043139901012182236\n",
      "Epoch[0], Batch[1388], Train loss: 0.044283632189035416\n",
      "Epoch[0], Val loss: 0.041159458458423615\n",
      "Epoch[0], Batch[1389], Train loss: 0.046645209193229675\n",
      "Epoch[0], Val loss: 0.041865721344947815\n",
      "Epoch[0], Batch[1390], Train loss: 0.0470895916223526\n",
      "Epoch[0], Val loss: 0.041129130870103836\n",
      "Epoch[0], Batch[1391], Train loss: 0.04393063485622406\n",
      "Epoch[0], Val loss: 0.040907181799411774\n",
      "Epoch[0], Batch[1392], Train loss: 0.043937500566244125\n",
      "Epoch[0], Val loss: 0.04230695217847824\n",
      "Epoch[0], Batch[1393], Train loss: 0.045944247394800186\n",
      "Epoch[0], Val loss: 0.040454477071762085\n",
      "Epoch[0], Batch[1394], Train loss: 0.04359469935297966\n",
      "Epoch[0], Val loss: 0.04364293813705444\n",
      "Epoch[0], Batch[1395], Train loss: 0.04183166101574898\n",
      "Epoch[0], Val loss: 0.042371585965156555\n",
      "Epoch[0], Batch[1396], Train loss: 0.04689282551407814\n",
      "Epoch[0], Val loss: 0.04082190617918968\n",
      "Epoch[0], Batch[1397], Train loss: 0.04507513344287872\n",
      "Epoch[0], Val loss: 0.04070207104086876\n",
      "Epoch[0], Batch[1398], Train loss: 0.044378045946359634\n",
      "Epoch[0], Val loss: 0.04155130684375763\n",
      "Epoch[0], Batch[1399], Train loss: 0.0458497516810894\n",
      "Epoch[0], Val loss: 0.0413045696914196\n",
      "Epoch[0], Batch[1400], Train loss: 0.04689921438694\n",
      "Epoch[0], Val loss: 0.04172505810856819\n",
      "Epoch[0], Batch[1401], Train loss: 0.042855747044086456\n",
      "Epoch[0], Val loss: 0.03984149545431137\n",
      "Epoch[0], Batch[1402], Train loss: 0.04521763697266579\n",
      "Epoch[0], Val loss: 0.04041524976491928\n",
      "Epoch[0], Batch[1403], Train loss: 0.04281539097428322\n",
      "Epoch[0], Val loss: 0.04285109415650368\n",
      "Epoch[0], Batch[1404], Train loss: 0.04903751611709595\n",
      "Epoch[0], Val loss: 0.04116026312112808\n",
      "Epoch[0], Batch[1405], Train loss: 0.042453426867723465\n",
      "Epoch[0], Val loss: 0.04215220361948013\n",
      "Epoch[0], Batch[1406], Train loss: 0.04711746424436569\n",
      "Epoch[0], Val loss: 0.04391459748148918\n",
      "Epoch[0], Batch[1407], Train loss: 0.04474653676152229\n",
      "Epoch[0], Val loss: 0.04421369358897209\n",
      "Epoch[0], Batch[1408], Train loss: 0.04506407305598259\n",
      "Epoch[0], Val loss: 0.04252000153064728\n",
      "Epoch[0], Batch[1409], Train loss: 0.0474032424390316\n",
      "Epoch[0], Val loss: 0.04130185395479202\n",
      "Epoch[0], Batch[1410], Train loss: 0.04470758140087128\n",
      "Epoch[0], Val loss: 0.0409739725291729\n",
      "Epoch[0], Batch[1411], Train loss: 0.04190243408083916\n",
      "Epoch[0], Val loss: 0.04272251948714256\n",
      "Epoch[0], Batch[1412], Train loss: 0.0462191179394722\n",
      "Epoch[0], Val loss: 0.04431876167654991\n",
      "Epoch[0], Batch[1413], Train loss: 0.04338277503848076\n",
      "Epoch[0], Val loss: 0.04384256526827812\n",
      "Epoch[0], Batch[1414], Train loss: 0.044638797640800476\n",
      "Epoch[0], Val loss: 0.04218649864196777\n",
      "Epoch[0], Batch[1415], Train loss: 0.04872093349695206\n",
      "Epoch[0], Val loss: 0.04392945021390915\n",
      "Epoch[0], Batch[1416], Train loss: 0.04610145464539528\n",
      "Epoch[0], Val loss: 0.03924185037612915\n",
      "Epoch[0], Batch[1417], Train loss: 0.04554534703493118\n",
      "Epoch[0], Val loss: 0.042789276689291\n",
      "Epoch[0], Batch[1418], Train loss: 0.043212033808231354\n",
      "Epoch[0], Val loss: 0.04030894860625267\n",
      "Epoch[0], Batch[1419], Train loss: 0.04382151737809181\n",
      "Epoch[0], Val loss: 0.042651187628507614\n",
      "Epoch[0], Batch[1420], Train loss: 0.04479410871863365\n",
      "Epoch[0], Val loss: 0.042891766875982285\n",
      "Epoch[0], Batch[1421], Train loss: 0.04473523423075676\n",
      "Epoch[0], Val loss: 0.041804369539022446\n",
      "Epoch[0], Batch[1422], Train loss: 0.04477788880467415\n",
      "Epoch[0], Val loss: 0.042534299194812775\n",
      "Epoch[0], Batch[1423], Train loss: 0.04287527874112129\n",
      "Epoch[0], Val loss: 0.04097488150000572\n",
      "Epoch[0], Batch[1424], Train loss: 0.042544860392808914\n",
      "Epoch[0], Val loss: 0.04204072430729866\n",
      "Epoch[0], Batch[1425], Train loss: 0.04652124270796776\n",
      "Epoch[0], Val loss: 0.04010258987545967\n",
      "Epoch[0], Batch[1426], Train loss: 0.04407525062561035\n",
      "Epoch[0], Val loss: 0.04570506885647774\n",
      "Epoch[0], Batch[1427], Train loss: 0.042400259524583817\n",
      "Epoch[0], Val loss: 0.04113003611564636\n",
      "Epoch[0], Batch[1428], Train loss: 0.04670122265815735\n",
      "Epoch[0], Val loss: 0.04262399300932884\n",
      "Epoch[0], Batch[1429], Train loss: 0.04533162713050842\n",
      "Epoch[0], Val loss: 0.04410446062684059\n",
      "Epoch[0], Batch[1430], Train loss: 0.043883100152015686\n",
      "Epoch[0], Val loss: 0.04080234467983246\n",
      "Epoch[0], Batch[1431], Train loss: 0.045219339430332184\n",
      "Epoch[0], Val loss: 0.03979352489113808\n",
      "Epoch[0], Batch[1432], Train loss: 0.0470394641160965\n",
      "Epoch[0], Val loss: 0.0387677401304245\n",
      "Epoch[0], Batch[1433], Train loss: 0.04339677467942238\n",
      "Epoch[0], Val loss: 0.04089750349521637\n",
      "Epoch[0], Batch[1434], Train loss: 0.04565088078379631\n",
      "Epoch[0], Val loss: 0.042398810386657715\n",
      "Epoch[0], Batch[1435], Train loss: 0.04443138465285301\n",
      "Epoch[0], Val loss: 0.04326159134507179\n",
      "Epoch[0], Batch[1436], Train loss: 0.045330047607421875\n",
      "Epoch[0], Val loss: 0.042214494198560715\n",
      "Epoch[0], Batch[1437], Train loss: 0.04672577604651451\n",
      "Epoch[0], Val loss: 0.042988598346710205\n",
      "Epoch[0], Batch[1438], Train loss: 0.044963426887989044\n",
      "Epoch[0], Val loss: 0.040544718503952026\n",
      "Epoch[0], Batch[1439], Train loss: 0.04353988170623779\n",
      "Epoch[0], Val loss: 0.04201459512114525\n",
      "Epoch[0], Batch[1440], Train loss: 0.044854115694761276\n",
      "Epoch[0], Val loss: 0.043258193880319595\n",
      "Epoch[0], Batch[1441], Train loss: 0.04524361714720726\n",
      "Epoch[0], Val loss: 0.04010862857103348\n",
      "Epoch[0], Batch[1442], Train loss: 0.045479483902454376\n",
      "Epoch[0], Val loss: 0.041955724358558655\n",
      "Epoch[0], Batch[1443], Train loss: 0.04492778703570366\n",
      "Epoch[0], Val loss: 0.04263024777173996\n",
      "Epoch[0], Batch[1444], Train loss: 0.044229988008737564\n",
      "Epoch[0], Val loss: 0.041866276413202286\n",
      "Epoch[0], Batch[1445], Train loss: 0.04683523625135422\n",
      "Epoch[0], Val loss: 0.04012325033545494\n",
      "Epoch[0], Batch[1446], Train loss: 0.04287488013505936\n",
      "Epoch[0], Val loss: 0.04154939204454422\n",
      "Epoch[0], Batch[1447], Train loss: 0.04405216500163078\n",
      "Epoch[0], Val loss: 0.041798558086156845\n",
      "Epoch[0], Batch[1448], Train loss: 0.04405481740832329\n",
      "Epoch[0], Val loss: 0.04049273952841759\n",
      "Epoch[0], Batch[1449], Train loss: 0.045827701687812805\n",
      "Epoch[0], Val loss: 0.040264811366796494\n",
      "Epoch[0], Batch[1450], Train loss: 0.04284966364502907\n",
      "Epoch[0], Val loss: 0.04308413341641426\n",
      "Epoch[0], Batch[1451], Train loss: 0.046025678515434265\n",
      "Epoch[0], Val loss: 0.041361819952726364\n",
      "Epoch[0], Batch[1452], Train loss: 0.05047108978033066\n",
      "Epoch[0], Val loss: 0.04193204641342163\n",
      "Epoch[0], Batch[1453], Train loss: 0.04490458220243454\n",
      "Epoch[0], Val loss: 0.03978389874100685\n",
      "Epoch[0], Batch[1454], Train loss: 0.04515945538878441\n",
      "Epoch[0], Val loss: 0.04127047210931778\n",
      "Epoch[0], Batch[1455], Train loss: 0.04415976628661156\n",
      "Epoch[0], Val loss: 0.04260290041565895\n",
      "Epoch[0], Batch[1456], Train loss: 0.04359433054924011\n",
      "Epoch[0], Val loss: 0.04167499765753746\n",
      "Epoch[0], Batch[1457], Train loss: 0.04323899745941162\n",
      "Epoch[0], Val loss: 0.04367435351014137\n",
      "Epoch[0], Batch[1458], Train loss: 0.04302980378270149\n",
      "Epoch[0], Val loss: 0.038713399320840836\n",
      "Epoch[0], Batch[1459], Train loss: 0.04417784512042999\n",
      "Epoch[0], Val loss: 0.041651319712400436\n",
      "Epoch[0], Batch[1460], Train loss: 0.044778551906347275\n",
      "Epoch[0], Val loss: 0.04286380112171173\n",
      "Epoch[0], Batch[1461], Train loss: 0.04317081719636917\n",
      "Epoch[0], Val loss: 0.044250860810279846\n",
      "Epoch[0], Batch[1462], Train loss: 0.044339340180158615\n",
      "Epoch[0], Val loss: 0.04151207208633423\n",
      "Epoch[0], Batch[1463], Train loss: 0.045886021107435226\n",
      "Epoch[0], Val loss: 0.04278596490621567\n",
      "Epoch[0], Batch[1464], Train loss: 0.0432596281170845\n",
      "Epoch[0], Val loss: 0.04200592637062073\n",
      "Epoch[0], Batch[1465], Train loss: 0.04258067160844803\n",
      "Epoch[0], Val loss: 0.03959821164608002\n",
      "Epoch[0], Batch[1466], Train loss: 0.044095300137996674\n",
      "Epoch[0], Val loss: 0.04229934513568878\n",
      "Epoch[0], Batch[1467], Train loss: 0.04396146535873413\n",
      "Epoch[0], Val loss: 0.0418662391602993\n",
      "Epoch[0], Batch[1468], Train loss: 0.04204309359192848\n",
      "Epoch[0], Val loss: 0.04120281711220741\n",
      "Epoch[0], Batch[1469], Train loss: 0.04422624781727791\n",
      "Epoch[0], Val loss: 0.041162289679050446\n",
      "Epoch[0], Batch[1470], Train loss: 0.0436866320669651\n",
      "Epoch[0], Val loss: 0.038717590272426605\n",
      "Epoch[0], Batch[1471], Train loss: 0.045868102461099625\n",
      "Epoch[0], Val loss: 0.042852867394685745\n",
      "Epoch[0], Batch[1472], Train loss: 0.04502551257610321\n",
      "Epoch[0], Val loss: 0.04225854575634003\n",
      "Epoch[0], Batch[1473], Train loss: 0.04434015974402428\n",
      "Epoch[0], Val loss: 0.041264526546001434\n",
      "Epoch[0], Batch[1474], Train loss: 0.0418299064040184\n",
      "Epoch[0], Val loss: 0.04225941374897957\n",
      "Epoch[0], Batch[1475], Train loss: 0.04549798369407654\n",
      "Epoch[0], Val loss: 0.04474850371479988\n",
      "Epoch[0], Batch[1476], Train loss: 0.04411550611257553\n",
      "Epoch[0], Val loss: 0.038092903792858124\n",
      "Epoch[0], Batch[1477], Train loss: 0.04298289865255356\n",
      "Epoch[0], Val loss: 0.0409456267952919\n",
      "Epoch[0], Batch[1478], Train loss: 0.04375559836626053\n",
      "Epoch[0], Val loss: 0.040480028837919235\n",
      "Epoch[0], Batch[1479], Train loss: 0.04357244819402695\n",
      "Epoch[0], Val loss: 0.03986254706978798\n",
      "Epoch[0], Batch[1480], Train loss: 0.04584089294075966\n",
      "Epoch[0], Val loss: 0.04135292023420334\n",
      "Epoch[0], Batch[1481], Train loss: 0.04283770173788071\n",
      "Epoch[0], Val loss: 0.041999638080596924\n",
      "Epoch[0], Batch[1482], Train loss: 0.043202243745326996\n",
      "Epoch[0], Val loss: 0.03982676565647125\n",
      "Epoch[0], Batch[1483], Train loss: 0.047094590961933136\n",
      "Epoch[0], Val loss: 0.044203780591487885\n",
      "Epoch[0], Batch[1484], Train loss: 0.04644123464822769\n",
      "Epoch[0], Val loss: 0.04298493266105652\n",
      "Epoch[0], Batch[1485], Train loss: 0.046152256429195404\n",
      "Epoch[0], Val loss: 0.04211714491248131\n",
      "Epoch[0], Batch[1486], Train loss: 0.0417257659137249\n",
      "Epoch[0], Val loss: 0.04156307131052017\n",
      "Epoch[0], Batch[1487], Train loss: 0.04170830547809601\n",
      "Epoch[0], Val loss: 0.039561837911605835\n",
      "Epoch[0], Batch[1488], Train loss: 0.045699719339609146\n",
      "Epoch[0], Val loss: 0.039258409291505814\n",
      "Epoch[0], Batch[1489], Train loss: 0.041558437049388885\n",
      "Epoch[0], Val loss: 0.0426734983921051\n",
      "Epoch[0], Batch[1490], Train loss: 0.04349786788225174\n",
      "Epoch[0], Val loss: 0.04206671565771103\n",
      "Epoch[0], Batch[1491], Train loss: 0.043158698827028275\n",
      "Epoch[0], Val loss: 0.0453621931374073\n",
      "Epoch[0], Batch[1492], Train loss: 0.043825529515743256\n",
      "Epoch[0], Val loss: 0.04360609129071236\n",
      "Epoch[0], Batch[1493], Train loss: 0.04529162868857384\n",
      "Epoch[0], Val loss: 0.04461541399359703\n",
      "Epoch[0], Batch[1494], Train loss: 0.043019335716962814\n",
      "Epoch[0], Val loss: 0.041376177221536636\n",
      "Epoch[0], Batch[1495], Train loss: 0.042378153651952744\n",
      "Epoch[0], Val loss: 0.04064461216330528\n",
      "Epoch[0], Batch[1496], Train loss: 0.048345934599637985\n",
      "Epoch[0], Val loss: 0.03824787214398384\n",
      "Epoch[0], Batch[1497], Train loss: 0.045716430991888046\n",
      "Epoch[0], Val loss: 0.041699569672346115\n",
      "Epoch[0], Batch[1498], Train loss: 0.04394391179084778\n",
      "Epoch[0], Val loss: 0.03939051926136017\n",
      "Epoch[0], Batch[1499], Train loss: 0.041065238416194916\n",
      "Epoch[0], Val loss: 0.04195026308298111\n",
      "Epoch[0], Batch[1500], Train loss: 0.04282880574464798\n",
      "Epoch[0], Val loss: 0.04119741916656494\n",
      "Epoch[0], Batch[1501], Train loss: 0.046542342752218246\n",
      "Epoch[0], Val loss: 0.04068409651517868\n",
      "Epoch[0], Batch[1502], Train loss: 0.045888449996709824\n",
      "Epoch[0], Val loss: 0.041043106466531754\n",
      "Epoch[0], Batch[1503], Train loss: 0.04894651845097542\n",
      "Epoch[0], Val loss: 0.04427797347307205\n",
      "Epoch[0], Batch[1504], Train loss: 0.04539507254958153\n",
      "Epoch[0], Val loss: 0.041129209101200104\n",
      "Epoch[0], Batch[1505], Train loss: 0.04122930020093918\n",
      "Epoch[0], Val loss: 0.04185865819454193\n",
      "Epoch[0], Batch[1506], Train loss: 0.04428938031196594\n",
      "Epoch[0], Val loss: 0.041048336774110794\n",
      "Epoch[0], Batch[1507], Train loss: 0.04384539648890495\n",
      "Epoch[0], Val loss: 0.042046405375003815\n",
      "Epoch[0], Batch[1508], Train loss: 0.04377226531505585\n",
      "Epoch[0], Val loss: 0.03900342807173729\n",
      "Epoch[0], Batch[1509], Train loss: 0.04357665777206421\n",
      "Epoch[0], Val loss: 0.04562060162425041\n",
      "Epoch[0], Batch[1510], Train loss: 0.04522086679935455\n",
      "Epoch[0], Val loss: 0.04380646347999573\n",
      "Epoch[0], Batch[1511], Train loss: 0.04382747411727905\n",
      "Epoch[0], Val loss: 0.0414118617773056\n",
      "Epoch[0], Batch[1512], Train loss: 0.04408571869134903\n",
      "Epoch[0], Val loss: 0.03903350606560707\n",
      "Epoch[0], Batch[1513], Train loss: 0.04256558045744896\n",
      "Epoch[0], Val loss: 0.04023323953151703\n",
      "Epoch[0], Batch[1514], Train loss: 0.04308110103011131\n",
      "Epoch[0], Val loss: 0.03974981978535652\n",
      "Epoch[0], Batch[1515], Train loss: 0.04593043401837349\n",
      "Epoch[0], Val loss: 0.041200023144483566\n",
      "Epoch[0], Batch[1516], Train loss: 0.04278434440493584\n",
      "Epoch[0], Val loss: 0.03883146122097969\n",
      "Epoch[0], Batch[1517], Train loss: 0.047583311796188354\n",
      "Epoch[0], Val loss: 0.04305311292409897\n",
      "Epoch[0], Batch[1518], Train loss: 0.04483785107731819\n",
      "Epoch[0], Val loss: 0.04456264153122902\n",
      "Epoch[0], Batch[1519], Train loss: 0.04185822233557701\n",
      "Epoch[0], Val loss: 0.04182328283786774\n",
      "Epoch[0], Batch[1520], Train loss: 0.045451968908309937\n",
      "Epoch[0], Val loss: 0.04154055193066597\n",
      "Epoch[0], Batch[1521], Train loss: 0.04722122475504875\n",
      "Epoch[0], Val loss: 0.04218107461929321\n",
      "Epoch[0], Batch[1522], Train loss: 0.04310299828648567\n",
      "Epoch[0], Val loss: 0.04055789113044739\n",
      "Epoch[0], Batch[1523], Train loss: 0.04561018571257591\n",
      "Epoch[0], Val loss: 0.041850604116916656\n",
      "Epoch[0], Batch[1524], Train loss: 0.04160503298044205\n",
      "Epoch[0], Val loss: 0.04146562144160271\n",
      "Epoch[0], Batch[1525], Train loss: 0.04430072009563446\n",
      "Epoch[0], Val loss: 0.043886978179216385\n",
      "Epoch[0], Batch[1526], Train loss: 0.04313452169299126\n",
      "Epoch[0], Val loss: 0.04069408029317856\n",
      "Epoch[0], Batch[1527], Train loss: 0.04382532462477684\n",
      "Epoch[0], Val loss: 0.04139767587184906\n",
      "Epoch[0], Batch[1528], Train loss: 0.044286198914051056\n",
      "Epoch[0], Val loss: 0.0420815534889698\n",
      "Epoch[0], Batch[1529], Train loss: 0.04347808659076691\n",
      "Epoch[0], Val loss: 0.03986107185482979\n",
      "Epoch[0], Batch[1530], Train loss: 0.046338699758052826\n",
      "Epoch[0], Val loss: 0.039427947252988815\n",
      "Epoch[0], Batch[1531], Train loss: 0.04538002237677574\n",
      "Epoch[0], Val loss: 0.04026229307055473\n",
      "Epoch[0], Batch[1532], Train loss: 0.04459008201956749\n",
      "Epoch[0], Val loss: 0.04132453352212906\n",
      "Epoch[0], Batch[1533], Train loss: 0.04645529016852379\n",
      "Epoch[0], Val loss: 0.04156697168946266\n",
      "Epoch[0], Batch[1534], Train loss: 0.04210100322961807\n",
      "Epoch[0], Val loss: 0.041834816336631775\n",
      "Epoch[0], Batch[1535], Train loss: 0.04704485833644867\n",
      "Epoch[0], Val loss: 0.0440116710960865\n",
      "Epoch[0], Batch[1536], Train loss: 0.0443045049905777\n",
      "Epoch[0], Val loss: 0.04277990758419037\n",
      "Epoch[0], Batch[1537], Train loss: 0.04315941780805588\n",
      "Epoch[0], Val loss: 0.04039746895432472\n",
      "Epoch[0], Batch[1538], Train loss: 0.04604543372988701\n",
      "Epoch[0], Val loss: 0.04110726714134216\n",
      "Epoch[0], Batch[1539], Train loss: 0.043159905821084976\n",
      "Epoch[0], Val loss: 0.04034615680575371\n",
      "Epoch[0], Batch[1540], Train loss: 0.04267742112278938\n",
      "Epoch[0], Val loss: 0.040275342762470245\n",
      "Epoch[0], Batch[1541], Train loss: 0.04331584647297859\n",
      "Epoch[0], Val loss: 0.040853552520275116\n",
      "Epoch[0], Batch[1542], Train loss: 0.042956553399562836\n",
      "Epoch[0], Val loss: 0.041974276304244995\n",
      "Epoch[0], Batch[1543], Train loss: 0.04438704997301102\n",
      "Epoch[0], Val loss: 0.0410933718085289\n",
      "Epoch[0], Batch[1544], Train loss: 0.04225533455610275\n",
      "Epoch[0], Val loss: 0.04262298345565796\n",
      "Epoch[0], Batch[1545], Train loss: 0.044911060482263565\n",
      "Epoch[0], Val loss: 0.04044007882475853\n",
      "Epoch[0], Batch[1546], Train loss: 0.042818475514650345\n",
      "Epoch[0], Val loss: 0.03945989906787872\n",
      "Epoch[0], Batch[1547], Train loss: 0.04241063445806503\n",
      "Epoch[0], Val loss: 0.04007318988442421\n",
      "Epoch[0], Batch[1548], Train loss: 0.042283110320568085\n",
      "Epoch[0], Val loss: 0.042214974761009216\n",
      "Epoch[0], Batch[1549], Train loss: 0.044109366834163666\n",
      "Epoch[0], Val loss: 0.041671618819236755\n",
      "Epoch[0], Batch[1550], Train loss: 0.04370364919304848\n",
      "Epoch[0], Val loss: 0.04311497136950493\n",
      "Epoch[0], Batch[1551], Train loss: 0.04713000729680061\n",
      "Epoch[0], Val loss: 0.039835620671510696\n",
      "Epoch[0], Batch[1552], Train loss: 0.0418379008769989\n",
      "Epoch[0], Val loss: 0.03783668205142021\n",
      "Epoch[0], Batch[1553], Train loss: 0.045061565935611725\n",
      "Epoch[0], Val loss: 0.03970670327544212\n",
      "Epoch[0], Batch[1554], Train loss: 0.043647319078445435\n",
      "Epoch[0], Val loss: 0.04227874428033829\n",
      "Epoch[0], Batch[1555], Train loss: 0.04035221040248871\n",
      "Epoch[0], Val loss: 0.04013475030660629\n",
      "Epoch[0], Batch[1556], Train loss: 0.04429711773991585\n",
      "Epoch[0], Val loss: 0.04089304059743881\n",
      "Epoch[0], Batch[1557], Train loss: 0.043340474367141724\n",
      "Epoch[0], Val loss: 0.04398754984140396\n",
      "Epoch[0], Batch[1558], Train loss: 0.04062039032578468\n",
      "Epoch[0], Val loss: 0.04376579821109772\n",
      "Epoch[0], Batch[1559], Train loss: 0.04411601647734642\n",
      "Epoch[0], Val loss: 0.04005094990134239\n",
      "Epoch[0], Batch[1560], Train loss: 0.043740589171648026\n",
      "Epoch[0], Val loss: 0.04064081236720085\n",
      "Epoch[0], Batch[1561], Train loss: 0.04506487399339676\n",
      "Epoch[0], Val loss: 0.04090011492371559\n",
      "Epoch[0], Batch[1562], Train loss: 0.04514024406671524\n",
      "Epoch[0], Val loss: 0.04222482815384865\n",
      "Epoch[0], Batch[1563], Train loss: 0.04079457372426987\n",
      "Epoch[0], Val loss: 0.0405864492058754\n",
      "Epoch[0], Batch[1564], Train loss: 0.04380758851766586\n",
      "Epoch[0], Val loss: 0.0398491732776165\n",
      "Epoch[0], Batch[1565], Train loss: 0.042073529213666916\n",
      "Epoch[0], Val loss: 0.03865532577037811\n",
      "Epoch[0], Batch[1566], Train loss: 0.043358881026506424\n",
      "Epoch[0], Val loss: 0.04123478755354881\n",
      "Epoch[0], Batch[1567], Train loss: 0.04143410176038742\n",
      "Epoch[0], Val loss: 0.041037727147340775\n",
      "Epoch[0], Batch[1568], Train loss: 0.042705900967121124\n",
      "Epoch[0], Val loss: 0.038867536932229996\n",
      "Epoch[0], Batch[1569], Train loss: 0.043281685560941696\n",
      "Epoch[0], Val loss: 0.041849564760923386\n",
      "Epoch[0], Batch[1570], Train loss: 0.043021660298109055\n",
      "Epoch[0], Val loss: 0.04082782194018364\n",
      "Epoch[0], Batch[1571], Train loss: 0.044351667165756226\n",
      "Epoch[0], Val loss: 0.042202867567539215\n",
      "Epoch[0], Batch[1572], Train loss: 0.04083133116364479\n",
      "Epoch[0], Val loss: 0.040991686284542084\n",
      "Epoch[0], Batch[1573], Train loss: 0.04407826438546181\n",
      "Epoch[0], Val loss: 0.03948661684989929\n",
      "Epoch[0], Batch[1574], Train loss: 0.042845312505960464\n",
      "Epoch[0], Val loss: 0.04102617874741554\n",
      "Epoch[0], Batch[1575], Train loss: 0.04168272390961647\n",
      "Epoch[0], Val loss: 0.04233920946717262\n",
      "Epoch[0], Batch[1576], Train loss: 0.043576136231422424\n",
      "Epoch[0], Val loss: 0.03833068162202835\n",
      "Epoch[0], Batch[1577], Train loss: 0.044205453246831894\n",
      "Epoch[0], Val loss: 0.04073231667280197\n",
      "Epoch[0], Batch[1578], Train loss: 0.04084603115916252\n",
      "Epoch[0], Val loss: 0.03956027701497078\n",
      "Epoch[0], Batch[1579], Train loss: 0.042450640350580215\n",
      "Epoch[0], Val loss: 0.04122845083475113\n",
      "Epoch[0], Batch[1580], Train loss: 0.040924739092588425\n",
      "Epoch[0], Val loss: 0.04422653838992119\n",
      "Epoch[0], Batch[1581], Train loss: 0.0419558584690094\n",
      "Epoch[0], Val loss: 0.04057047888636589\n",
      "Epoch[0], Batch[1582], Train loss: 0.04313013330101967\n",
      "Epoch[0], Val loss: 0.04219178482890129\n",
      "Epoch[0], Batch[1583], Train loss: 0.043679315596818924\n",
      "Epoch[0], Val loss: 0.04206780344247818\n",
      "Epoch[0], Batch[1584], Train loss: 0.040732644498348236\n",
      "Epoch[0], Val loss: 0.04137595742940903\n",
      "Epoch[0], Batch[1585], Train loss: 0.0434526763856411\n",
      "Epoch[0], Val loss: 0.04212016984820366\n",
      "Epoch[0], Batch[1586], Train loss: 0.044023480266332626\n",
      "Epoch[0], Val loss: 0.04036542773246765\n",
      "Epoch[0], Batch[1587], Train loss: 0.04048439487814903\n",
      "Epoch[0], Val loss: 0.042500145733356476\n",
      "Epoch[0], Batch[1588], Train loss: 0.0437544509768486\n",
      "Epoch[0], Val loss: 0.03893621265888214\n",
      "Epoch[0], Batch[1589], Train loss: 0.04582829028367996\n",
      "Epoch[0], Val loss: 0.040707699954509735\n",
      "Epoch[0], Batch[1590], Train loss: 0.041890501976013184\n",
      "Epoch[0], Val loss: 0.039810288697481155\n",
      "Epoch[0], Batch[1591], Train loss: 0.04298531636595726\n",
      "Epoch[0], Val loss: 0.04070057347416878\n",
      "Epoch[0], Batch[1592], Train loss: 0.04361255466938019\n",
      "Epoch[0], Val loss: 0.04132774844765663\n",
      "Epoch[0], Batch[1593], Train loss: 0.041734397411346436\n",
      "Epoch[0], Val loss: 0.03908327594399452\n",
      "Epoch[0], Batch[1594], Train loss: 0.04292812943458557\n",
      "Epoch[0], Val loss: 0.041434694081544876\n",
      "Epoch[0], Batch[1595], Train loss: 0.04421114921569824\n",
      "Epoch[0], Val loss: 0.0402630940079689\n",
      "Epoch[0], Batch[1596], Train loss: 0.0436914823949337\n",
      "Epoch[0], Val loss: 0.04321726784110069\n",
      "Epoch[0], Batch[1597], Train loss: 0.04245799407362938\n",
      "Epoch[0], Val loss: 0.03928830474615097\n",
      "Epoch[0], Batch[1598], Train loss: 0.041145920753479004\n",
      "Epoch[0], Val loss: 0.0420859232544899\n",
      "Epoch[0], Batch[1599], Train loss: 0.04354657232761383\n",
      "Epoch[0], Val loss: 0.0415952205657959\n",
      "Epoch[0], Batch[1600], Train loss: 0.04068060964345932\n",
      "Epoch[0], Val loss: 0.04150790721178055\n",
      "Epoch[0], Batch[1601], Train loss: 0.042126432061195374\n",
      "Epoch[0], Val loss: 0.04236982390284538\n",
      "Epoch[0], Batch[1602], Train loss: 0.042597170919179916\n",
      "Epoch[0], Val loss: 0.038305364549160004\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/siddharth/DL-A3/dl-a3.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.2.16.106/home/siddharth/DL-A3/dl-a3.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.2.16.106/home/siddharth/DL-A3/dl-a3.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output[:,:,:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], encoded_tgt[:,\u001b[39m1\u001b[39m:])\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.2.16.106/home/siddharth/DL-A3/dl-a3.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.2.16.106/home/siddharth/DL-A3/dl-a3.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m optimizer_decoder\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.2.16.106/home/siddharth/DL-A3/dl-a3.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m optimizer_encoder\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/DLA3/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/DLA3/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(4):\n",
    "  batch = 1\n",
    "  for data, vis in train_loader:\n",
    "    # Training Loop\n",
    "      encoder.train()\n",
    "      decoder.train()\n",
    "      optimizer_encoder.zero_grad()\n",
    "      optimizer_decoder.zero_grad()\n",
    "      \n",
    "      context = encoder(data)\n",
    "      output, encoded_tgt = decoder(context, vis, 1.0)\n",
    "      output = output.permute(0,2,1)\n",
    "      loss = criterion(output[:,:,:-1], encoded_tgt[:,1:])\n",
    "      \n",
    "      loss.backward()\n",
    "      optimizer_decoder.step()\n",
    "      optimizer_encoder.step()\n",
    "      \n",
    "      print(f\"Epoch[{epoch}], Batch[{batch}], Train loss: {loss.item()}\")\n",
    "        \n",
    "    # Validation (with one minibatch only)\n",
    "      with torch.no_grad():\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        for data, vis in eval_loader:\n",
    "          context_ = encoder(data)\n",
    "          output_, encoded_tgt_ = decoder(context_,vis,1.0)\n",
    "          output_ = output_.permute(0,2,1)\n",
    "          loss_val = criterion(output_[:,:,:-1], encoded_tgt_[:,1:])\n",
    "          print(f\"Epoch[{epoch}], Val loss: {loss_val.item()}\")\n",
    "        \n",
    "          checkpoint_encoder = {\n",
    "                'model_state_dict': encoder.state_dict(),\n",
    "                'optimizer_state_dict': optimizer_encoder.state_dict(),\n",
    "          }\n",
    "          checkpoint_decoder = {\n",
    "                'model_state_dict': decoder.state_dict(),\n",
    "                'optimizer_state_dict': optimizer_decoder.state_dict(),\n",
    "          }\n",
    "          \n",
    "          if loss_val < best_val_loss:\n",
    "              best_val_loss = loss_val\n",
    "              epochs_since_improvement = 0\n",
    "              torch.save(checkpoint_encoder, 'Model/encoder.pth')\n",
    "              torch.save(checkpoint_decoder, 'Model/decoder.pth')\n",
    "          else:\n",
    "              epochs_since_improvement += 1\n",
    "          break\n",
    "\n",
    "      # Check if we should stop training early\n",
    "      if epochs_since_improvement >= patience:\n",
    "        print(f\"Early stopping after {epoch} epochs , {batch} batches with no improvement.\")\n",
    "        break\n",
    "      batch += 1\n",
    "      \n",
    "  if loss_val < best_val_loss:\n",
    "      torch.save(checkpoint_encoder, 'Model/encoder.pth')\n",
    "      torch.save(checkpoint_decoder, 'Model/decoder.pth')\n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T04:27:07.995271Z",
     "iopub.status.busy": "2023-11-13T04:27:07.994287Z",
     "iopub.status.idle": "2023-11-13T04:27:08.200504Z",
     "shell.execute_reply": "2023-11-13T04:27:08.199369Z",
     "shell.execute_reply.started": "2023-11-13T04:27:07.995219Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average log perplexity on the test set: 0.03871905228495598\n"
     ]
    }
   ],
   "source": [
    "encoder_ = Encoder().to(device)\n",
    "decoder_ = Decoder().to(device)\n",
    "checkpoint_enc = torch.load('Model/encoder.pth',map_location=torch.device('cpu'))\n",
    "checkpoint_dec = torch.load('Model/decoder.pth',map_location=torch.device('cpu'))\n",
    "encoder_.load_state_dict(checkpoint_enc['model_state_dict'])\n",
    "decoder_.load_state_dict(checkpoint_dec['model_state_dict'])\n",
    "encoder_.eval()\n",
    "decoder_.eval()\n",
    "# optimizer_encoder = optim.Adam(encoder_.parameters(), lr=0.0001)\n",
    "# optimizer_decoder = optim.Adam(decoder_.parameters(), lr=0.0001)\n",
    "optimizer_encoder.load_state_dict(checkpoint_enc['optimizer_state_dict'])\n",
    "optimizer_decoder.load_state_dict(checkpoint_dec['optimizer_state_dict'])\n",
    "\n",
    "criterion = nn.NLLLoss(ignore_index = 46)\n",
    "\n",
    "# with torch.no_grad():\n",
    "total_test = 0.0\n",
    "batch = 1\n",
    "for data, vis in train_loader:\n",
    "  context_ = encoder_(data)\n",
    "  output_, encoded_tgt_ = decoder_(context_,vis,1.0)\n",
    "  output_ = output_.permute(0,2,1)\n",
    "  loss_test = criterion(output_[:,:,:-1], encoded_tgt_[:,1:])\n",
    "  total_test += loss_test.item()\n",
    "  print(f\"Batch [{batch}], loss: {loss_test.item()}\")\n",
    "  batch += 1\n",
    "print(f\"Average log perplexity on the test set: {total_test / len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate sequences by changing the diversity penalty in the beam search code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, context):\n",
    "    model.context = context\n",
    "    start = torch.zeros(1,1, dtype = int).to(device) + 44\n",
    "    target_seq = model.embedding(start)\n",
    "    \n",
    "    initial_hidden1 = torch.rand(target_seq.shape[0], model.hidden_size).to(device)\n",
    "    initial_cell1 = torch.rand(target_seq.shape[0], model.hidden_size).to(device)\n",
    "    initial_hidden2 = torch.rand(target_seq.shape[0], model.hidden_size).to(device)\n",
    "    initial_cell2 = torch.rand(target_seq.shape[0], model.hidden_size).to(device)\n",
    "    \n",
    "    outputs = []\n",
    "    hidden_states = []\n",
    "    cell_states = []\n",
    "    query = [initial_hidden2]\n",
    "    for timestep in range(model.max_tgt+1):\n",
    "        if(timestep == 0):\n",
    "            (h_t1, c_t1) = model.dec_cells[0](model.drop(torch.cat((target_seq[:,timestep],model.calcontext(0,query[-1])),dim=1)), (initial_hidden1, initial_cell1))\n",
    "            (h_t2, c_t2) = model.dec_cells[1](model.drop(h_t1), (initial_hidden2, initial_cell2))\n",
    "        else:\n",
    "            input = model.embedding(torch.argmax(nn.Softmax(dim = 1)(outputs[-1][:,0]),dim=1))\n",
    "                \n",
    "            (h_t1, c_t1) = model.dec_cells[0](model.drop(torch.cat((input,model.calcontext(timestep,query[-1])),dim=1)), (hidden_states[-1][0], cell_states[-1][0]))\n",
    "            (h_t2, c_t2) = model.dec_cells[1](model.drop(h_t1), (hidden_states[-1][1], cell_states[-1][1]))\n",
    "        hidden_states.append([h_t1, h_t2])\n",
    "        cell_states.append([c_t1, c_t2])\n",
    "        query.append(h_t2)\n",
    "        out = model.linear(h_t2)\n",
    "        outputs.append(out.unsqueeze(1))\n",
    "\n",
    "    output_prob = torch.cat(outputs,dim = 1)\n",
    "    out = torch.argmax(nn.Softmax(dim = 2)(output_prob), dim = 2)\n",
    "    \n",
    "#         Example usage:\n",
    "#         Replace 'probabilities', 'beam_width', and 'max_length' with your actual values\n",
    "#         probabilities = torch.tensor(...)  # Shape: (sequence_length, vocab_size)\n",
    "#         beam_width = 3\n",
    "#         max_length = 10\n",
    "    decoded_sequences = beam_search_decoder(output_prob[0], beam_width = 15, max_length = 1500, diversity_penalty_weight = 5)\n",
    "    vis = []\n",
    "    for sequence, score in decoded_sequences:\n",
    "        vis.append(model.seq_to_vis(sequence))\n",
    "        # print(f\"Sequence: {sequence}, {self.seq_to_vis(sequence)}, Log-Likelihood Score: {score}\")\n",
    "\n",
    "    return vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}\n",
      "\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}\n",
      "\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}\n",
      "\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}\n",
      "\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}\n",
      "\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}\n",
      "\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}\n",
      "\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}\n",
      "\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}\n",
      "\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}\n",
      "\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}\n",
      "\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}\n",
      "\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}\n",
      "\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}\n",
      "\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('Data/A3 files/progression.txt') as f:\n",
    "    progression, progression_transformed = f.readlines()\n",
    "    progression_transformed = [progression_transformed]\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoder_.eval()\n",
    "    decoder_.eval()\n",
    "    context_ = encoder_(progression_transformed)\n",
    "    output = inference(decoder_,context_)\n",
    "    for vis in output:\n",
    "        print(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}\n",
      "\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}}\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"} \n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"},\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}i\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}a\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}:\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}k\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}l\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}e\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\":\n",
      "\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}c\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}o\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}y\n"
     ]
    }
   ],
   "source": [
    "for vis in output:\n",
    "    print(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}\n",
      "\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}}\n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"} \n",
      "{\"encoding\": {\"x\": {\"field\": \"num2\", \"type\": \"quantitative\", \"bin\": true}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"},\n",
      "{\"encoding\": {\"x\": {\"field\": \"str0\", \"type\": \"nominal\"}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}\n",
      "{\"encoding\": {\"x\": {\"field\": \"str0\", \"type\": \"nominal\"}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}?\n",
      "{\"encoding\": {\"x\": {\"field\": \"str0\", \"type\": \"nominal\"}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}\"\n",
      "{\"encoding\": {\"x\": {\"field\": \"str0\", \"type\": \"nominal\"}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}m\n",
      "{\"encoding\": {\"x\": {\"field\": \"str0\", \"type\": \"nominal\"}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}t\n",
      "{\"encoding\": {\"x\": {\"field\": \"str0\", \"type\": \"nominal\"}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}o\n",
      "{\"encoding\": {\"x\": {\"field\": \"str0\", \"type\": \"nominal\"}, \"y\": {\"field\": \"num0\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}d\n",
      "{\"encoding\": {\"x\": {\"field\": \"str0\", \"type\": \"nominal\"}, \"y\": {\"field\": \"num1\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}}\n",
      "{\"encoding\": {\"x\": {\"field\": \"str0\", \"type\": \"nominal\"}, \"y\": {\"field\": \"num1\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"},\n",
      "{\"encoding\": {\"x\": {\"field\": \"str0\", \"type\": \"nominal\"}, \"y\": {\"field\": \"num1\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}s\n"
     ]
    }
   ],
   "source": [
    "with open('Data/A3 files/progression.txt') as f:\n",
    "    progression, progression_transformed = f.readlines()\n",
    "    progression_transformed = [progression_transformed]\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoder_.eval()\n",
    "    decoder_.eval()\n",
    "    context_ = encoder_(progression_transformed)\n",
    "    output = inference(decoder_,context_)\n",
    "    for vis in output:\n",
    "        print(vis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "{\"encoding\": {\"x\": {\"field\": \"str0\", \"type\": \"nominal\"}, \"y\": {\"field\": \"num1\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}}\n",
      "{\"encoding\": {\"x\": {\"field\": \"str0\", \"type\": \"nominal\"}, \"y\": {\"field\": \"num1\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"},\n",
      "{\"encoding\": {\"x\": {\"field\": \"str0\", \"type\": \"nominal\"}, \"y\": {\"field\": \"num1\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}s\n",
      "{\"encoding\": {\"x\": {\"field\": \"str0\", \"type\": \"nominal\"}, \"y\": {\"field\": \"num1\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}}\n",
      "{\"encoding\": {\"x\": {\"field\": \"str0\", \"type\": \"nominal\"}, \"y\": {\"field\": \"num1\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}}\n",
      "{\"encoding\": {\"x\": {\"field\": \"str0\", \"type\": \"nominal\"}, \"y\": {\"field\": \"num1\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}}\n",
      "{\"encoding\": {\"x\": {\"field\": \"str0\", \"type\": \"nominal\"}, \"y\": {\"field\": \"num1\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}}\n",
      "{\"encoding\": {\"x\": {\"field\": \"str0\", \"type\": \"nominal\"}, \"y\": {\"field\": \"num1\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}}\n",
      "{\"encoding\": {\"x\": {\"field\": \"str0\", \"type\": \"nominal\"}, \"y\": {\"field\": \"num1\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}}\n",
      "{\"encoding\": {\"x\": {\"field\": \"str0\", \"type\": \"nominal\"}, \"y\": {\"field\": \"num1\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}}\n",
      "{\"encoding\": {\"x\": {\"field\": \"str0\", \"type\": \"nominal\"}, \"y\": {\"field\": \"num1\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}}\n",
      "{\"encoding\": {\"x\": {\"field\": \"str0\", \"type\": \"nominal\"}, \"y\": {\"field\": \"num1\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}}\n",
      "{\"encoding\": {\"x\": {\"field\": \"str0\", \"type\": \"nominal\"}, \"y\": {\"field\": \"num1\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}}\n",
      "{\"encoding\": {\"x\": {\"field\": \"str0\", \"type\": \"nominal\"}, \"y\": {\"field\": \"num1\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}}\n",
      "{\"encoding\": {\"x\": {\"field\": \"str0\", \"type\": \"nominal\"}, \"y\": {\"field\": \"num1\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}}\n",
      "{\"encoding\": {\"x\": {\"field\": \"str0\", \"type\": \"nominal\"}, \"y\": {\"field\": \"num1\", \"type\": \"ordinal\"}, \"detail\": {\"field\": \"*\", \"aggregate\": \"count\", \"type\": \"quantitative\"}}, \"mark\": \"bar\"}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with penalty 5\n",
    "with open('Data/A3 files/progression.txt') as f:\n",
    "    progression, progression_transformed = f.readlines()\n",
    "    progression_transformed = [progression_transformed]\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoder_.eval()\n",
    "    decoder_.eval()\n",
    "    context_ = encoder_(progression_transformed)\n",
    "    output = inference(decoder_,context_)\n",
    "    for vis in output:\n",
    "        print(vis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with penalty 10 and max length and changinng progression.txt data\n",
    "# with open('Data/A3 files/progression.txt') as f:\n",
    "#     progression, progression_transformed = f.readlines()\n",
    "#     progression_transformed = [progression_transformed]\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     encoder_.eval()\n",
    "#     decoder_.eval()\n",
    "#     context_ = encoder_(progression_transformed)\n",
    "#     output = inference(decoder_,context_)\n",
    "#     for vis in output:\n",
    "#         print(vis)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 293.852,
   "position": {
    "height": "180.852px",
    "left": "1097px",
    "right": "20px",
    "top": "120px",
    "width": "323px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
